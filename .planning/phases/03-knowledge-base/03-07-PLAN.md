---
phase: 03-knowledge-base
plan: 07
type: tdd
wave: 5
depends_on: ["03-01", "03-03", "03-04", "03-05", "03-06"]
files_modified:
  - src/knowledge/rag/__init__.py
  - src/knowledge/rag/decomposer.py
  - src/knowledge/rag/retriever.py
  - src/knowledge/rag/synthesizer.py
  - src/knowledge/rag/pipeline.py
  - tests/knowledge/test_rag_pipeline.py
autonomous: true

must_haves:
  truths:
    - "Complex queries are decomposed into sub-queries targeting different knowledge sources"
    - "Retriever fetches from multiple sources (products, methodology, regional, conversations) and merges results"
    - "Synthesizer produces coherent answers grounded in retrieved source documents"
    - "Agentic RAG pipeline handles multi-hop queries (query -> retrieve -> evaluate -> rewrite if needed -> retrieve again)"
    - "Retrieved chunks are graded for relevance before synthesis"
  artifacts:
    - path: "src/knowledge/rag/decomposer.py"
      provides: "Query decomposition that breaks complex questions into sub-queries"
      exports: ["QueryDecomposer"]
    - path: "src/knowledge/rag/retriever.py"
      provides: "Multi-source retriever that fetches from products, methodology, regional, conversations"
      exports: ["MultiSourceRetriever"]
    - path: "src/knowledge/rag/synthesizer.py"
      provides: "Answer synthesis grounded in retrieved documents"
      exports: ["ResponseSynthesizer"]
    - path: "src/knowledge/rag/pipeline.py"
      provides: "LangGraph-based agentic RAG pipeline with iterative retrieval"
      exports: ["AgenticRAGPipeline"]
  key_links:
    - from: "src/knowledge/rag/retriever.py"
      to: "src/knowledge/qdrant_client.py"
      via: "hybrid search over knowledge_base collection"
      pattern: "QdrantKnowledgeStore|hybrid_search"
    - from: "src/knowledge/rag/retriever.py"
      to: "src/knowledge/conversations/store.py"
      via: "search conversation history for context"
      pattern: "ConversationStore|search_conversations"
    - from: "src/knowledge/rag/pipeline.py"
      to: "src/knowledge/rag/decomposer.py"
      via: "decompose query before retrieval"
      pattern: "QueryDecomposer"
    - from: "src/knowledge/rag/pipeline.py"
      to: "src/knowledge/rag/retriever.py"
      via: "retrieve chunks for each sub-query"
      pattern: "MultiSourceRetriever"
    - from: "src/knowledge/rag/pipeline.py"
      to: "src/knowledge/rag/synthesizer.py"
      via: "synthesize answer from retrieved chunks"
      pattern: "ResponseSynthesizer"
---

<objective>
Build the agentic RAG pipeline using LangGraph that decomposes complex queries, retrieves from multiple knowledge sources, grades document relevance, and synthesizes coherent answers grounded in source documents.

Purpose: This is the intelligence layer that makes the knowledge base useful. Raw retrieval returns chunks; the RAG pipeline understands intent, queries multiple sources, evaluates relevance, and produces answers that cite sources. It's the bridge between stored knowledge and useful agent responses.

Output: Working AgenticRAGPipeline that handles queries from simple ("What is Monetization Platform pricing?") to complex ("How should I position the Monetization Platform vs Competitor A for a CTO in APAC who's evaluating enterprise deployment?").
</objective>

<execution_context>
@/Users/RAZER/.claude/get-shit-done/workflows/execute-plan.md
@/Users/RAZER/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/phases/03-knowledge-base/03-01-SUMMARY.md
@.planning/phases/03-knowledge-base/03-03-SUMMARY.md
@.planning/phases/03-knowledge-base/03-04-SUMMARY.md
@.planning/phases/03-knowledge-base/03-05-SUMMARY.md
@.planning/phases/03-knowledge-base/03-06-SUMMARY.md

Depends on:
- Plan 01: Qdrant client, embedding service (QdrantKnowledgeStore, EmbeddingService)
- Plan 03: Ingestion pipeline wiring (IngestionPipeline)
- Plan 04: ESW product knowledge ingested in Qdrant (Monetization Platform, Charging, Billing -- features, pricing, positioning)
- Plan 05: Methodology and regional content ingested in Qdrant
- Plan 06: Conversation history storage (ConversationStore)

Key research findings:
- Agentic RAG via LangGraph with nodes: generate_query_or_respond, retrieve (ToolNode), grade_documents, rewrite_question
- Query decomposition: Break complex queries into sub-queries targeting specific knowledge types
- Document grading: LLM evaluates if retrieved chunks are relevant to the query
- Question rewriting: If retrieved documents are insufficient, rewrite query and retry (max 2 iterations)
- RRF fusion already handled at Qdrant level (Plan 01); this layer adds multi-source and grading

Locked decisions:
- Query decomposition: Yes (agentic RAG pattern)
- Retrieval count: Top 5-10 chunks per query
- Hybrid search (semantic + keyword) -- handled by Plan 01's QdrantKnowledgeStore
</context>

<feature>
  <name>Agentic RAG Pipeline</name>
  <files>
    src/knowledge/rag/decomposer.py
    src/knowledge/rag/retriever.py
    src/knowledge/rag/synthesizer.py
    src/knowledge/rag/pipeline.py
    src/knowledge/rag/__init__.py
    tests/knowledge/test_rag_pipeline.py
  </files>
  <behavior>
    The RAG pipeline has four components with testable input/output behavior:

    1. QueryDecomposer:
       Input: "How should I position the Monetization Platform vs Competitor A for a CTO in APAC?"
       Output: [
         {"query": "Monetization Platform key features and differentiators", "source_type": "product", "filters": {"product_category": "monetization"}},
         {"query": "Competitive positioning vs Competitor A", "source_type": "product", "filters": {"content_type": "positioning"}},
         {"query": "APAC sales approach for technical buyers", "source_type": "regional", "filters": {"region": "apac"}},
         {"query": "CTO buyer persona engagement strategy", "source_type": "methodology", "filters": {"buyer_persona": "technical"}}
       ]

       Simple query passthrough:
       Input: "What is Monetization Platform pricing?"
       Output: [{"query": "Monetization Platform pricing", "source_type": "product", "filters": {"content_type": "pricing"}}]

    2. MultiSourceRetriever:
       Input: list of SubQuery objects + tenant_id
       Output: list of RetrievedChunk objects (chunk + relevance_score + source_type)
       Behavior: Executes each sub-query against Qdrant, merges results, deduplicates, returns top_k (5-10)

    3. ResponseSynthesizer:
       Input: original_query + list of RetrievedChunk objects
       Output: SynthesizedResponse(answer: str, sources: list[SourceCitation], confidence: float)
       Behavior: Uses LLM to synthesize coherent answer grounded in retrieved chunks. Each claim cites its source.

    4. AgenticRAGPipeline (LangGraph):
       Input: query (str) + tenant_id (str) + context (optional conversation history)
       Output: RAGResponse(answer: str, sources: list, sub_queries: list, iterations: int)
       Behavior: Orchestrates decompose -> retrieve -> grade -> (rewrite if needed) -> synthesize
       - If grade_documents marks >50% chunks as irrelevant, rewrite query and retry (max 2 iterations)
       - Tracks iteration count for observability

    Test cases:
    - Simple product query -> single sub-query -> direct retrieval -> synthesis
    - Complex multi-faceted query -> multiple sub-queries -> multi-source retrieval -> synthesis
    - Poor initial retrieval -> document grading triggers rewrite -> better retrieval on retry
    - Query with conversation context -> includes conversation history in retrieval
    - Tenant isolation -> results only from querying tenant
  </behavior>
  <implementation>
    QueryDecomposer (src/knowledge/rag/decomposer.py):
    - Uses LLM (OpenAI) to analyze query and determine if decomposition needed
    - Simple queries (single intent, single source) pass through as single sub-query
    - Complex queries decomposed into 2-4 sub-queries with source_type and filter hints
    - SubQuery(BaseModel): query (str), source_type (Literal["product", "methodology", "regional", "conversation"]), filters (dict)
    - LLM prompt includes available source types and filter fields for structured output

    MultiSourceRetriever (src/knowledge/rag/retriever.py):
    - Takes list of SubQuery objects
    - For each sub-query: call QdrantKnowledgeStore.hybrid_search() with appropriate filters
    - For conversation sub-queries: call ConversationStore.search_conversations()
    - Merge results from all sub-queries, deduplicate by chunk ID
    - Re-rank merged results (simple score-based initially; LLM re-ranking as future enhancement)
    - Return top_k (configurable, default 7) RetrievedChunk objects
    - RetrievedChunk(BaseModel): chunk (KnowledgeChunk), relevance_score (float), source_type (str), sub_query (str)

    ResponseSynthesizer (src/knowledge/rag/synthesizer.py):
    - Takes original query + retrieved chunks
    - Builds LLM prompt with: query, chunk contents (numbered), instruction to synthesize and cite sources
    - LLM generates answer with inline citations [1], [2], etc.
    - Parses response into SynthesizedResponse with answer text and source citations
    - SourceCitation(BaseModel): citation_id (int), chunk_id (str), source_document (str), content_snippet (str)
    - confidence field: ratio of relevant chunks to total chunks (from grading step)

    AgenticRAGPipeline (src/knowledge/rag/pipeline.py):
    - LangGraph StateGraph with nodes:
      * "decompose": QueryDecomposer.decompose(query) -> sub_queries
      * "retrieve": MultiSourceRetriever.retrieve(sub_queries, tenant_id) -> chunks
      * "grade": Grade each chunk for relevance (LLM call: "Is this chunk relevant to the query? yes/no")
      * "decide": If >50% irrelevant AND iterations < 2: route to "rewrite". Else: route to "synthesize"
      * "rewrite": Rewrite query based on what was found to be irrelevant (LLM call), increment iteration, route back to "retrieve"
      * "synthesize": ResponseSynthesizer.synthesize(query, relevant_chunks) -> response
    - State: RAGState(query, tenant_id, sub_queries, retrieved_chunks, graded_chunks, iterations, response)
    - Compile graph with entry="decompose", conditional edges from "decide"
    - async run(query: str, tenant_id: str, conversation_context: list[ConversationMessage] | None = None) -> RAGResponse
    - RAGResponse(BaseModel): answer (str), sources (list[SourceCitation]), sub_queries (list[SubQuery]), iterations (int), confidence (float)

    Tests (tests/knowledge/test_rag_pipeline.py):
    - RED phase: Write tests with mock dependencies (mock LLM, mock Qdrant, mock embeddings)
      * test_simple_query_decomposition: Single-intent query produces 1 sub-query
      * test_complex_query_decomposition: Multi-faceted query produces 3-4 sub-queries with correct source_types
      * test_multi_source_retrieval: Given sub-queries for product + methodology + regional, retriever returns merged results
      * test_document_grading_passes: All relevant chunks pass grading, pipeline proceeds to synthesis
      * test_document_grading_triggers_rewrite: >50% irrelevant triggers query rewrite
      * test_max_iterations_respected: Rewrite happens max 2 times then proceeds to synthesis anyway
      * test_synthesis_cites_sources: Synthesized answer includes source citations
      * test_full_pipeline_simple: End-to-end with simple query
      * test_full_pipeline_complex: End-to-end with complex multi-source query
      * test_pipeline_with_conversation_context: Pipeline includes conversation history in retrieval
    - GREEN phase: Implement to pass all tests
    - REFACTOR: Clean up, ensure consistent error handling
  </implementation>
</feature>

<verification>
- `pytest tests/knowledge/test_rag_pipeline.py -v` all tests pass
- Simple queries produce single sub-query and direct retrieval
- Complex queries decompose into multiple sub-queries targeting different sources
- Document grading filters irrelevant chunks
- Query rewriting improves retrieval on retry
- Synthesis produces answers with source citations
- Pipeline completes within max iteration limit
</verification>

<success_criteria>
- QueryDecomposer breaks complex queries into source-targeted sub-queries
- MultiSourceRetriever fetches from products, methodology, regional, and conversations
- Document grading filters irrelevant chunks and triggers rewrite when needed
- ResponseSynthesizer produces answers grounded in source documents with citations
- AgenticRAGPipeline orchestrates the full flow via LangGraph state machine
- Max 2 rewrite iterations prevent infinite loops
- All operations are tenant-scoped
- Tests pass with mocked LLM and embeddings (no external API calls in tests)
</success_criteria>

<output>
After completion, create `.planning/phases/03-knowledge-base/03-07-SUMMARY.md`
</output>
