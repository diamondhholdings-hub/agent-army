---
phase: 03-knowledge-base
plan: 03
type: execute
wave: 3
depends_on: ["03-01", "03-02"]
files_modified:
  - src/knowledge/ingestion/pipeline.py
  - src/knowledge/products/__init__.py
  - src/knowledge/products/esw_data.py
  - tests/knowledge/test_pipeline.py
autonomous: true

must_haves:
  truths:
    - "IngestionPipeline orchestrates load -> chunk -> enrich -> embed -> store end-to-end"
    - "New product documents can be ingested through the pipeline end-to-end"
    - "Document updates increment version and mark old chunks as not current"
    - "Directory ingestion processes all supported files recursively"
  artifacts:
    - path: "src/knowledge/ingestion/pipeline.py"
      provides: "End-to-end ingestion pipeline connecting loaders, chunking, embedding, and storage"
      exports: ["IngestionPipeline", "IngestionResult"]
    - path: "src/knowledge/products/esw_data.py"
      provides: "ESW product ingestion helper and verification utilities"
      exports: ["ingest_all_esw_products", "verify_product_retrieval"]
  key_links:
    - from: "src/knowledge/ingestion/pipeline.py"
      to: "src/knowledge/qdrant_client.py"
      via: "stores embedded chunks in Qdrant"
      pattern: "QdrantKnowledgeStore"
    - from: "src/knowledge/ingestion/pipeline.py"
      to: "src/knowledge/embeddings.py"
      via: "generates embeddings before storage"
      pattern: "EmbeddingService"
    - from: "src/knowledge/ingestion/pipeline.py"
      to: "src/knowledge/ingestion/loaders.py"
      via: "loads documents from files"
      pattern: "DocumentLoader"
    - from: "src/knowledge/ingestion/pipeline.py"
      to: "src/knowledge/ingestion/chunker.py"
      via: "chunks loaded sections"
      pattern: "KnowledgeChunker"
    - from: "src/knowledge/ingestion/pipeline.py"
      to: "src/knowledge/ingestion/metadata_extractor.py"
      via: "enriches chunk metadata"
      pattern: "MetadataExtractor"
---

<objective>
Wire the complete ingestion pipeline (IngestionPipeline class) that connects document loading, chunking, metadata enrichment, embedding, and Qdrant storage into a single orchestrated flow.

Purpose: Plan 01 created the Qdrant store and embedding service. Plan 02 created the loaders, chunker, and metadata extractor. This plan wires them into a single pipeline class that any content (products, methodology, etc.) can be ingested through. It also creates the ESW-specific ingestion helper.

Output: IngestionPipeline class that accepts any supported document and stores it as enriched, embedded chunks in Qdrant. ESW product ingestion helper for batch ingestion of product data.
</objective>

<execution_context>
@/Users/RAZER/.claude/get-shit-done/workflows/execute-plan.md
@/Users/RAZER/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/phases/03-knowledge-base/03-01-SUMMARY.md
@.planning/phases/03-knowledge-base/03-02-SUMMARY.md

This plan depends on:
- Plan 01: Qdrant client, embedding service, and models (QdrantKnowledgeStore, EmbeddingService, KnowledgeChunk)
- Plan 02: Ingestion pipeline components (DocumentLoader, KnowledgeChunker, MetadataExtractor)

This plan wires them together into the IngestionPipeline orchestrator.

Locked decisions:
- Per-product organization (each ESW product is a separate knowledge unit: Monetization Platform, Charging, Billing)
- Product categories: Monetization, Charging, Billing (NOT fictional names)
- Feature-level chunks
- Pricing: Both structured data AND natural language docs
- Positioning: Both per-competitor battlecards AND per-use-case value props
- Both manual upload AND auto-sync triggers
- Versioning: Track knowledge versions
</context>

<tasks>

<task type="auto">
  <name>Task 1: Implement IngestionPipeline orchestrator</name>
  <files>
    src/knowledge/ingestion/pipeline.py
  </files>
  <action>
    src/knowledge/ingestion/pipeline.py - IngestionPipeline class:
    - __init__(store: QdrantKnowledgeStore, embedder: EmbeddingService, chunker: KnowledgeChunker, extractor: MetadataExtractor)
    - async ingest_document(file_path: str | Path, tenant_id: str, metadata_overrides: dict | None = None) -> IngestionResult:
      1. Load document via DocumentLoader.load_document()
      2. Chunk sections via KnowledgeChunker.chunk_sections()
      3. Enrich metadata via MetadataExtractor.enrich_chunks() + apply overrides
      4. Generate embeddings via EmbeddingService.embed_batch()
      5. Store in Qdrant via QdrantKnowledgeStore.upsert_chunks()
      6. Return IngestionResult(chunks_created: int, document_source: str, errors: list[str])
    - async ingest_directory(dir_path: str | Path, tenant_id: str, recursive: bool = True) -> list[IngestionResult]:
      * Walk directory, ingest each supported file
      * Skip unsupported formats with warning
      * Return results for each file
    - async update_document(file_path: str | Path, tenant_id: str) -> IngestionResult:
      * Re-ingest document: set is_current=False on old version chunks, increment version on new
      * This handles the versioning requirement
    - IngestionResult(BaseModel): chunks_created (int), document_source (str), errors (list[str])
  </action>
  <verify>
    Run: `python -c "from src.knowledge.ingestion.pipeline import IngestionPipeline, IngestionResult; print('Imports OK')"`
    Verify no import errors.
  </verify>
  <done>
    IngestionPipeline orchestrates the complete flow: load -> chunk -> enrich -> embed -> store. Supports single document, directory, and document update (versioning) operations.
  </done>
</task>

<task type="auto">
  <name>Task 2: Create ESW ingestion helper and test pipeline end-to-end</name>
  <files>
    src/knowledge/products/__init__.py
    src/knowledge/products/esw_data.py
    tests/knowledge/test_pipeline.py
  </files>
  <action>
    src/knowledge/products/esw_data.py:
    - ESW_DEFAULT_TENANT_ID = "esw-default"
    - PRODUCT_DATA_DIR = Path("data/products")
    - async ingest_all_esw_products(pipeline: IngestionPipeline) -> list[IngestionResult]:
      * Ingest all documents from data/products/ directory
      * Return combined results
    - Helper to verify ingestion: async verify_product_retrieval(store: QdrantKnowledgeStore) -> dict:
      * Run sample queries and verify results returned
      * Return {"total_chunks": N, "sample_results": [...]}

    src/knowledge/products/__init__.py:
    - Re-export ingest_all_esw_products, verify_product_retrieval, ESW_DEFAULT_TENANT_ID

    tests/knowledge/test_pipeline.py:
    - Uses a mock EmbeddingService that returns deterministic vectors (to avoid OpenAI API calls in tests)
    - Uses small inline test fixtures (short markdown docs, simple JSON) -- NOT the full ESW product data
    - test_ingest_single_document: Create a small test markdown file, ingest via pipeline, verify chunks stored in Qdrant (use local Qdrant with tmp_path)
    - test_ingest_directory: Create temp dir with 2-3 test files, ingest all, verify chunk count
    - test_document_update_versioning: Ingest, then re-ingest with update_document. Verify old chunks marked is_current=False, new chunks have version=2
    - test_tenant_scoped_ingestion: Ingest for tenant A, verify tenant B cannot retrieve
    - test_metadata_overrides: Ingest with metadata_overrides, verify overrides applied to stored chunks
  </action>
  <verify>
    Run: `pytest tests/knowledge/test_pipeline.py -v`
    All tests pass. Specifically:
    - Full pipeline processes documents end-to-end
    - Versioning works (old -> is_current=False, new -> version+1)
    - Tenant isolation maintained through pipeline
    - Metadata overrides applied correctly
  </verify>
  <done>
    IngestionPipeline verified end-to-end with test fixtures. ESW ingestion helper ready for use by Plan 04 (product data ingestion). Versioning, tenant isolation, and metadata overrides all tested.
  </done>
</task>

</tasks>

<verification>
- `pytest tests/knowledge/test_pipeline.py -v` all pass
- IngestionPipeline can process any supported file format end-to-end
- Document versioning tracks updates correctly
- Python imports work: `from src.knowledge.ingestion.pipeline import IngestionPipeline`
- Python imports work: `from src.knowledge.products import ingest_all_esw_products`
</verification>

<success_criteria>
- IngestionPipeline wires load -> chunk -> enrich -> embed -> store into single orchestrated flow
- Single document, directory, and update operations all functional
- Document updates increment version and mark old chunks as not current
- Tenant isolation enforced through pipeline
- All tests pass with mocked embeddings and local Qdrant
</success_criteria>

<output>
After completion, create `.planning/phases/03-knowledge-base/03-03-SUMMARY.md`
</output>
