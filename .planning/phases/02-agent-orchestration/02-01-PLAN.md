---
phase: 02-agent-orchestration
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - pyproject.toml
  - src/app/events/__init__.py
  - src/app/events/schemas.py
  - src/app/events/bus.py
  - src/app/events/consumer.py
  - src/app/events/dlq.py
  - tests/test_events.py
autonomous: true

must_haves:
  truths:
    - "Events can be published to tenant-scoped Redis Streams and consumed by consumer groups"
    - "Failed events retry with exponential backoff (1s, 4s, 16s) then move to dead letter queue after 3 attempts"
    - "Event payloads carry source attribution (agent ID + full call chain) and tenant context"
    - "Stream keys are tenant-scoped: t:{tenant_id}:events:{stream_name}"
  artifacts:
    - path: "src/app/events/schemas.py"
      provides: "AgentEvent Pydantic model, EventType enum, EventPriority enum"
      exports: ["AgentEvent", "EventType", "EventPriority"]
    - path: "src/app/events/bus.py"
      provides: "TenantEventBus with publish/subscribe/ack"
      exports: ["TenantEventBus"]
    - path: "src/app/events/consumer.py"
      provides: "EventConsumer with retry logic and consumer group management"
      exports: ["EventConsumer"]
    - path: "src/app/events/dlq.py"
      provides: "Dead letter queue handler with DLQ stream management"
      exports: ["DeadLetterQueue"]
    - path: "tests/test_events.py"
      provides: "Unit tests for event schemas, bus, consumer, and DLQ"
      min_lines: 80
  key_links:
    - from: "src/app/events/bus.py"
      to: "redis.asyncio"
      via: "xadd/xreadgroup/xack Redis Streams commands"
      pattern: "xadd|xreadgroup|xack"
    - from: "src/app/events/consumer.py"
      to: "src/app/events/dlq.py"
      via: "moves failed messages after MAX_RETRIES"
      pattern: "DeadLetterQueue|dlq"
    - from: "src/app/events/schemas.py"
      to: "pydantic"
      via: "BaseModel with validation"
      pattern: "class AgentEvent.*BaseModel"
---

<objective>
Build the event-driven backbone for agent coordination using Redis Streams with tenant isolation, structured event schemas, consumer group processing, and dead letter queue handling.

Purpose: PLT-03 requires an event-driven backbone for agent coordination. Redis Streams (already available via Phase 1's redis-py >=5.0.0) provide the pub/sub infrastructure with consumer groups, acknowledgment, and built-in message ordering. This plan establishes the communication layer that all agents will use.

Output: Complete events/ package with schemas, tenant-scoped bus, consumer with retry logic, and DLQ handler. Also updates pyproject.toml with ALL Phase 2 dependencies upfront so subsequent plans don't need to manage dependencies.
</objective>

<execution_context>
@/Users/RAZER/.claude/get-shit-done/workflows/execute-plan.md
@/Users/RAZER/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-agent-orchestration/02-CONTEXT.md
@.planning/phases/02-agent-orchestration/02-RESEARCH.md
@src/app/core/redis.py
@src/app/config.py
@pyproject.toml
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add Phase 2 dependencies and create event schemas</name>
  <files>pyproject.toml, src/app/events/__init__.py, src/app/events/schemas.py</files>
  <action>
1. Update pyproject.toml dependencies to add ALL Phase 2 libraries at once:
   - langgraph>=1.0.8
   - langgraph-supervisor>=0.0.31
   - langgraph-checkpoint-postgres>=3.0.4
   - langfuse>=3.14.1
   - langchain-anthropic>=0.3.0
   - langchain-openai>=0.3.0
   - tiktoken>=0.7.0
   - pgvector>=0.3.0

2. Create src/app/events/__init__.py with exports for AgentEvent, EventType, EventPriority, TenantEventBus, EventConsumer, DeadLetterQueue.

3. Create src/app/events/schemas.py with:
   - EventPriority enum: LOW, NORMAL, HIGH, CRITICAL
   - EventType enum: TASK_ASSIGNED, TASK_COMPLETED, TASK_FAILED, HANDOFF_REQUEST, HANDOFF_VALIDATED, HANDOFF_REJECTED, CONTEXT_UPDATED, AGENT_REGISTERED, AGENT_HEALTH
   - AgentEvent Pydantic model with fields:
     - event_id: str (default uuid4)
     - version: str = "1.0"
     - event_type: EventType
     - timestamp: datetime (default utcnow)
     - tenant_id: str
     - priority: EventPriority = NORMAL
     - source_agent_id: str (LOCKED DECISION: agent ID in every event)
     - call_chain: list[str] (LOCKED DECISION: full call chain for traceability, e.g. ["user", "supervisor", "research_agent"])
     - data: dict[str, Any] = {} (LOCKED DECISION: small inline data)
     - context_refs: list[str] = [] (LOCKED DECISION: references to large data in shared context store)
     - correlation_id: str | None = None (groups related events)
     - parent_event_id: str | None = None (links to triggering event)
   - to_stream_dict() method: serializes all fields to dict[str, str] for Redis Streams (all values must be strings -- json.dumps for dicts/lists, comma-join for string lists, isoformat for datetime)
   - from_stream_dict() classmethod: deserializes from Redis Streams format back to AgentEvent
   - model_validator: call_chain must not be empty, source_agent_id must appear in call_chain

Follow existing code patterns: use `from __future__ import annotations`, structlog for logging, docstrings matching Phase 1 style.
  </action>
  <verify>
Run: `python -c "from src.app.events.schemas import AgentEvent, EventType, EventPriority; e = AgentEvent(event_type=EventType.TASK_ASSIGNED, tenant_id='test', source_agent_id='agent1', call_chain=['agent1']); d = e.to_stream_dict(); e2 = AgentEvent.from_stream_dict(d); assert e.event_id == e2.event_id; print('Schema OK')"` succeeds.
Run: `pip install -e ".[dev]"` succeeds with new dependencies.
  </verify>
  <done>AgentEvent model validates source attribution and call chains, serializes/deserializes to Redis Streams format, and all Phase 2 dependencies are installed.</done>
</task>

<task type="auto">
  <name>Task 2: Build TenantEventBus, EventConsumer, and DeadLetterQueue</name>
  <files>src/app/events/bus.py, src/app/events/consumer.py, src/app/events/dlq.py, tests/test_events.py</files>
  <action>
1. Create src/app/events/bus.py with TenantEventBus class:
   - __init__(self, redis: aioredis.Redis, tenant_id: str) -- takes raw redis client and tenant_id (NOT using TenantRedis wrapper because Streams need their own key pattern)
   - _stream_key(self, stream: str) -> str -- returns "t:{tenant_id}:events:{stream}"
   - async publish(self, stream: str, event: AgentEvent) -> str -- validates event.tenant_id matches self._tenant_id, calls event.to_stream_dict(), xadd to stream, returns message_id. Uses MAXLEN ~1000 for stream trimming (approximate).
   - async subscribe(self, stream: str, group: str, consumer: str, count: int = 10, block: int = 5000) -> list -- creates consumer group if not exists (catch ResponseError), xreadgroup with ">" for new messages
   - async ack(self, stream: str, group: str, message_id: str) -> None -- xack
   - async get_stream_info(self, stream: str) -> dict -- xinfo_stream for monitoring
   - async get_pending(self, stream: str, group: str) -> list -- xpending for monitoring backlog

2. Create src/app/events/consumer.py with EventConsumer class:
   - MAX_RETRIES = 3
   - RETRY_DELAYS = [1, 4, 16] (exponential backoff: 1s, 4s, 16s -- LOCKED DECISION)
   - __init__(self, bus: TenantEventBus, stream: str, group: str, consumer_name: str, dlq: DeadLetterQueue)
   - async process_loop(self, handler: Callable[[AgentEvent], Awaitable[None]]) -> None -- main loop: subscribe, deserialize each message via AgentEvent.from_stream_dict(), call handler, ack on success, retry on failure
   - async _process_with_retry(self, message_id, raw_data, handler) -- try handler, on failure check retry count: if >= MAX_RETRIES, send to DLQ and ack original; else sleep with backoff delay and re-publish with incremented _retry_count
   - async reclaim_abandoned(self, idle_time_ms: int = 60000) -> list -- xautoclaim for dead consumer recovery

3. Create src/app/events/dlq.py with DeadLetterQueue class:
   - __init__(self, redis: aioredis.Redis, tenant_id: str)
   - _dlq_key(self, original_stream: str) -> str -- returns "t:{tenant_id}:events:{original_stream}:dlq"
   - async send_to_dlq(self, original_stream: str, message_id: str, data: dict, error: str, retry_count: int) -- xadd to DLQ stream with original metadata + error + retry_count + timestamp
   - async list_dlq_messages(self, original_stream: str, count: int = 50) -> list -- xrange to read DLQ messages for review
   - async replay_message(self, original_stream: str, dlq_message_id: str) -> str -- read from DLQ, re-publish to original stream with reset retry count, delete from DLQ

4. Create tests/test_events.py with:
   - Test AgentEvent creation, validation (missing call_chain fails, source_agent_id not in call_chain fails)
   - Test to_stream_dict / from_stream_dict roundtrip
   - Test TenantEventBus tenant_id mismatch raises ValueError
   - Test EventConsumer retry count tracking (mock handler that fails)
   - Use pytest-asyncio, mock Redis where needed (or use real Redis if available)
   - At least 8 test cases covering happy path and error cases
  </action>
  <verify>
Run: `python -m pytest tests/test_events.py -v` -- all tests pass.
Run: `python -c "from src.app.events import TenantEventBus, EventConsumer, DeadLetterQueue; print('Imports OK')"` succeeds.
  </verify>
  <done>TenantEventBus publishes/subscribes to tenant-scoped Redis Streams, EventConsumer retries with exponential backoff then DLQs failed messages, DeadLetterQueue supports review and replay, and all tests pass.</done>
</task>

</tasks>

<verification>
1. All event classes import cleanly: `from src.app.events import AgentEvent, EventType, EventPriority, TenantEventBus, EventConsumer, DeadLetterQueue`
2. AgentEvent validates source attribution (source_agent_id must be in call_chain)
3. Stream keys follow tenant isolation pattern: t:{tenant_id}:events:{stream}
4. EventConsumer retries exactly 3 times with delays [1, 4, 16] seconds before DLQ
5. All tests in tests/test_events.py pass
6. `pip install -e ".[dev]"` succeeds with all Phase 2 dependencies
</verification>

<success_criteria>
- AgentEvent model enforces source attribution and call chain integrity
- TenantEventBus uses tenant-scoped Redis Streams with MAXLEN trimming
- EventConsumer implements exponential backoff retry (3 attempts) then DLQ
- Dead letter queue supports message review and replay
- Tests validate schema roundtrip, tenant isolation, retry logic, and DLQ flow
- All Phase 2 dependencies installed in pyproject.toml
</success_criteria>

<output>
After completion, create `.planning/phases/02-agent-orchestration/02-01-SUMMARY.md`
</output>
