---
phase: 04.1-agent-learning-performance-feedback
plan: 03
type: execute
wave: 3
depends_on: ["04.1-01", "04.1-02"]
files_modified:
  - src/app/learning/analytics.py
  - src/app/learning/scheduler.py
  - src/app/api/v1/learning.py
  - src/app/api/v1/router.py
  - src/app/main.py
  - tests/test_learning_integration.py
autonomous: true

must_haves:
  truths:
    - "Sales reps can view their individual agent performance metrics via API"
    - "Managers can view team-level analytics with comparative performance"
    - "Executives can view strategic ROI metrics and agent effectiveness"
    - "Human feedback can be submitted via POST /api/v1/learning/feedback endpoint"
    - "Calibration curves are queryable per action type via API"
    - "Real-time analytics updates stream via SSE endpoint"
    - "Background scheduler runs time-windowed outcome signal detection at configured intervals"
    - "All learning components are initialized in main.py lifespan with failure tolerance"
  artifacts:
    - path: "src/app/learning/analytics.py"
      provides: "AnalyticsService with role-based dashboard views"
      contains: "class AnalyticsService"
    - path: "src/app/learning/scheduler.py"
      provides: "Scheduler configuration for background outcome detection"
      contains: "setup_learning_scheduler"
    - path: "src/app/api/v1/learning.py"
      provides: "REST API + SSE endpoints for learning/feedback/analytics"
      contains: "router = APIRouter"
    - path: "src/app/api/v1/router.py"
      provides: "Updated router including learning endpoints"
      contains: "learning"
    - path: "src/app/main.py"
      provides: "Phase 4.1 learning system initialization in lifespan"
      contains: "phase4_1"
    - path: "tests/test_learning_integration.py"
      provides: "Integration tests proving full learning pipeline"
  key_links:
    - from: "src/app/api/v1/learning.py"
      to: "src/app/learning/feedback.py"
      via: "FeedbackCollector for feedback submission endpoint"
      pattern: "FeedbackCollector"
    - from: "src/app/api/v1/learning.py"
      to: "src/app/learning/analytics.py"
      via: "AnalyticsService for dashboard endpoints"
      pattern: "AnalyticsService"
    - from: "src/app/api/v1/learning.py"
      to: "src/app/learning/calibration.py"
      via: "CalibrationEngine for calibration curve endpoints"
      pattern: "CalibrationEngine"
    - from: "src/app/main.py"
      to: "src/app/learning/scheduler.py"
      via: "Scheduler setup in lifespan"
      pattern: "setup_learning_scheduler"
    - from: "src/app/api/v1/router.py"
      to: "src/app/api/v1/learning.py"
      via: "Router inclusion"
      pattern: "learning.router"
---

<objective>
Wire the learning system into the application: AnalyticsService computes role-based performance metrics, scheduler runs background outcome detection, API endpoints expose feedback/analytics/calibration/SSE, and main.py initializes all learning components with failure tolerance.

Purpose: This plan connects all the learning infrastructure (plans 01+02) to the running application, making it accessible via API and operational via background scheduling. Without this wiring, the learning system exists but cannot be used.

Output: AnalyticsService, scheduler setup, 8+ API endpoints, main.py integration, SSE streaming, and integration tests.
</objective>

<execution_context>
@/Users/RAZER/.claude/get-shit-done/workflows/execute-plan.md
@/Users/RAZER/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/04.1-agent-learning-performance-feedback/04.1-CONTEXT.md
@.planning/phases/04.1-agent-learning-performance-feedback/04.1-RESEARCH.md
@.planning/phases/04.1-agent-learning-performance-feedback/04.1-01-SUMMARY.md
@.planning/phases/04.1-agent-learning-performance-feedback/04.1-02-SUMMARY.md
@src/app/learning/models.py
@src/app/learning/schemas.py
@src/app/learning/outcomes.py
@src/app/learning/feedback.py
@src/app/learning/calibration.py
@src/app/learning/coaching.py
@src/app/api/v1/sales.py
@src/app/api/v1/router.py
@src/app/main.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create AnalyticsService, scheduler, and API endpoints</name>
  <files>
    src/app/learning/analytics.py
    src/app/learning/scheduler.py
    src/app/api/v1/learning.py
  </files>
  <action>
**src/app/learning/analytics.py** -- AnalyticsService with role-based views:

```python
class AnalyticsService:
    """Role-based analytics computation from outcome/feedback/calibration data.

    Provides three dashboard views matching CONTEXT.md locked decisions:
    - Sales reps: Individual agent performance
    - Managers: Team-level trends and coaching opportunities
    - Executives: Strategic ROI and effectiveness metrics

    Uses pre-computed aggregates where possible (cached in Redis with 5-min TTL
    per RESEARCH.md Pitfall 4 recommendation).
    """

    CACHE_TTL = 300  # 5 minutes

    def __init__(self, session_factory, outcome_tracker, feedback_collector, calibration_engine, coaching_extractor, redis_client=None):
        """Accept all learning services for cross-component analytics."""

    async def get_rep_dashboard(
        self, tenant_id: str, rep_id: str | None = None, days: int = 30
    ) -> dict:
        """Individual agent performance for a sales rep.

        Returns:
        {
            "role": "rep",
            "period_days": days,
            "response_rates": {
                "total_actions": int,
                "positive_outcomes": int,
                "negative_outcomes": int,
                "pending_outcomes": int,
                "success_rate": float,  # positive / (positive + negative)
            },
            "escalation_history": [
                {"date": str, "trigger": str, "outcome": str, "conversation_id": str}
            ],
            "deal_impact": {
                "deals_progressed": int,
                "deals_stalled": int,
                "deals_closed_won": int,
                "stage_advancement_rate": float,
            },
            "feedback_scores": {
                "average_rating": float,
                "total_feedbacks": int,
                "recent_trend": str,  # "improving", "stable", "declining"
            },
            "calibration_summary": {
                "action_types": [
                    {"action_type": str, "brier_score": float, "is_calibrated": bool, "direction": str}
                ]
            },
            "generated_at": str,  # ISO datetime
        }
        """

    async def get_manager_dashboard(
        self, tenant_id: str, days: int = 30
    ) -> dict:
        """Team-level performance for managers.

        Returns:
        {
            "role": "manager",
            "period_days": days,
            "team_trends": {
                "total_outcomes": int,
                "success_rate": float,
                "trend": str,  # "improving", "stable", "declining"
                "daily_activity": [{"date": str, "actions": int, "positive": int}],
            },
            "comparative_performance": [
                {"action_type": str, "success_rate": float, "volume": int}
            ],
            "coaching_opportunities": [
                {"pattern": str, "recommendation": str, "confidence": float}
            ],
            "aggregate_calibration": {
                "overall_brier": float,
                "by_action_type": [{"type": str, "brier": float, "calibrated": bool}],
            },
            "escalation_rate": float,
            "feedback_health": {
                "submission_rate": float,
                "coverage_percent": float,
            },
            "generated_at": str,
        }
        """

    async def get_executive_summary(
        self, tenant_id: str, days: int = 30
    ) -> dict:
        """Strategic insights for executives.

        Returns:
        {
            "role": "executive",
            "period_days": days,
            "roi_metrics": {
                "total_actions": int,
                "positive_outcomes": int,
                "estimated_time_saved_hours": float,  # actions * avg_human_time
                "cost_per_interaction": float | None,  # from cost tracker if available
            },
            "agent_effectiveness": {
                "overall_success_rate": float,
                "qualification_completion_rate": float,
                "escalation_rate": float,
            },
            "engagement_trends": {
                "daily": [{"date": str, "actions": int, "success_rate": float}],
                "trend_direction": str,
            },
            "strategic_insights": [str],  # High-level pattern descriptions
            "generated_at": str,
        }
        """

    async def _get_cached_or_compute(self, cache_key: str, compute_fn, ttl: int = None):
        """Try Redis cache first, compute and cache on miss.

        Uses TenantRedis if available, falls back to direct computation.
        """
```

**src/app/learning/scheduler.py** -- Scheduler configuration:

```python
async def setup_learning_scheduler(
    outcome_tracker: OutcomeTracker,
    calibration_engine: CalibrationEngine,
    analytics_service: AnalyticsService,
) -> dict:
    """Configure background tasks for learning system.

    Returns a dict of async task functions that can be scheduled by
    APScheduler or run as asyncio tasks. This decouples task definition
    from scheduler configuration so tests can run tasks directly.

    Task definitions:
    1. check_immediate_signals: Every 15 min, check 24h window outcomes
    2. check_engagement_signals: Every 6 hours, check 7-day window outcomes
    3. check_deal_progression: Every 24 hours, check 30-day window outcomes
    4. expire_overdue_outcomes: Every hour, bulk-expire past-window outcomes
    5. run_calibration_check: Every 6 hours, check all action types for miscalibration

    Each task:
    - Wraps in try/except for resilience (individual failures don't crash scheduler)
    - Logs results via structlog
    - Returns count of items processed

    APScheduler integration pattern (for main.py):
    - If APScheduler is available, use AsyncScheduler with IntervalTrigger
    - If APScheduler not installed, fall back to simple asyncio.create_task
      with sleep loops (graceful degradation)
    """

    async def check_immediate_signals_task():
        """Check for immediate signals (email replies) -- runs every 15 min."""
        try:
            count = await outcome_tracker.check_immediate_signals()
            log.info("scheduler.immediate_signals_checked", resolved=count)
            return count
        except Exception:
            log.warning("scheduler.immediate_signals_failed", exc_info=True)
            return 0

    async def check_engagement_signals_task():
        """Check engagement signals -- runs every 6 hours."""
        # Similar pattern: calls outcome_tracker methods for meeting/escalation outcomes

    async def check_deal_progression_task():
        """Check deal progression signals -- runs every 24 hours."""
        # Similar pattern: calls outcome_tracker.check_deal_progression_signals()

    async def expire_overdue_task():
        """Expire overdue outcomes -- runs every hour."""
        try:
            count = await outcome_tracker.expire_overdue_outcomes()
            log.info("scheduler.outcomes_expired", count=count)
            return count
        except Exception:
            log.warning("scheduler.expire_failed", exc_info=True)
            return 0

    async def calibration_check_task():
        """Run calibration check across all action types -- runs every 6 hours."""
        try:
            action_types = await calibration_engine.get_all_action_types(tenant_id=None)
            adjustments = []
            for at_info in action_types:
                adj = await calibration_engine.check_and_adjust(
                    at_info["tenant_id"], at_info["action_type"]
                )
                if adj:
                    adjustments.append(adj)
            log.info("scheduler.calibration_checked", adjustments=len(adjustments))
            return adjustments
        except Exception:
            log.warning("scheduler.calibration_check_failed", exc_info=True)
            return []

    return {
        "check_immediate_signals": check_immediate_signals_task,
        "check_engagement_signals": check_engagement_signals_task,
        "check_deal_progression": check_deal_progression_task,
        "expire_overdue_outcomes": expire_overdue_task,
        "calibration_check": calibration_check_task,
    }


async def start_scheduler_background(tasks: dict, app_state) -> None:
    """Start scheduler tasks as background asyncio tasks.

    Tries APScheduler first (if installed). Falls back to simple
    asyncio background loops.

    APScheduler integration:
    - Uses AsyncScheduler with SQLAlchemyDataStore
    - IntervalTrigger for each task at configured intervals
    - ConflictPolicy.replace for idempotent schedule registration

    Fallback (no APScheduler):
    - asyncio.create_task with while True + asyncio.sleep loops
    - Stores task references on app_state for cleanup

    Intervals:
    - immediate_signals: 15 minutes
    - engagement_signals: 6 hours
    - deal_progression: 24 hours
    - expire_overdue: 1 hour
    - calibration_check: 6 hours
    """
```

**src/app/api/v1/learning.py** -- REST API + SSE endpoints:

Create FastAPI router with prefix `/learning`, tags `["learning"]`. Follow exact patterns from `src/app/api/v1/sales.py`.

Endpoints:

1. **POST /learning/feedback** - Submit feedback
   - Request: SubmitFeedbackRequest (from learning schemas)
   - Response: SubmitFeedbackResponse
   - Auth: get_current_user + get_tenant
   - Calls FeedbackCollector.record_feedback()

2. **GET /learning/feedback/{conversation_state_id}** - Get feedback for conversation
   - Response: list of feedback entries
   - Auth: get_current_user + get_tenant

3. **GET /learning/outcomes/{conversation_state_id}** - Get outcomes for conversation
   - Response: list of OutcomeRecordResponse
   - Auth: get_current_user + get_tenant

4. **GET /learning/calibration/{action_type}** - Get calibration curve
   - Response: CalibrationCurveResponse
   - Auth: get_current_user + get_tenant
   - Calls CalibrationEngine.get_calibration_curve()

5. **GET /learning/analytics/rep** - Rep dashboard
   - Query params: days (int, default 30)
   - Response: AnalyticsDashboardResponse
   - Auth: get_current_user + get_tenant
   - Calls AnalyticsService.get_rep_dashboard()

6. **GET /learning/analytics/manager** - Manager dashboard
   - Query params: days (int, default 30)
   - Response: AnalyticsDashboardResponse
   - Auth: get_current_user + get_tenant
   - Calls AnalyticsService.get_manager_dashboard()

7. **GET /learning/analytics/executive** - Executive dashboard
   - Query params: days (int, default 30)
   - Response: AnalyticsDashboardResponse
   - Auth: get_current_user + get_tenant
   - Calls AnalyticsService.get_executive_summary()

8. **GET /learning/coaching/patterns** - Coaching patterns
   - Query params: days (int, default 90)
   - Response: list of coaching patterns
   - Auth: get_current_user + get_tenant

9. **GET /learning/analytics/stream** - SSE real-time stream
   - Uses sse-starlette EventSourceResponse
   - Subscribes to Redis pub/sub channel `t:{tenant_id}:analytics`
   - Yields metric_update events as JSON
   - Handles client disconnect gracefully
   - Falls back to polling if Redis pub/sub unavailable

Dependency injection helpers (following _get_sales_agent pattern):
- `_get_outcome_tracker()` - returns app.state.outcome_tracker, 503 if not available
- `_get_feedback_collector()` - returns app.state.feedback_collector, 503 if not available
- `_get_calibration_engine()` - returns app.state.calibration_engine, 503 if not available
- `_get_analytics_service()` - returns app.state.analytics_service, 503 if not available
- `_get_coaching_extractor()` - returns app.state.coaching_extractor, 503 if not available

Install sse-starlette: `pip install sse-starlette` (new dependency from RESEARCH.md).
Import: `from sse_starlette.sse import EventSourceResponse`

For the SSE endpoint, handle graceful fallback if sse-starlette is not installed:
```python
try:
    from sse_starlette.sse import EventSourceResponse
    SSE_AVAILABLE = True
except ImportError:
    SSE_AVAILABLE = False
```
If not available, the /analytics/stream endpoint returns 501 Not Implemented.
  </action>
  <verify>
Run `python -c "from src.app.learning.analytics import AnalyticsService; print('Analytics OK')"` succeeds.
Run `python -c "from src.app.learning.scheduler import setup_learning_scheduler; print('Scheduler OK')"` succeeds.
Run `python -c "from src.app.api.v1.learning import router; print('Learning API OK')"` succeeds.
  </verify>
  <done>AnalyticsService computes role-based dashboards (rep/manager/executive). Scheduler configures 5 background tasks with APScheduler or asyncio fallback. 9 API endpoints expose feedback, outcomes, calibration, analytics, coaching, and SSE streaming. All dependency injection follows established patterns.</done>
</task>

<task type="auto">
  <name>Task 2: Wire learning system into main.py and create integration tests</name>
  <files>
    src/app/api/v1/router.py
    src/app/main.py
    tests/test_learning_integration.py
  </files>
  <action>
**src/app/api/v1/router.py** -- Add learning router import:

Add `from src.app.api.v1 import learning` to the imports (alongside auth, health, llm, sales, tenants).
Add `router.include_router(learning.router)` after the sales router line.

**src/app/main.py** -- Add Phase 4.1 learning system initialization:

After the Phase 4 sales agent block (before `yield`), add a new Phase 4.1 block following the exact per-module try/except pattern from Phase 2 and Phase 4:

```python
    # ── Phase 4.1: Agent Learning & Performance Feedback ─────────────
    # All Phase 4.1 init is additive and failure-tolerant. Each component
    # is wrapped in its own try/except so a single failure (e.g., numpy
    # not installed, APScheduler missing) does not prevent startup.

    try:
        from src.app.learning.outcomes import OutcomeTracker
        from src.app.learning.feedback import FeedbackCollector
        from src.app.learning.calibration import CalibrationEngine
        from src.app.learning.coaching import CoachingPatternExtractor
        from src.app.learning.analytics import AnalyticsService
        from src.app.learning.scheduler import setup_learning_scheduler, start_scheduler_background
        from src.app.core.database import get_tenant_session

        outcome_tracker = OutcomeTracker(session_factory=get_tenant_session)
        feedback_collector = FeedbackCollector(session_factory=get_tenant_session)
        calibration_engine = CalibrationEngine(session_factory=get_tenant_session)
        coaching_extractor = CoachingPatternExtractor(session_factory=get_tenant_session)

        redis_client = getattr(app.state, "redis_client", None) or get_redis_pool()
        analytics_service = AnalyticsService(
            session_factory=get_tenant_session,
            outcome_tracker=outcome_tracker,
            feedback_collector=feedback_collector,
            calibration_engine=calibration_engine,
            coaching_extractor=coaching_extractor,
            redis_client=redis_client,
        )

        # Store on app.state for API endpoint dependency injection
        app.state.outcome_tracker = outcome_tracker
        app.state.feedback_collector = feedback_collector
        app.state.calibration_engine = calibration_engine
        app.state.coaching_extractor = coaching_extractor
        app.state.analytics_service = analytics_service

        # Start background scheduler tasks
        scheduler_tasks = await setup_learning_scheduler(
            outcome_tracker=outcome_tracker,
            calibration_engine=calibration_engine,
            analytics_service=analytics_service,
        )
        await start_scheduler_background(scheduler_tasks, app.state)

        log.info("phase4_1.learning_system_initialized")
    except Exception as exc:
        log.warning("phase4_1.learning_system_init_failed", error=str(exc))
        # Set all to None for graceful 503 responses from API
        app.state.outcome_tracker = None
        app.state.feedback_collector = None
        app.state.calibration_engine = None
        app.state.coaching_extractor = None
        app.state.analytics_service = None
```

In the shutdown section (after yield), add cleanup for any scheduler background tasks:
```python
    # Clean up Phase 4.1 scheduler tasks
    scheduler_tasks_refs = getattr(app.state, "learning_scheduler_tasks", None)
    if scheduler_tasks_refs:
        for task_ref in scheduler_tasks_refs:
            task_ref.cancel()
```

**tests/test_learning_integration.py** -- Integration tests:

Follow the exact pattern from tests/test_sales_integration.py (InMemory repositories, mocked external services, real domain logic). Create in-memory test doubles:

- `InMemoryOutcomeTracker` - Stores outcomes in a dict, implements record/resolve/query/expire/check methods
- `InMemoryFeedbackCollector` - Stores feedback in a list, implements record/query/summary
- `InMemoryCalibrationEngine` - Stores bins in a dict, implements update/curve/check_and_adjust
- `InMemoryCoachingExtractor` - Returns canned patterns
- `InMemoryAnalyticsService` - Delegates to in-memory sub-services

Tests:

1. **test_record_and_resolve_outcome** - Record outcome, verify pending. Resolve it, verify status changed.
2. **test_outcome_expiry_lifecycle** - Record outcome with short window, expire it, verify EXPIRED status.
3. **test_feedback_submission_inline** - Submit inline feedback (-1/0/1), verify recorded.
4. **test_feedback_submission_dashboard** - Submit dashboard feedback (1-5), verify recorded.
5. **test_feedback_linked_to_outcome** - Submit feedback with outcome_record_id, verify link.
6. **test_calibration_update_and_curve** - Update calibration with multiple outcomes, get curve, verify shape.
7. **test_calibration_brier_score** - Feed known predictions and outcomes, verify Brier score matches expected.
8. **test_miscalibration_auto_adjust** - Create overconfident calibration, run check_and_adjust, verify adjustment returned.
9. **test_analytics_rep_dashboard** - Call get_rep_dashboard, verify structure and required fields.
10. **test_analytics_manager_dashboard** - Call get_manager_dashboard, verify structure.
11. **test_analytics_executive_summary** - Call get_executive_summary, verify structure.
12. **test_coaching_patterns_extraction** - Create outcomes with patterns, extract, verify non-empty.
13. **test_full_learning_pipeline** - End-to-end: record outcome -> submit feedback -> update calibration -> get analytics. Verify all components work together.
14. **test_scheduler_tasks_defined** - Call setup_learning_scheduler, verify all 5 task functions returned.
15. **test_learning_api_feedback_endpoint** - Test POST /learning/feedback returns 200 with valid data (mock services).
16. **test_learning_api_analytics_endpoint** - Test GET /learning/analytics/rep returns 200 (mock services).

All tests use `@pytest.mark.asyncio`. For API endpoint tests, use FastAPI TestClient with mocked app.state attributes.
  </action>
  <verify>
Run `python -m pytest tests/test_learning_integration.py -v` -- all tests pass.
Run `python -m pytest tests/ -v --tb=short` -- full suite passes, no regressions.
Run `python -c "from src.app.api.v1.router import router; print('Router OK')"` -- confirms learning router included.
  </verify>
  <done>Learning system fully wired: main.py initializes all 5 learning components with failure tolerance and background scheduler. Learning API router mounted at /learning with 9 endpoints. 16+ integration tests prove full pipeline from outcome recording through feedback, calibration, analytics, and coaching. Full test suite passes with no regressions.</done>
</task>

</tasks>

<verification>
- `python -c "from src.app.learning.analytics import AnalyticsService"` succeeds
- `python -c "from src.app.learning.scheduler import setup_learning_scheduler"` succeeds
- `python -c "from src.app.api.v1.learning import router"` succeeds
- `python -c "from src.app.api.v1.router import router"` succeeds (includes learning)
- `python -m pytest tests/test_learning_integration.py -v` -- all pass
- `python -m pytest tests/ -v --tb=short` -- full suite passes, no regressions
</verification>

<success_criteria>
1. AnalyticsService provides role-based dashboards for rep, manager, and executive
2. Scheduler defines 5 background tasks with correct intervals
3. 9 API endpoints operational under /learning prefix
4. SSE streaming endpoint available (or 501 if sse-starlette not installed)
5. main.py initializes all learning components with failure tolerance
6. Learning router included in v1 router
7. Dependency injection follows established _get_sales_agent() pattern
8. Background scheduler tasks run with resilience (individual task failure doesn't crash)
9. All integration tests pass, full suite passes with no regressions
</success_criteria>

<output>
After completion, create `.planning/phases/04.1-agent-learning-performance-feedback/04.1-03-SUMMARY.md`
</output>
