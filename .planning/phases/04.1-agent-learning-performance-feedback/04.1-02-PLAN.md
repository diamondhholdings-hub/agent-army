---
phase: 04.1-agent-learning-performance-feedback
plan: 02
type: execute
wave: 2
depends_on: ["04.1-01"]
files_modified:
  - src/app/learning/feedback.py
  - src/app/learning/calibration.py
  - src/app/learning/coaching.py
  - tests/test_learning_feedback_calibration.py
autonomous: true

must_haves:
  truths:
    - "Sales reps and managers can submit feedback on agent messages, decisions, and conversations with rating and optional comment"
    - "Feedback is persisted with reviewer identity, role, target type, and links to the relevant outcome record"
    - "Per-action-type calibration bins track predicted confidence vs actual success rate with 10 bins from 0.0 to 1.0"
    - "Calibration engine computes Brier score and detects miscalibration when gap exceeds threshold"
    - "Auto-adjustment applies damped corrections (max 10% per cycle) with hard bounds on scaling factors"
    - "Coaching patterns are extracted from outcome and feedback data as statistical correlations"
  artifacts:
    - path: "src/app/learning/feedback.py"
      provides: "FeedbackCollector service for recording and querying feedback"
      contains: "class FeedbackCollector"
    - path: "src/app/learning/calibration.py"
      provides: "CalibrationEngine with per-action-type calibration, Brier score, and auto-adjustment"
      contains: "class CalibrationEngine"
    - path: "src/app/learning/coaching.py"
      provides: "CoachingPatternExtractor for identifying training insights from outcomes"
      contains: "class CoachingPatternExtractor"
    - path: "tests/test_learning_feedback_calibration.py"
      provides: "Tests for feedback collection, calibration engine, and coaching extraction"
  key_links:
    - from: "src/app/learning/feedback.py"
      to: "src/app/learning/models.py"
      via: "FeedbackEntryModel for database operations"
      pattern: "from src.app.learning.models import FeedbackEntryModel"
    - from: "src/app/learning/calibration.py"
      to: "src/app/learning/models.py"
      via: "CalibrationBinModel and OutcomeRecordModel for calibration data"
      pattern: "from src.app.learning.models import CalibrationBinModel"
    - from: "src/app/learning/calibration.py"
      to: "numpy"
      via: "Brier score and calibration curve computation"
      pattern: "import numpy as np"
---

<objective>
Create the feedback collection, confidence calibration, and coaching pattern extraction services -- the three core learning components that transform raw outcome data into actionable intelligence.

Purpose: FeedbackCollector enables human-in-the-loop learning; CalibrationEngine makes the agent's confidence scores trustworthy; CoachingPatternExtractor turns agent data into human sales training. Together they close the learning loop from Phase 4's Sales Agent Core.

Output: Three service classes with full business logic, numpy-based calibration math, and comprehensive tests.
</objective>

<execution_context>
@/Users/RAZER/.claude/get-shit-done/workflows/execute-plan.md
@/Users/RAZER/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/04.1-agent-learning-performance-feedback/04.1-CONTEXT.md
@.planning/phases/04.1-agent-learning-performance-feedback/04.1-RESEARCH.md
@.planning/phases/04.1-agent-learning-performance-feedback/04.1-01-SUMMARY.md
@src/app/learning/models.py
@src/app/learning/schemas.py
@src/app/learning/outcomes.py
@src/app/agents/sales/state_repository.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create FeedbackCollector and CalibrationEngine services</name>
  <files>
    src/app/learning/feedback.py
    src/app/learning/calibration.py
  </files>
  <action>
**src/app/learning/feedback.py** -- FeedbackCollector service:

```python
class FeedbackCollector:
    """Collects and manages human feedback on agent behavior.

    Supports both inline reactions (quick thumbs up/down) and detailed
    dashboard reviews. Links feedback to outcome records for calibration.
    """

    def __init__(self, session_factory):
        """Accept session_factory callable (same pattern as ConversationStateRepository)."""

    async def record_feedback(
        self,
        tenant_id: str,
        conversation_state_id: str,
        target_type: str,  # "message", "decision", "conversation"
        target_id: str,
        source: str,  # "inline", "dashboard"
        rating: int,  # -1/0/1 for inline; 1-5 for dashboard
        reviewer_id: str,
        reviewer_role: str,  # "rep", "manager", "executive"
        outcome_record_id: str | None = None,
        comment: str | None = None,
        metadata: dict | None = None,
    ) -> FeedbackEntry:
        """Record feedback and optionally link to an outcome record.

        Validates:
        - rating range: -1 to 1 for inline source, 1 to 5 for dashboard source
        - target_type is one of MESSAGE, DECISION, CONVERSATION
        - source is one of INLINE, DASHBOARD

        Creates FeedbackEntryModel in database. If outcome_record_id is provided,
        also triggers calibration update via the outcome resolution (human_label signal).
        """

    async def get_feedback_for_conversation(
        self,
        tenant_id: str,
        conversation_state_id: str,
    ) -> list[FeedbackEntry]:
        """Get all feedback entries for a conversation."""

    async def get_feedback_by_reviewer(
        self,
        tenant_id: str,
        reviewer_id: str,
        limit: int = 50,
    ) -> list[FeedbackEntry]:
        """Get recent feedback from a specific reviewer."""

    async def get_feedback_summary(
        self,
        tenant_id: str,
        days: int = 30,
    ) -> dict:
        """Compute feedback summary metrics for a tenant.

        Returns dict with:
        - total_feedback_count: int
        - average_rating: float
        - rating_distribution: dict[int, int] (rating -> count)
        - by_target_type: dict[str, dict] (target_type -> {count, avg_rating})
        - by_source: dict[str, int] (source -> count)
        - by_reviewer_role: dict[str, dict] (role -> {count, avg_rating})
        - feedback_rate_trend: list[dict] (daily feedback counts for trend)
        """

    async def get_feedback_rate(
        self,
        tenant_id: str,
        reviewer_id: str,
    ) -> float:
        """Get feedback submission rate for fatigue detection (Pitfall 3).

        Returns feedbacks per day over the last 7 days. Declining rates
        signal feedback fatigue.
        """
```

**src/app/learning/calibration.py** -- CalibrationEngine service:

```python
import numpy as np

class CalibrationEngine:
    """Per-action-type confidence calibration using binned averages.

    Maintains 10 calibration bins per action type, tracking predicted
    confidence vs actual success rate. Computes Brier score for
    overall calibration quality. Auto-adjusts agent behavior when
    miscalibration detected.

    LOCKED decisions from CONTEXT.md:
    - Per-action-type calibration (separate curves for each action type)
    - Continuous updates (every outcome updates calibration)
    - Auto-adjust behavior when miscalibration detected
    """

    N_BINS = 10
    BIN_EDGES = np.linspace(0.0, 1.0, 11)  # 11 edges = 10 bins
    MIN_SAMPLES_PER_BIN = 10  # Pitfall 2: cold start protection
    MISCALIBRATION_THRESHOLD = 0.15  # >15% gap triggers adjustment
    MAX_ADJUSTMENT_RATE = 0.10  # Pitfall 5: max 10% correction per cycle
    SCALING_BOUNDS = (0.5, 1.5)  # Hard bounds on confidence scaling
    ESCALATION_BOUNDS = (0.3, 0.9)  # Hard bounds on escalation thresholds

    def __init__(self, session_factory):
        """Accept session_factory callable."""

    async def initialize_bins(self, tenant_id: str, action_type: str) -> None:
        """Create initial 10 calibration bins for an action type if they don't exist.

        Idempotent -- checks for existing bins before creating.
        Creates bins with bin_lower/bin_upper from BIN_EDGES.
        """

    async def update_calibration(
        self,
        tenant_id: str,
        action_type: str,
        predicted_confidence: float,
        actual_outcome: bool,  # True = positive, False = negative
    ) -> None:
        """Update calibration bin for this action type (continuous update).

        1. Find the correct bin for predicted_confidence using np.digitize
        2. Increment sample_count
        3. Add to outcome_sum (1.0 if positive, 0.0 if negative)
        4. Recompute actual_rate = outcome_sum / sample_count
        5. Recompute brier_contribution = (predicted_midpoint - actual_rate)^2
        6. Persist to database

        Uses SELECT ... FOR UPDATE to prevent concurrent bin updates.
        """

    async def get_calibration_curve(
        self, tenant_id: str, action_type: str
    ) -> CalibrationCurve:
        """Return calibration curve data for an action type.

        Loads all bins, filters to those with sample_count >= MIN_SAMPLES_PER_BIN.
        Returns CalibrationCurve with midpoints, actual_rates, counts, brier_score.
        """

    async def compute_brier_score(
        self, tenant_id: str, action_type: str
    ) -> float:
        """Compute overall Brier score from calibration bins.

        Brier = weighted mean of per-bin (midpoint - actual_rate)^2,
        weighted by sample_count. Returns 0.25 (random guessing baseline)
        if no data.

        Uses numpy: np.average(gaps**2, weights=counts)
        """

    async def check_and_adjust(
        self, tenant_id: str, action_type: str
    ) -> CalibrationAdjustment | None:
        """Check calibration and recommend/apply damped adjustment.

        For each bin with enough samples:
        - gap = predicted_midpoint - actual_rate
        - If |gap| > MISCALIBRATION_THRESHOLD: flag

        If overconfident (predicted > actual): reduce confidence scaling
        If underconfident (predicted < actual): increase confidence scaling

        Apply damped adjustment: adjust by min(gap, MAX_ADJUSTMENT_RATE).
        Clamp to SCALING_BOUNDS and ESCALATION_BOUNDS.

        Returns CalibrationAdjustment if adjustment applied, None if calibrated.
        """

    async def get_all_action_types(self, tenant_id: str) -> list[str]:
        """Get all action types that have calibration bins for this tenant."""

    @staticmethod
    def brier_score(predicted: np.ndarray, actual: np.ndarray) -> float:
        """Compute Brier score from arrays. Lower is better. 0.0=perfect, 0.25=random."""
        if len(predicted) == 0:
            return 0.25
        return float(np.mean((predicted - actual) ** 2))

    @staticmethod
    def calibration_curve_from_arrays(
        predicted: np.ndarray, actual: np.ndarray, n_bins: int = 10
    ) -> dict:
        """Compute calibration curve from raw prediction/outcome arrays.

        Returns dict with midpoints, actual_rates, counts, brier_score.
        Used for bulk computation (e.g., from outcome records directly).
        """
```

Key implementation details:
- Use `numpy.digitize()` for bin assignment
- Use `numpy.linspace()` for bin edges
- Use `numpy.average()` with weights for weighted Brier score
- All database operations use session_factory pattern from ConversationStateRepository
- Use `SELECT ... FOR UPDATE` on bin rows during update to prevent concurrent modification
- `initialize_bins` is idempotent and should be called on first outcome for any new action_type
  </action>
  <verify>
Run `python -c "from src.app.learning.feedback import FeedbackCollector; print('Feedback OK')"` succeeds.
Run `python -c "from src.app.learning.calibration import CalibrationEngine; print('Calibration OK')"` succeeds.
Run `python -c "import numpy as np; print('numpy', np.__version__)"` succeeds (confirm numpy available).
  </verify>
  <done>FeedbackCollector records and queries human feedback with validation and summary metrics. CalibrationEngine maintains per-action-type calibration bins, computes Brier score, and applies damped auto-adjustments with cold-start protection and instability guards.</done>
</task>

<task type="auto">
  <name>Task 2: Create CoachingPatternExtractor and comprehensive tests</name>
  <files>
    src/app/learning/coaching.py
    tests/test_learning_feedback_calibration.py
  </files>
  <action>
**src/app/learning/coaching.py** -- CoachingPatternExtractor service:

```python
class CoachingPatternExtractor:
    """Identifies coaching patterns from outcome and feedback data.

    Extracts statistical correlations between agent action attributes
    and outcomes to provide actionable sales training insights.
    Success Criterion 5: "turning AI insights into human coaching."

    Starts with statistical patterns (RESEARCH.md recommendation),
    not LLM-based analysis. Correlations include:
    - Time-of-day impact on email response rates
    - Persona type effectiveness per deal stage
    - Escalation patterns that led to positive vs negative outcomes
    - Deal stage where most qualification signals are gathered
    - Channel preference correlations with outcome success
    """

    def __init__(self, session_factory):
        """Accept session_factory callable."""

    async def extract_patterns(
        self, tenant_id: str, days: int = 90
    ) -> list[CoachingPattern]:
        """Extract coaching patterns from recent outcome data.

        Queries resolved outcomes and feedback, computes correlations,
        and returns ranked patterns by statistical significance.

        Pattern extraction pipeline:
        1. Load resolved outcomes for the period
        2. Group by dimensions (action_type, outcome_type, time_of_day, persona, etc.)
        3. Compute success rates per group
        4. Compare groups to identify significant differences
        5. Format as CoachingPattern objects with human-readable insights

        Returns list of CoachingPattern sorted by confidence descending.
        """

    async def get_escalation_patterns(
        self, tenant_id: str, days: int = 90
    ) -> list[CoachingPattern]:
        """Identify patterns in escalation outcomes.

        Analyzes: which escalation triggers led to best outcomes?
        Which deal stages have highest escalation-to-resolution success?
        """

    async def get_top_performing_actions(
        self, tenant_id: str, days: int = 90, top_k: int = 5
    ) -> list[dict]:
        """Identify the most effective action types by success rate.

        Returns top_k action types with highest positive outcome ratio,
        filtered to those with MIN_SAMPLES threshold.
        """

    async def get_improvement_areas(
        self, tenant_id: str, days: int = 90
    ) -> list[dict]:
        """Identify areas where the agent underperforms.

        Finds action types or scenarios with low success rates,
        high escalation rates, or poor human feedback scores.
        Used for targeted training recommendations.
        """
```

Add `CoachingPattern` to schemas.py (if not already there -- add to the file):
```python
class CoachingPattern(BaseModel):
    """A coaching insight extracted from outcome/feedback data."""
    pattern_id: str
    pattern_type: str  # "time_correlation", "persona_effectiveness", "escalation_pattern", "channel_preference", "stage_insight"
    description: str  # Human-readable insight
    confidence: float  # Statistical confidence 0.0-1.0
    sample_size: int
    supporting_data: dict  # Raw data backing the pattern
    recommendation: str  # Actionable advice for sales reps
    created_at: datetime
```

**tests/test_learning_feedback_calibration.py** -- Comprehensive tests:

**Feedback tests:**
1. `test_record_inline_feedback` - Record inline feedback with rating -1/0/1, verify persisted
2. `test_record_dashboard_feedback` - Record dashboard feedback with rating 1-5, verify persisted
3. `test_feedback_rating_validation_inline` - Inline source rejects ratings outside -1 to 1
4. `test_feedback_rating_validation_dashboard` - Dashboard source rejects ratings outside 1 to 5
5. `test_get_feedback_for_conversation` - Create multiple feedbacks, query by conversation
6. `test_feedback_summary_computation` - Create varied feedback, verify summary metrics (avg rating, distribution, by target type)
7. `test_feedback_rate_for_fatigue` - Simulate declining feedback, verify rate computation

**Calibration tests:**
8. `test_initialize_bins` - Create bins for action type, verify 10 bins with correct edges
9. `test_initialize_bins_idempotent` - Call twice, verify no duplicate bins
10. `test_update_calibration_single` - Update one bin with a positive outcome, verify counts/rates
11. `test_update_calibration_multiple` - Multiple outcomes across bins, verify distribution
12. `test_brier_score_perfect` - Predicted == actual for all bins, Brier should be near 0.0
13. `test_brier_score_random` - Predicted all 0.5, outcomes 50/50, Brier should be near 0.25
14. `test_brier_score_static_method` - Test static brier_score with numpy arrays directly
15. `test_calibration_curve_from_arrays` - Test static calibration_curve_from_arrays
16. `test_miscalibration_detection` - Create overconfident bins (predicted 0.9, actual 0.5), verify check_and_adjust returns adjustment
17. `test_adjustment_damping` - Verify adjustment magnitude capped at MAX_ADJUSTMENT_RATE
18. `test_cold_start_protection` - Bins with < MIN_SAMPLES do not trigger adjustment
19. `test_scaling_bounds` - Verify adjustments clamped to SCALING_BOUNDS

**Coaching tests:**
20. `test_extract_patterns_with_data` - Create outcomes with patterns, verify patterns extracted
21. `test_top_performing_actions` - Create outcomes for multiple action types, verify ranking
22. `test_improvement_areas` - Create low-performing outcomes, verify identified

Use `pytest` with `@pytest.mark.asyncio`. Create lightweight in-memory test doubles for session_factory where needed (matching the InMemoryStateRepository pattern from test_sales_integration.py).
  </action>
  <verify>
Run `python -m pytest tests/test_learning_feedback_calibration.py -v` -- all tests pass.
Run `python -m pytest tests/ -v --tb=short` -- full suite passes, no regressions.
  </verify>
  <done>FeedbackCollector validates and persists feedback with summary metrics. CalibrationEngine maintains per-action-type bins with Brier score and damped auto-adjustment. CoachingPatternExtractor identifies statistical patterns for sales training. 22+ tests cover feedback validation, calibration math, cold start, damping, bounds, and pattern extraction. Full test suite passes.</done>
</task>

</tasks>

<verification>
- `python -c "from src.app.learning.feedback import FeedbackCollector"` succeeds
- `python -c "from src.app.learning.calibration import CalibrationEngine"` succeeds
- `python -c "from src.app.learning.coaching import CoachingPatternExtractor"` succeeds
- `python -m pytest tests/test_learning_feedback_calibration.py -v` -- all pass
- `python -m pytest tests/ -v --tb=short` -- full suite passes
</verification>

<success_criteria>
1. FeedbackCollector records inline and dashboard feedback with validation
2. Feedback summary metrics computed correctly (avg rating, distribution, trends)
3. CalibrationEngine maintains 10 bins per action type with continuous updates
4. Brier score computed correctly using numpy
5. Miscalibration detection triggers when gap > 15% threshold
6. Auto-adjustment applies damped corrections (max 10%) with hard bounds
7. Cold start protection prevents adjustments with insufficient data
8. CoachingPatternExtractor identifies patterns from outcome/feedback data
9. All new tests pass, existing test suite still passes
</success_criteria>

<output>
After completion, create `.planning/phases/04.1-agent-learning-performance-feedback/04.1-02-SUMMARY.md`
</output>
