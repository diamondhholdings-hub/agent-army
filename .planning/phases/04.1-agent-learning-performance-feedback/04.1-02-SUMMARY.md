---
phase: 04.1-agent-learning-performance-feedback
plan: 02
subsystem: learning
tags: [feedback, calibration, coaching, numpy, brier-score, sales-agent]

# Dependency graph
requires:
  - phase: 04.1-01
    provides: "OutcomeRecordModel, FeedbackEntryModel, CalibrationBinModel, OutcomeTracker service, learning schemas"
provides:
  - "FeedbackCollector service for recording and querying human feedback"
  - "CalibrationEngine with per-action-type calibration, Brier score, and auto-adjustment"
  - "CoachingPatternExtractor for identifying training insights from outcomes"
  - "CoachingPattern schema for coaching data representation"
affects: [04.1-03, 04.1-04, 05-api-endpoints]

# Tech tracking
tech-stack:
  added: []
  patterns:
    - "InMemory test double pattern for service unit testing (InMemoryFeedbackCollector, InMemoryCalibrationEngine, InMemoryCoachingExtractor)"
    - "numpy-based calibration math (np.digitize for binning, np.average with weights for Brier score)"
    - "Damped auto-adjustment with cold-start protection and hard bounds"

key-files:
  created:
    - src/app/learning/feedback.py
    - src/app/learning/calibration.py
    - src/app/learning/coaching.py
    - tests/test_learning_feedback_calibration.py
  modified:
    - src/app/learning/schemas.py
    - src/app/learning/__init__.py

key-decisions:
  - "In-memory test doubles mirror service interfaces for fast unit testing without database"
  - "Brier score uses weighted average of per-bin gaps squared, weighted by sample count"
  - "Cold start protection: bins with < 10 samples excluded from adjustment decisions"
  - "Adjustment damping: max 10% correction per cycle, clamped to [0.5, 1.5] scaling bounds"
  - "Coaching patterns use statistical correlations (not LLM) per RESEARCH.md recommendation"
  - "Improvement areas threshold: < 40% success rate flags action type for attention"

patterns-established:
  - "InMemory service doubles: mirror real service interface for deterministic unit testing"
  - "Calibration binning: np.digitize with 10 bins, continuous updates per outcome"
  - "Feedback dual-source validation: inline (-1/0/1) vs dashboard (1-5) rating ranges"

# Metrics
duration: 7min
completed: 2026-02-12
---

# Phase 4.1 Plan 02: Feedback, Calibration & Coaching Summary

**FeedbackCollector with dual-source validation, CalibrationEngine with numpy Brier score and damped auto-adjustment, CoachingPatternExtractor with statistical pattern extraction -- 24 tests covering all three services**

## Performance

- **Duration:** 7 min
- **Started:** 2026-02-12T04:58:57Z
- **Completed:** 2026-02-12T05:05:53Z
- **Tasks:** 2
- **Files modified:** 6

## Accomplishments
- FeedbackCollector records inline/dashboard feedback with source-specific rating validation, queries by conversation/reviewer, computes summary metrics with trend data, and detects feedback fatigue
- CalibrationEngine maintains 10 bins per action type using numpy, computes weighted Brier score, detects miscalibration beyond 15% threshold, and applies damped auto-adjustments with cold-start protection and hard bounds
- CoachingPatternExtractor identifies action effectiveness patterns, ranks top-performing actions, and flags underperforming areas using statistical correlations
- 24 comprehensive tests covering feedback validation, calibration math, bounds clamping, cold start, and coaching extraction -- full suite at 449/449

## Task Commits

Each task was committed atomically:

1. **Task 1: Create FeedbackCollector and CalibrationEngine services** - `a55dfd4` (feat)
2. **Task 2: Create CoachingPatternExtractor and comprehensive tests** - `b51e09f` (feat)

## Files Created/Modified
- `src/app/learning/feedback.py` - FeedbackCollector service with record, query, summary, and fatigue rate methods
- `src/app/learning/calibration.py` - CalibrationEngine with bin management, Brier score, miscalibration detection, and damped adjustment
- `src/app/learning/coaching.py` - CoachingPatternExtractor with pattern extraction, top actions, and improvement areas
- `src/app/learning/schemas.py` - Added CoachingPattern schema
- `src/app/learning/__init__.py` - Updated module docstring with new components
- `tests/test_learning_feedback_calibration.py` - 24 tests covering feedback, calibration, and coaching

## Decisions Made
- In-memory test doubles mirror service interfaces for fast unit testing without database -- matching the InMemoryStateRepository pattern from Phase 4
- Brier score computed as weighted average of per-bin (midpoint - actual_rate)^2, weighted by sample_count, using numpy.average
- Cold start protection excludes bins with fewer than 10 samples from adjustment decisions
- Adjustment damping caps corrections at 10% per cycle with hard bounds [0.5, 1.5] for scaling factors
- Coaching uses statistical correlations (not LLM analysis) per RESEARCH.md recommendation for v1
- Improvement area threshold set at 40% success rate -- below this flags the action type

## Deviations from Plan

None - plan executed exactly as written.

## Issues Encountered
None

## User Setup Required
None - no external service configuration required.

## Next Phase Readiness
- FeedbackCollector, CalibrationEngine, and CoachingPatternExtractor ready for integration
- Analytics dashboard and API endpoints can consume these services
- Performance loop and adaptive strategy modules can build on calibration adjustments
- 449 tests passing with zero regressions

---
*Phase: 04.1-agent-learning-performance-feedback*
*Completed: 2026-02-12*
