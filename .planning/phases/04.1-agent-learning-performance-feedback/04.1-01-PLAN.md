---
phase: 04.1-agent-learning-performance-feedback
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - src/app/learning/__init__.py
  - src/app/learning/models.py
  - src/app/learning/schemas.py
  - src/app/learning/outcomes.py
  - alembic/versions/add_learning_outcome_feedback_tables.py
  - tests/test_learning_outcomes.py
autonomous: true

must_haves:
  truths:
    - "Every agent action (send_email, send_chat, process_reply) can record an OutcomeRecord with predicted confidence, action type, and pending status"
    - "Outcome records have time windows (24h immediate, 7d engagement, 30d deal progression) that determine when they expire"
    - "Outcome signal detection can resolve pending outcomes to positive, negative, ambiguous, or expired based on detected signals"
    - "FeedbackEntry and CalibrationBin data models exist for downstream plans"
  artifacts:
    - path: "src/app/learning/models.py"
      provides: "OutcomeRecordModel, FeedbackEntryModel, CalibrationBinModel SQLAlchemy models"
      contains: "class OutcomeRecordModel"
    - path: "src/app/learning/schemas.py"
      provides: "Pydantic schemas for outcomes, feedback, calibration, analytics"
      contains: "class OutcomeStatus"
    - path: "src/app/learning/outcomes.py"
      provides: "OutcomeTracker service for recording and resolving outcomes"
      contains: "class OutcomeTracker"
    - path: "alembic/versions/add_learning_outcome_feedback_tables.py"
      provides: "Database migration for outcome_records, feedback_entries, calibration_bins"
      contains: "outcome_records"
    - path: "tests/test_learning_outcomes.py"
      provides: "Tests for outcome tracking models and service"
  key_links:
    - from: "src/app/learning/models.py"
      to: "src/app/core/database.py"
      via: "TenantBase import"
      pattern: "from src.app.core.database import TenantBase"
    - from: "src/app/learning/outcomes.py"
      to: "src/app/learning/models.py"
      via: "OutcomeRecordModel for database operations"
      pattern: "from src.app.learning.models import OutcomeRecordModel"
---

<objective>
Create the data foundation for the learning and feedback system: SQLAlchemy models for outcome records, feedback entries, and calibration bins with tenant isolation; Pydantic schemas for all learning domain types; OutcomeTracker service for recording agent action outcomes and resolving them via signal detection; and database migration.

Purpose: Establishes the persistence layer and core service that all other learning components (feedback, calibration, analytics) depend on. Without outcome tracking, there is nothing to calibrate against or analyze.

Output: Three SQLAlchemy models (OutcomeRecordModel, FeedbackEntryModel, CalibrationBinModel), comprehensive Pydantic schemas, OutcomeTracker service, Alembic migration, and tests.
</objective>

<execution_context>
@/Users/RAZER/.claude/get-shit-done/workflows/execute-plan.md
@/Users/RAZER/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/04.1-agent-learning-performance-feedback/04.1-CONTEXT.md
@.planning/phases/04.1-agent-learning-performance-feedback/04.1-RESEARCH.md
@src/app/models/sales.py
@src/app/core/database.py
@src/app/agents/sales/schemas.py
@src/app/agents/sales/state_repository.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create learning module with SQLAlchemy models, Pydantic schemas, and migration</name>
  <files>
    src/app/learning/__init__.py
    src/app/learning/models.py
    src/app/learning/schemas.py
    alembic/versions/add_learning_outcome_feedback_tables.py
  </files>
  <action>
Create the `src/app/learning/` module directory with `__init__.py`.

**src/app/learning/models.py** -- Three TenantBase models following the exact pattern from `src/app/models/sales.py`:

1. **OutcomeRecordModel** (table: `outcome_records`, schema: `tenant`):
   - `id`: UUID primary key with `gen_random_uuid()` server_default
   - `tenant_id`: UUID, not null
   - `conversation_state_id`: UUID, not null (links to conversation_states.id)
   - `action_type`: String(50), not null -- "send_email", "send_chat", "process_reply", "qualify", "recommend_action", "escalate"
   - `action_id`: String(100), nullable -- optional reference to specific message/decision ID
   - `predicted_confidence`: Float, not null -- agent's confidence at time of action
   - `outcome_type`: String(50), not null -- "email_engagement", "deal_progression", "meeting_outcome", "escalation_result"
   - `outcome_status`: String(20), default "pending", server_default "pending" -- "pending", "positive", "negative", "ambiguous", "expired"
   - `outcome_score`: Float, nullable -- null until resolved; scale depends on outcome_type
   - `signal_source`: String(20), nullable -- "automatic" or "human_label"
   - `window_expires_at`: DateTime(timezone=True), nullable -- when the detection window closes
   - `resolved_at`: DateTime(timezone=True), nullable
   - `metadata_json`: JSON, default dict, server_default '{}'::json
   - `created_at`: DateTime(timezone=True), server_default func.now()
   - Composite indexes: (tenant_id, action_type, created_at), (tenant_id, outcome_status, created_at), (tenant_id, outcome_type, outcome_status)

2. **FeedbackEntryModel** (table: `feedback_entries`, schema: `tenant`):
   - `id`: UUID primary key with `gen_random_uuid()` server_default
   - `tenant_id`: UUID, not null
   - `outcome_record_id`: UUID, nullable -- optional link to specific outcome
   - `conversation_state_id`: UUID, not null
   - `target_type`: String(20), not null -- "message", "decision", "conversation"
   - `target_id`: String(100), not null -- message ID, decision ID, or conversation state ID
   - `source`: String(20), not null -- "inline", "dashboard"
   - `rating`: Integer, not null -- -1/0/1 for inline; 1-5 for dashboard
   - `comment`: String(1000), nullable
   - `reviewer_id`: UUID, not null -- user who gave feedback
   - `reviewer_role`: String(20), not null -- "rep", "manager", "executive"
   - `metadata_json`: JSON, default dict, server_default '{}'::json
   - `created_at`: DateTime(timezone=True), server_default func.now()
   - Composite indexes: (tenant_id, conversation_state_id), (tenant_id, reviewer_id, created_at)

3. **CalibrationBinModel** (table: `calibration_bins`, schema: `tenant`):
   - `id`: UUID primary key with `gen_random_uuid()` server_default
   - `tenant_id`: UUID, not null
   - `action_type`: String(50), not null -- per-action-type calibration (LOCKED decision from CONTEXT.md)
   - `bin_index`: Integer, not null -- 0-9 for 10 bins [0,.1), [.1,.2), etc.
   - `bin_lower`: Float, not null -- lower edge of confidence bin
   - `bin_upper`: Float, not null -- upper edge of confidence bin
   - `sample_count`: Integer, default 0
   - `outcome_sum`: Float, default 0.0 -- sum of positive outcomes (1.0 each)
   - `actual_rate`: Float, nullable -- outcome_sum / sample_count when sample_count > 0
   - `brier_contribution`: Float, nullable -- running contribution to Brier score
   - `last_updated`: DateTime(timezone=True), nullable
   - `created_at`: DateTime(timezone=True), server_default func.now()
   - UniqueConstraint on (tenant_id, action_type, bin_index), name "uq_calibration_bin_tenant_action_bin"
   - Composite index: (tenant_id, action_type)

**src/app/learning/schemas.py** -- Pydantic schemas:

Enums:
- `OutcomeStatus(str, Enum)`: PENDING, POSITIVE, NEGATIVE, AMBIGUOUS, EXPIRED
- `OutcomeType(str, Enum)`: EMAIL_ENGAGEMENT, DEAL_PROGRESSION, MEETING_OUTCOME, ESCALATION_RESULT
- `FeedbackTarget(str, Enum)`: MESSAGE, DECISION, CONVERSATION
- `FeedbackSource(str, Enum)`: INLINE, DASHBOARD

Schemas:
- `OutcomeRecord(BaseModel)`: All OutcomeRecordModel fields as Pydantic, with datetime.now(timezone.utc) factory defaults
- `FeedbackEntry(BaseModel)`: All FeedbackEntryModel fields as Pydantic
- `CalibrationBin(BaseModel)`: All CalibrationBinModel fields as Pydantic
- `OutcomeWindow(BaseModel)`: action_type, outcome_type, window_hours (int), created helper classmethod for each window tier: `immediate(cls) -> 24h`, `engagement(cls) -> 168h (7 days)`, `deal_progression(cls) -> 720h (30 days)`
- `CalibrationCurve(BaseModel)`: action_type, midpoints (list[float]), actual_rates (list[float]), counts (list[int]), brier_score (float)
- `CalibrationAdjustment(BaseModel)`: action_type, direction ("increase" | "decrease"), magnitude (float), old_threshold (float), new_threshold (float), reason (str)

Request/Response schemas for API (used in plan 03):
- `SubmitFeedbackRequest(BaseModel)`: conversation_state_id (str), target_type (str), target_id (str), source (str), rating (int, ge=-1, le=5), comment (str | None)
- `SubmitFeedbackResponse(BaseModel)`: feedback_id (str), status (str) = "recorded"
- `OutcomeRecordResponse(BaseModel)`: All OutcomeRecord fields serialized for JSON
- `AnalyticsDashboardResponse(BaseModel)`: role (str), metrics (dict), period (str), generated_at (str)
- `CalibrationCurveResponse(BaseModel)`: action_type (str), curve (dict), brier_score (float), sample_count (int), is_calibrated (bool)

**Alembic migration** -- Create all three tables in the tenant schema. Follow the exact pattern from `alembic/versions/add_sales_conversation_state.py`:
- Use `op.execute("SET search_path TO tenant")` to target tenant schema
- Create all three tables with their indexes and constraints
- Include proper downgrade (drop tables)
- Revision depends on the sales conversation state migration (chain after it)
- Add RLS policy for all three tables: `ENABLE ROW LEVEL SECURITY`, `CREATE POLICY tenant_isolation ON tenant.{table_name} FOR ALL USING (tenant_id = current_setting('app.current_tenant_id')::uuid)`
  </action>
  <verify>
Run `python -c "from src.app.learning.models import OutcomeRecordModel, FeedbackEntryModel, CalibrationBinModel; print('Models OK')"` succeeds.
Run `python -c "from src.app.learning.schemas import OutcomeStatus, OutcomeType, FeedbackTarget, FeedbackSource, OutcomeRecord, FeedbackEntry, CalibrationBin, SubmitFeedbackRequest; print('Schemas OK')"` succeeds.
Migration file exists in alembic/versions/.
  </verify>
  <done>Three SQLAlchemy models with tenant isolation, comprehensive Pydantic schemas for all learning domain types, and Alembic migration for all tables with RLS and indexes.</done>
</task>

<task type="auto">
  <name>Task 2: Create OutcomeTracker service with signal detection and tests</name>
  <files>
    src/app/learning/outcomes.py
    tests/test_learning_outcomes.py
  </files>
  <action>
**src/app/learning/outcomes.py** -- OutcomeTracker service:

```python
class OutcomeTracker:
    """Records and resolves agent action outcomes with time-windowed signal detection."""

    # Window configurations (from CONTEXT.md locked decisions)
    WINDOW_CONFIG = {
        "email_engagement": 24,       # 24 hours for immediate signals
        "deal_progression": 720,      # 30 days
        "meeting_outcome": 168,       # 7 days
        "escalation_result": 168,     # 7 days
    }

    def __init__(self, session_factory):
        """Accept session_factory callable (same pattern as ConversationStateRepository)."""

    async def record_outcome(
        self,
        tenant_id: str,
        conversation_state_id: str,
        action_type: str,
        predicted_confidence: float,
        outcome_type: str,
        action_id: str | None = None,
        metadata: dict | None = None,
    ) -> OutcomeRecord:
        """Record a new pending outcome for an agent action.

        Calculates window_expires_at from WINDOW_CONFIG based on outcome_type.
        Creates OutcomeRecordModel in database with status=pending.
        Returns the created OutcomeRecord schema.
        """

    async def resolve_outcome(
        self,
        outcome_id: str,
        tenant_id: str,
        outcome_status: str,  # "positive", "negative", "ambiguous"
        outcome_score: float | None = None,
        signal_source: str = "automatic",
    ) -> OutcomeRecord:
        """Resolve a pending outcome with a final status and optional score.

        Sets resolved_at to now, updates outcome_status and outcome_score.
        Validates that outcome is currently pending (no double-resolution).
        Uses SELECT ... FOR UPDATE to prevent race conditions (Pitfall 1 from RESEARCH.md).
        """

    async def get_pending_outcomes(
        self,
        tenant_id: str | None = None,
        outcome_type: str | None = None,
        expired_only: bool = False,
    ) -> list[OutcomeRecord]:
        """Query pending outcomes, optionally filtered by type or expired window.

        If expired_only=True, only returns outcomes where window_expires_at < now().
        Used by scheduler tasks to find outcomes ready for resolution.
        """

    async def get_outcomes_for_conversation(
        self,
        tenant_id: str,
        conversation_state_id: str,
    ) -> list[OutcomeRecord]:
        """Get all outcomes (any status) for a specific conversation."""

    async def check_immediate_signals(self, tenant_id: str | None = None) -> int:
        """Check for immediate signals (email reply detection).

        Queries pending email_engagement outcomes within their 24h window.
        For each, checks if the conversation_state has had new interactions
        since the outcome was recorded (reply detected = positive).
        Returns count of outcomes resolved.

        Detection logic:
        - Load conversation state for each pending outcome
        - If state.interaction_count increased since outcome.created_at: POSITIVE
        - If window expired: EXPIRED
        - Otherwise: still PENDING (leave for next check)
        """

    async def check_deal_progression_signals(self, tenant_id: str | None = None) -> int:
        """Check for deal progression signals.

        Queries pending deal_progression outcomes.
        For each, checks if the deal stage has advanced from the stage at outcome creation.
        Returns count of outcomes resolved.

        Detection logic:
        - Load conversation state
        - Compare current deal_stage vs stage stored in outcome metadata_json["deal_stage_at_creation"]
        - Stage advanced forward: POSITIVE (score based on number of stages advanced)
        - Stage moved to CLOSED_LOST or STALLED: NEGATIVE
        - No change but window expired: EXPIRED
        """

    async def expire_overdue_outcomes(self, tenant_id: str | None = None) -> int:
        """Bulk-expire outcomes past their window with no signal detected.

        Efficiently updates all pending outcomes where window_expires_at < now()
        to status=EXPIRED. Uses a single UPDATE statement for performance.
        Returns count of expired outcomes.
        """
```

Use the session_factory callable pattern from ConversationStateRepository. Use `SELECT ... FOR UPDATE SKIP LOCKED` in resolve_outcome to prevent race conditions (RESEARCH.md Pitfall 1).

**tests/test_learning_outcomes.py** -- Test with InMemory approach matching existing test patterns:

Create `InMemoryOutcomeRepository` as a thin test double (stores outcomes in a dict). Tests:

1. `test_outcome_record_creation` - Record an outcome, verify all fields set correctly, status=pending, window_expires_at calculated
2. `test_outcome_resolution` - Record then resolve an outcome, verify status changed, resolved_at set
3. `test_double_resolution_prevented` - Resolve same outcome twice, verify error/no-op
4. `test_window_expiry_calculation` - Verify each outcome_type gets correct window (24h, 168h, 720h)
5. `test_get_pending_outcomes_filtered` - Create multiple outcomes with different types/statuses, verify filtering
6. `test_expire_overdue_outcomes` - Create outcomes with past window, call expire, verify status=EXPIRED
7. `test_immediate_signal_detection_reply` - Simulate reply detection (interaction count increased) resolves to POSITIVE
8. `test_deal_progression_signal_detection` - Simulate stage advance resolves to POSITIVE
9. `test_model_instantiation` - Verify all three SQLAlchemy models can be instantiated
10. `test_schema_serialization_roundtrip` - Verify OutcomeRecord, FeedbackEntry, CalibrationBin serialize/deserialize correctly
11. `test_outcome_window_classmethods` - Verify OutcomeWindow.immediate(), engagement(), deal_progression() return correct hours
12. `test_outcome_status_enum_values` - Verify all OutcomeStatus enum values match expected strings

All tests use `pytest` with `@pytest.mark.asyncio` for async tests. Follow the existing test patterns from tests/test_sales_state.py.
  </action>
  <verify>
Run `python -m pytest tests/test_learning_outcomes.py -v` -- all tests pass.
Run `python -m pytest tests/ -v --tb=short` -- full test suite still passes (no regressions).
  </verify>
  <done>OutcomeTracker service records agent action outcomes, resolves them via signal detection (immediate reply, deal progression, expiry), handles race conditions, and 12+ tests cover all critical paths. Full test suite passes with no regressions.</done>
</task>

</tasks>

<verification>
- `python -c "from src.app.learning.models import OutcomeRecordModel, FeedbackEntryModel, CalibrationBinModel"` succeeds
- `python -c "from src.app.learning.schemas import OutcomeStatus, OutcomeType, FeedbackTarget, SubmitFeedbackRequest"` succeeds
- `python -c "from src.app.learning.outcomes import OutcomeTracker"` succeeds
- `python -m pytest tests/test_learning_outcomes.py -v` -- all pass
- `python -m pytest tests/ -v --tb=short` -- full suite passes, no regressions
</verification>

<success_criteria>
1. OutcomeRecordModel, FeedbackEntryModel, CalibrationBinModel exist with TenantBase and proper indexes
2. Alembic migration creates all three tables with RLS
3. Pydantic schemas cover all learning domain types including API request/response
4. OutcomeTracker can record, resolve, query, and expire outcomes
5. Signal detection checks (immediate, deal progression) correctly resolve outcomes
6. Race condition protection via SELECT FOR UPDATE SKIP LOCKED
7. All new tests pass, existing 396 tests still pass
</success_criteria>

<output>
After completion, create `.planning/phases/04.1-agent-learning-performance-feedback/04.1-01-SUMMARY.md`
</output>
