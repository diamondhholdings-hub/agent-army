# Phase 4.1: Agent Learning & Performance Feedback - Research

**Researched:** 2026-02-11
**Domain:** Outcome tracking, confidence calibration, human feedback loops, and performance analytics for an LLM-powered sales agent -- built on an existing FastAPI + SQLAlchemy + Redis Streams + PostgreSQL multi-tenant stack
**Confidence:** HIGH

## Summary

Phase 4.1 adds a learning and feedback layer on top of Phase 4's Sales Agent Core. The system needs five interconnected capabilities: (1) outcome signal detection that captures whether agent actions led to positive or negative results across multiple time horizons, (2) a human feedback collection interface with both inline reactions and a dedicated review dashboard, (3) per-action-type confidence calibration that continuously compares predicted confidence against actual outcomes and auto-adjusts agent behavior, (4) role-appropriate analytics presentation for sales reps, managers, and executives, and (5) a coaching pattern extraction module that surfaces sales training insights.

The standard approach is to extend the existing PostgreSQL models (via TenantBase) with new outcome and feedback tables, use the existing Redis Streams event bus for real-time signal propagation, implement calibration math using lightweight numpy-based Welford online statistics (avoiding heavy scikit-learn dependency for what is fundamentally binned averaging), schedule time-windowed outcome checks via APScheduler (which integrates natively with FastAPI's async lifespan and already uses SQLAlchemy + asyncpg as a data store), and expose analytics via new FastAPI API endpoints that serve JSON data for dashboard consumption plus SSE for real-time updates.

Key insight: This phase is data-model-heavy and compute-light. The primary engineering challenge is designing the right database models to capture outcomes, feedback, and calibration data with proper tenant isolation, and wiring time-based signal detection into the existing event-driven architecture. The statistical methods (Brier score, calibration curves) are straightforward math operations on stored data, not complex ML pipelines.

**Primary recommendation:** Extend the existing tenant-scoped PostgreSQL schema with new models for outcomes, feedback, and calibration data. Use APScheduler for time-windowed signal detection. Implement calibration as lightweight numpy math on binned prediction-outcome pairs. Expose analytics through new FastAPI endpoints with JSON + SSE for real-time dashboard support.

## Standard Stack

### Core
| Library | Version | Purpose | Why Standard |
|---------|---------|---------|--------------|
| SQLAlchemy | >=2.0.0 (already installed) | Outcome, feedback, and calibration data models via TenantBase | Already in stack, proven async multi-tenant pattern with schema_translate_map |
| APScheduler | >=4.0.0 | Time-windowed outcome signal detection, periodic report generation, calibration update scheduling | Native async support, SQLAlchemy + asyncpg data store, FastAPI lifespan integration, PostgreSQL event broker |
| numpy | >=1.26.0 | Calibration math: Brier score, binned averages, running statistics | Lightweight, no ML framework overhead, already a transitive dependency via fastembed |
| sse-starlette | >=2.0.0 | Server-Sent Events for real-time dashboard updates | Production-ready SSE for Starlette/FastAPI, W3C spec compliant, auto disconnect detection |

### Supporting
| Library | Version | Purpose | When to Use |
|---------|---------|---------|-------------|
| Redis Streams (redis>=5.0.0) | Already installed | Real-time outcome signal propagation, feedback event streaming | Event-driven signal detection pipeline, pub/sub for dashboard updates |
| Pydantic (>=2.0.0) | Already installed | Outcome, feedback, calibration schemas | Request/response models for all new API endpoints |
| structlog (>=24.0.0) | Already installed | Structured logging for outcome tracking and calibration events | Debug/audit trail for signal detection and calibration adjustments |

### Alternatives Considered
| Instead of | Could Use | Tradeoff |
|------------|-----------|----------|
| APScheduler for time-window checks | Celery Beat | Celery adds broker complexity (RabbitMQ/Redis as broker); APScheduler is lighter, uses existing PostgreSQL, and integrates natively with FastAPI async |
| numpy for calibration math | scikit-learn CalibrationDisplay + brier_score_loss | scikit-learn is heavy (~50MB) for what amounts to binned averaging; numpy handles the math in 20 lines |
| sse-starlette for real-time | WebSockets | SSE is simpler for unidirectional server-to-client streaming (dashboards); WebSockets add bidirectional complexity not needed here |
| PostgreSQL for outcome storage | ClickHouse/TimescaleDB | Phase is data-model-heavy not analytics-heavy; PostgreSQL is already in stack and handles the volume; can optimize later if needed |

**Installation:**
```bash
pip install apscheduler[sqlalchemy,asyncpg] sse-starlette numpy
```

Note: numpy is likely already a transitive dependency. APScheduler extras provide SQLAlchemy data store and asyncpg event broker. sse-starlette is the only truly new dependency.

## Architecture Patterns

### Recommended Project Structure
```
src/
  app/
    agents/
      sales/
        agent.py              # EXISTING: Add outcome recording hooks to execute() handlers
        schemas.py            # EXISTING: Extend with feedback/outcome types
    learning/                 # NEW: Learning and feedback module
      __init__.py
      models.py              # SQLAlchemy models: OutcomeRecord, FeedbackEntry, CalibrationBin
      schemas.py             # Pydantic schemas: outcome types, feedback types, analytics responses
      outcomes.py            # OutcomeTracker: signal detection, time-window processing
      feedback.py            # FeedbackCollector: inline reactions, review submissions
      calibration.py         # CalibrationEngine: per-action-type calibration, auto-adjust
      analytics.py           # AnalyticsService: metric computation, report generation
      scheduler.py           # Scheduler setup: time-windowed checks, periodic reports
    api/
      v1/
        learning.py           # NEW: API endpoints for feedback submission, analytics queries
        router.py             # EXISTING: Add learning router
```

### Pattern 1: Outcome Signal Recording
**What:** Every agent action (send_email, send_chat, process_reply, recommend_action) records an OutcomeRecord with the action's predicted confidence, action type, and initial "pending" status. Time-windowed background tasks later update the outcome status based on detected signals.
**When to use:** After every agent action in the execute() handler.
**Example:**
```python
# Source: Extension of existing SalesAgent.execute() pattern
from datetime import datetime, timezone
from enum import Enum
from pydantic import BaseModel, Field

class OutcomeStatus(str, Enum):
    PENDING = "pending"        # Awaiting signal detection
    POSITIVE = "positive"      # Clear positive outcome detected
    NEGATIVE = "negative"      # Clear negative outcome detected
    AMBIGUOUS = "ambiguous"    # Needs human labeling
    EXPIRED = "expired"        # Time window passed with no signal

class OutcomeType(str, Enum):
    EMAIL_ENGAGEMENT = "email_engagement"
    DEAL_PROGRESSION = "deal_progression"
    MEETING_OUTCOME = "meeting_outcome"
    ESCALATION_RESULT = "escalation_result"

class OutcomeRecord(BaseModel):
    """Recorded outcome for a single agent action."""
    outcome_id: str
    tenant_id: str
    conversation_state_id: str
    action_type: str           # "send_email", "send_chat", etc.
    predicted_confidence: float # Agent's confidence at time of action
    outcome_type: OutcomeType
    outcome_status: OutcomeStatus = OutcomeStatus.PENDING
    outcome_score: float | None = None  # Null until resolved
    signal_source: str | None = None    # "automatic" or "human_label"
    created_at: datetime = Field(default_factory=lambda: datetime.now(timezone.utc))
    resolved_at: datetime | None = None
    window_expires_at: datetime | None = None  # When time window closes
```

### Pattern 2: Layered Time-Window Signal Detection
**What:** APScheduler runs periodic tasks at different intervals for each signal type: immediate signals every 15 minutes (24h window), engagement signals daily (7-day window), and deal progression signals weekly (30-day window). Each task queries pending OutcomeRecords whose window is due and checks for signals.
**When to use:** Background processing loop for outcome resolution.
**Example:**
```python
# Source: APScheduler FastAPI integration pattern (Context7 verified)
from apscheduler import AsyncScheduler, ConflictPolicy
from apscheduler.datastores.sqlalchemy import SQLAlchemyDataStore
from apscheduler.eventbrokers.asyncpg import AsyncpgEventBroker
from apscheduler.triggers.interval import IntervalTrigger

async def setup_outcome_scheduler(engine) -> AsyncScheduler:
    """Configure outcome signal detection schedules."""
    data_store = SQLAlchemyDataStore(engine)
    event_broker = AsyncpgEventBroker.from_async_sqla_engine(engine)

    scheduler = AsyncScheduler(data_store, event_broker)

    # Immediate signals: check every 15 minutes for 24h window outcomes
    await scheduler.add_schedule(
        check_immediate_signals,
        IntervalTrigger(minutes=15),
        id="immediate_signals",
        conflict_policy=ConflictPolicy.replace,
    )

    # Engagement signals: check every 6 hours for 7-day window outcomes
    await scheduler.add_schedule(
        check_engagement_signals,
        IntervalTrigger(hours=6),
        id="engagement_signals",
        conflict_policy=ConflictPolicy.replace,
    )

    # Deal progression signals: check daily for 30-day window outcomes
    await scheduler.add_schedule(
        check_deal_progression_signals,
        IntervalTrigger(hours=24),
        id="deal_progression_signals",
        conflict_policy=ConflictPolicy.replace,
    )

    return scheduler
```

### Pattern 3: Per-Action-Type Calibration with Binned Averages
**What:** For each action type (email, meeting, escalation, stage progression), maintain calibration bins that track predicted confidence vs actual success rate. When enough data accumulates in a bin, compute the calibration gap. If miscalibration exceeds a threshold, adjust the agent's confidence scaling and escalation thresholds for that action type.
**When to use:** After every outcome is resolved (automatic or human-labeled).
**Example:**
```python
import numpy as np

class CalibrationEngine:
    """Per-action-type confidence calibration using binned averages."""

    BIN_EDGES = np.linspace(0.0, 1.0, 11)  # 10 bins: [0,.1), [.1,.2), ...
    MIN_SAMPLES_PER_BIN = 10  # Need at least 10 outcomes to calibrate a bin
    MISCALIBRATION_THRESHOLD = 0.15  # >15% gap triggers adjustment

    async def update_calibration(
        self,
        action_type: str,
        predicted_confidence: float,
        actual_outcome: bool,  # True = positive, False = negative
    ) -> None:
        """Update calibration bin for this action type."""
        bin_idx = int(np.digitize(predicted_confidence, self.BIN_EDGES)) - 1
        bin_idx = max(0, min(bin_idx, len(self.BIN_EDGES) - 2))

        # Update bin statistics (Welford online update for mean)
        # bin.count += 1
        # bin.outcome_sum += (1.0 if actual_outcome else 0.0)
        # bin.actual_rate = bin.outcome_sum / bin.count
        # Store to database
        ...

    async def get_calibration_curve(self, action_type: str) -> dict:
        """Return calibration curve data for an action type."""
        # Load bins from database
        # Return {predicted: [midpoints], actual: [rates], counts: [n_per_bin]}
        ...

    async def compute_brier_score(self, action_type: str) -> float:
        """Brier score = mean((predicted - actual)^2) for this action type."""
        # Load all resolved outcomes for action_type
        # brier = np.mean((predictions - outcomes) ** 2)
        ...

    async def check_and_adjust(self, action_type: str) -> dict | None:
        """Check calibration and auto-adjust if miscalibrated."""
        curve = await self.get_calibration_curve(action_type)
        # For each bin with enough samples:
        #   gap = abs(predicted_midpoint - actual_rate)
        #   if gap > MISCALIBRATION_THRESHOLD: flag for adjustment
        # Return adjustment recommendation or None
        ...
```

### Pattern 4: Feedback Collection with Dual Interface
**What:** FeedbackEntry model supports both inline reactions (quick thumbs up/down from Slack/Gmail) and detailed reviews (from web dashboard). Each feedback targets either a specific message, an agent decision, or a full conversation thread. The feedback type determines required fields.
**When to use:** Any time a human evaluates agent behavior.
**Example:**
```python
class FeedbackTarget(str, Enum):
    MESSAGE = "message"           # Individual email/chat message
    DECISION = "decision"         # Agent decision (escalation, stage change, action)
    CONVERSATION = "conversation" # Full thread retrospective

class FeedbackSource(str, Enum):
    INLINE = "inline"       # Quick reaction from Slack/Gmail
    DASHBOARD = "dashboard" # Detailed review from web UI

class FeedbackEntry(BaseModel):
    """Human feedback on agent behavior."""
    feedback_id: str
    tenant_id: str
    outcome_record_id: str | None = None  # Links to specific outcome
    conversation_state_id: str
    target_type: FeedbackTarget
    target_id: str                  # Message ID, decision ID, or conversation ID
    source: FeedbackSource
    rating: int                     # -1 (bad), 0 (neutral), 1 (good) for inline;
                                    # 1-5 for dashboard detailed reviews
    comment: str | None = None      # Optional free text
    reviewer_id: str                # User who gave feedback
    reviewer_role: str              # "rep", "manager", "executive"
    created_at: datetime
```

### Pattern 5: Analytics Service with Role-Based Views
**What:** AnalyticsService computes metrics from outcome and feedback data, returning different aggregations based on the requesting user's role. Uses the existing tenant session factory for data isolation.
**When to use:** Dashboard API endpoints, report generation, alert checks.
**Example:**
```python
class AnalyticsService:
    """Role-based analytics computation from outcome/feedback data."""

    async def get_rep_dashboard(self, tenant_id: str, rep_id: str) -> dict:
        """Individual agent performance for a sales rep."""
        return {
            "response_rates": ...,      # % of actions with positive outcomes
            "escalation_history": ...,  # Recent escalations with outcomes
            "deal_impact": ...,         # Stage progressions attributed to agent
            "feedback_scores": ...,     # Average rating from rep's feedback
            "calibration_summary": ..., # Over/under confident indicators
        }

    async def get_manager_dashboard(self, tenant_id: str) -> dict:
        """Team-level performance for managers."""
        return {
            "team_trends": ...,          # Aggregate performance over time
            "comparative_performance": ..., # Per-rep agent effectiveness
            "coaching_opportunities": ...,  # Patterns from escalations
            "aggregate_calibration": ...,   # Team-wide calibration health
        }

    async def get_executive_summary(self, tenant_id: str) -> dict:
        """Strategic insights for executives."""
        return {
            "roi_metrics": ...,           # Cost per interaction, time saved
            "agent_vs_human_baseline": ..., # Comparative effectiveness
            "engagement_trends": ...,      # Customer engagement over time
            "strategic_insights": ...,     # High-level patterns
        }
```

### Pattern 6: SSE Real-Time Dashboard Updates
**What:** Use sse-starlette EventSourceResponse to stream live metric updates to dashboard clients. A Redis pub/sub channel broadcasts metric changes from outcome resolution and feedback events.
**When to use:** Real-time dashboard endpoint for ad-hoc monitoring.
**Example:**
```python
# Source: sse-starlette (verified via Context7 search results and PyPI)
from sse_starlette.sse import EventSourceResponse
from fastapi import APIRouter, Request

router = APIRouter()

@router.get("/analytics/stream")
async def analytics_stream(request: Request, tenant_id: str):
    """SSE endpoint for real-time analytics updates."""
    async def event_generator():
        # Subscribe to Redis pub/sub channel for this tenant
        # Yield new metric snapshots as they arrive
        async for metric_update in subscribe_metrics(tenant_id):
            if await request.is_disconnected():
                break
            yield {
                "event": metric_update["type"],
                "data": json.dumps(metric_update["data"]),
            }

    return EventSourceResponse(event_generator())
```

### Anti-Patterns to Avoid
- **Storing calibration in application memory:** Calibration data MUST be persisted to PostgreSQL. Application restarts would lose all calibration history. Use the same tenant-scoped TenantBase pattern as Phase 4.
- **Polling for outcome signals in request handlers:** Outcome signal detection is background work. Do NOT check for email opens/replies in API request handlers. Use APScheduler background tasks that run independently of HTTP requests.
- **One global calibration curve:** The CONTEXT.md decision explicitly requires per-action-type calibration. An agent may be well-calibrated for emails but miscalibrated for escalation decisions. Do NOT average across action types.
- **Heavy ML pipeline for calibration:** Do NOT import scikit-learn or train ML models for calibration. This is binned averaging with Brier score -- 20 lines of numpy. Keep it lightweight.
- **Blocking SSE endpoints:** SSE generators must be fully async. Do NOT perform database queries synchronously inside the event generator. Use the async session factory.
- **Computing analytics on every request:** Expensive aggregate queries (team trends, ROI calculations) should be pre-computed periodically and cached, not computed fresh on every dashboard load.

## Don't Hand-Roll

Problems that look simple but have existing solutions:

| Problem | Don't Build | Use Instead | Why |
|---------|-------------|-------------|-----|
| Background task scheduling | Custom asyncio.create_task loops with sleep() | APScheduler with SQLAlchemy data store | Handles missed tasks, persistence across restarts, distributed locking |
| Server-sent events | Custom StreamingResponse with manual event formatting | sse-starlette EventSourceResponse | W3C spec compliance, auto disconnect detection, graceful shutdown |
| Online statistics (running mean/variance) | Custom accumulator classes | Welford algorithm (10 lines numpy or `welford` package) | Numerically stable, O(1) memory per bin, proven algorithm |
| Brier score computation | Manual loop over predictions | `numpy.mean((predicted - actual) ** 2)` | One line, vectorized, correct |
| Tenant-scoped data access | New database connection patterns | Existing `get_tenant_session()` + `TenantBase` | Already handles schema_translate_map, RLS, connection pooling |
| Event propagation | New pub/sub system | Existing `TenantEventBus` (Redis Streams) | Already tenant-scoped, consumer groups, DLQ support |
| Metric caching | File-based or custom caching | Redis (already in stack) via existing `TenantRedis` | TTL support, tenant-scoped keys, already initialized |

**Key insight:** Phase 4.1 is primarily a data-model and scheduling layer. The heavy infrastructure (database, events, tenant isolation, auth) already exists from Phases 1-4. New code is mostly: (a) new SQLAlchemy models, (b) APScheduler task definitions, (c) numpy calibration math, (d) FastAPI endpoints for feedback/analytics.

## Common Pitfalls

### Pitfall 1: Time-Window Race Conditions
**What goes wrong:** Two scheduler tasks both try to resolve the same pending outcome record simultaneously, leading to double-counting or conflicting status updates.
**Why it happens:** Multiple scheduler instances or overlapping task windows.
**How to avoid:** Use PostgreSQL `SELECT ... FOR UPDATE SKIP LOCKED` when claiming pending outcomes for processing. APScheduler's `ConflictPolicy.skip_existing` prevents duplicate schedule instances.
**Warning signs:** Outcome counts don't match action counts; same outcome resolved twice with different statuses.

### Pitfall 2: Calibration Cold Start
**What goes wrong:** Calibration bins have too few samples to be statistically meaningful, leading to noisy adjustments that oscillate.
**Why it happens:** New tenants or new action types have sparse data.
**How to avoid:** Set `MIN_SAMPLES_PER_BIN` threshold (recommend 10-20). Do NOT auto-adjust until enough data accumulates. Show "insufficient data" in dashboards for sparse bins. Use Bayesian priors (start with prior of 0.5 calibration gap = 0) to smooth early estimates.
**Warning signs:** Calibration adjustments flip-flopping between over-confident and under-confident with each new batch.

### Pitfall 3: Feedback Fatigue
**What goes wrong:** Sales reps get prompted for feedback too often and start ignoring or auto-approving, making feedback data unreliable.
**Why it happens:** Prompting for feedback on every routine action.
**How to avoid:** Follow CONTEXT.md decision: real-time prompts ONLY for critical actions (escalations, stage changes). Routine actions get on-demand feedback (available but not prompted). Track feedback rate per user -- declining rates signal fatigue.
**Warning signs:** Feedback submission rate drops over time; ratings cluster at extremes (all 5s or all 1s suggesting auto-approval).

### Pitfall 4: Analytics Query Performance
**What goes wrong:** Dashboard loads take 5+ seconds because aggregate queries scan full outcome/feedback tables.
**Why it happens:** No indexes on common query patterns; computing aggregates on-the-fly for every dashboard load.
**How to avoid:** Add composite indexes on (tenant_id, action_type, created_at) and (tenant_id, outcome_status, created_at). Pre-compute daily/weekly aggregates in a materialized summary table via APScheduler periodic task. Cache recent aggregates in Redis with 5-minute TTL.
**Warning signs:** Increasing dashboard load times as data grows; database CPU spikes on dashboard refresh.

### Pitfall 5: Confidence Auto-Adjustment Instability
**What goes wrong:** Auto-adjustment changes escalation thresholds too aggressively, causing the agent to either escalate everything or nothing.
**Why it happens:** Large calibration gap detected, full correction applied immediately.
**How to avoid:** Apply damped adjustments: adjust by max 10% of the detected gap per calibration cycle. Use exponential moving average for the adjustment signal. Set hard bounds on confidence scaling factors (e.g., 0.5x to 1.5x) and escalation thresholds (e.g., 0.5 to 0.9).
**Warning signs:** Escalation rate swings wildly between calibration cycles; agent alternates between overly cautious and overly aggressive.

### Pitfall 6: Outcome Attribution Ambiguity
**What goes wrong:** Multiple agent actions contribute to a deal outcome, but the system attributes the outcome only to the last action.
**Why it happens:** Simple last-touch attribution model.
**How to avoid:** Record outcome at the conversation level as well as the action level. Use multi-touch attribution: when a deal progresses, all actions in the time window get partial credit. Document the attribution model explicitly so analytics consumers understand what the numbers mean.
**Warning signs:** Actions early in the sales cycle always show poor outcomes because credit goes to later actions.

## Code Examples

Verified patterns from official sources and existing codebase:

### SQLAlchemy TenantBase Model for Outcomes
```python
# Source: Existing TenantBase pattern from src/app/models/sales.py
import uuid
from datetime import datetime
from sqlalchemy import DateTime, Float, Integer, String, func, text
from sqlalchemy.dialects.postgresql import JSON, UUID
from sqlalchemy.orm import Mapped, mapped_column
from src.app.core.database import TenantBase

class OutcomeRecordModel(TenantBase):
    """Persistent outcome record for agent actions."""
    __tablename__ = "outcome_records"
    __table_args__ = {"schema": "tenant"}

    id: Mapped[uuid.UUID] = mapped_column(
        UUID(as_uuid=True), primary_key=True,
        server_default=text("gen_random_uuid()"),
    )
    tenant_id: Mapped[uuid.UUID] = mapped_column(UUID(as_uuid=True), nullable=False)
    conversation_state_id: Mapped[uuid.UUID] = mapped_column(
        UUID(as_uuid=True), nullable=False
    )
    action_type: Mapped[str] = mapped_column(String(50), nullable=False)
    predicted_confidence: Mapped[float] = mapped_column(Float, nullable=False)
    outcome_type: Mapped[str] = mapped_column(String(50), nullable=False)
    outcome_status: Mapped[str] = mapped_column(
        String(20), default="pending", server_default=text("'pending'")
    )
    outcome_score: Mapped[float | None] = mapped_column(Float, nullable=True)
    signal_source: Mapped[str | None] = mapped_column(String(20), nullable=True)
    window_expires_at: Mapped[datetime | None] = mapped_column(
        DateTime(timezone=True), nullable=True
    )
    resolved_at: Mapped[datetime | None] = mapped_column(
        DateTime(timezone=True), nullable=True
    )
    metadata_json: Mapped[dict] = mapped_column(
        JSON, default=dict, server_default=text("'{}'::json")
    )
    created_at: Mapped[datetime] = mapped_column(
        DateTime(timezone=True), server_default=func.now()
    )
```

### APScheduler FastAPI Lifespan Integration
```python
# Source: Context7 APScheduler docs (verified)
from contextlib import asynccontextmanager
from apscheduler import AsyncScheduler, ConflictPolicy
from apscheduler.datastores.sqlalchemy import SQLAlchemyDataStore
from apscheduler.eventbrokers.asyncpg import AsyncpgEventBroker
from apscheduler.triggers.interval import IntervalTrigger

@asynccontextmanager
async def lifespan(app: FastAPI):
    # ... existing Phase 1-4 initialization ...

    # Phase 4.1: Learning scheduler
    try:
        engine = get_engine()
        data_store = SQLAlchemyDataStore(engine)
        event_broker = AsyncpgEventBroker.from_async_sqla_engine(engine)

        async with AsyncScheduler(data_store, event_broker) as scheduler:
            await scheduler.add_schedule(
                check_immediate_signals,
                IntervalTrigger(minutes=15),
                id="immediate_signals",
                conflict_policy=ConflictPolicy.replace,
            )
            await scheduler.add_schedule(
                check_engagement_signals,
                IntervalTrigger(hours=6),
                id="engagement_signals",
                conflict_policy=ConflictPolicy.replace,
            )
            await scheduler.start_in_background()
            app.state.learning_scheduler = scheduler
            log.info("phase4_1.learning_scheduler_initialized")
    except Exception:
        log.warning("phase4_1.learning_scheduler_init_failed", exc_info=True)

    yield
    # ... existing shutdown ...
```

### Brier Score and Calibration Curve with numpy
```python
# Source: scikit-learn docs (verified via Context7), implemented with pure numpy
import numpy as np

def brier_score(predicted: np.ndarray, actual: np.ndarray) -> float:
    """Compute Brier score: mean squared error between predicted and actual.

    Lower is better. 0.0 = perfect calibration, 0.25 = random guessing.
    """
    return float(np.mean((predicted - actual) ** 2))

def calibration_curve(
    predicted: np.ndarray,
    actual: np.ndarray,
    n_bins: int = 10,
) -> dict:
    """Compute calibration curve data (reliability diagram).

    Returns dict with bin midpoints, actual rates, and sample counts.
    Bins with fewer than min_samples are excluded.
    """
    bin_edges = np.linspace(0.0, 1.0, n_bins + 1)
    bin_indices = np.digitize(predicted, bin_edges) - 1
    bin_indices = np.clip(bin_indices, 0, n_bins - 1)

    midpoints = []
    actual_rates = []
    counts = []

    for i in range(n_bins):
        mask = bin_indices == i
        count = mask.sum()
        if count > 0:
            midpoints.append(float((bin_edges[i] + bin_edges[i + 1]) / 2))
            actual_rates.append(float(actual[mask].mean()))
            counts.append(int(count))

    return {
        "midpoints": midpoints,
        "actual_rates": actual_rates,
        "counts": counts,
        "brier_score": brier_score(predicted, actual),
    }
```

### SSE Real-Time Analytics Endpoint
```python
# Source: sse-starlette PyPI docs + FastAPI patterns
import json
from fastapi import APIRouter, Depends, Request
from sse_starlette.sse import EventSourceResponse

router = APIRouter(prefix="/learning", tags=["learning"])

@router.get("/analytics/stream")
async def analytics_stream(
    request: Request,
    tenant: TenantContext = Depends(get_tenant),
):
    """Stream real-time analytics updates via SSE."""
    async def event_generator():
        redis = get_redis_pool()
        pubsub = redis.pubsub()
        channel = f"t:{tenant.tenant_id}:analytics"
        await pubsub.subscribe(channel)
        try:
            async for message in pubsub.listen():
                if await request.is_disconnected():
                    break
                if message["type"] == "message":
                    yield {
                        "event": "metric_update",
                        "data": message["data"].decode(),
                    }
        finally:
            await pubsub.unsubscribe(channel)

    return EventSourceResponse(event_generator())
```

### Feedback Collection API Endpoint
```python
# Source: Existing FastAPI endpoint pattern from src/app/api/v1/sales.py
from pydantic import BaseModel, Field

class SubmitFeedbackRequest(BaseModel):
    """Request body for submitting feedback on agent behavior."""
    conversation_state_id: str
    target_type: str  # "message", "decision", "conversation"
    target_id: str
    source: str  # "inline", "dashboard"
    rating: int = Field(ge=-1, le=5)
    comment: str | None = None

class SubmitFeedbackResponse(BaseModel):
    feedback_id: str
    status: str = "recorded"

@router.post("/feedback", response_model=SubmitFeedbackResponse)
async def submit_feedback(
    body: SubmitFeedbackRequest,
    user: User = Depends(get_current_user),
    tenant: TenantContext = Depends(get_tenant),
) -> SubmitFeedbackResponse:
    """Submit feedback on agent behavior."""
    collector = _get_feedback_collector()
    result = await collector.record_feedback(
        tenant_id=tenant.tenant_id,
        reviewer_id=str(user.id),
        reviewer_role=user.role,
        **body.model_dump(),
    )
    return SubmitFeedbackResponse(feedback_id=result.feedback_id)
```

## State of the Art

| Old Approach | Current Approach | When Changed | Impact |
|--------------|------------------|--------------|--------|
| Batch calibration retraining | Online/incremental calibration updates | 2024+ | Real-time calibration adjustment without batch ML pipelines |
| scikit-learn CalibratedClassifierCV | Lightweight numpy binned averages for LLM confidence calibration | 2025+ | LLM confidence is not a classifier output; traditional ML calibration methods don't directly apply |
| Celery for background tasks in FastAPI | APScheduler 4.x with native async + SQLAlchemy data store | 2024+ (APScheduler 4.0 release) | No separate broker needed; uses existing PostgreSQL; native asyncio |
| WebSockets for real-time dashboards | SSE for unidirectional server-to-client streaming | Ongoing | Simpler protocol for dashboard use case; automatic reconnection in browsers |
| Manual feedback collection spreadsheets | Inline reactions + dashboard reviews with structured API | Current best practice | Low-friction feedback at point of interaction; structured data for analysis |

**Deprecated/outdated:**
- APScheduler 3.x `BackgroundScheduler` and `AsyncIOScheduler`: APScheduler 4.x introduces `AsyncScheduler` with completely new API; old API is deprecated
- Polling-based dashboard updates: SSE provides real-time push with lower overhead than polling intervals

## Open Questions

Things that couldn't be fully resolved:

1. **APScheduler 4.x Async Engine Compatibility**
   - What we know: APScheduler 4.x SQLAlchemyDataStore accepts an engine. The existing codebase uses `create_async_engine` with asyncpg. APScheduler 4.x docs show `AsyncpgEventBroker.from_async_sqla_engine(engine)` for the event broker.
   - What's unclear: Whether APScheduler's SQLAlchemyDataStore works directly with an async engine or requires a separate sync engine. Context7 docs show async usage but the data store may need a sync engine internally.
   - Recommendation: Validate during implementation. If async engine doesn't work, create a parallel sync engine for APScheduler only (same connection string, swap `+asyncpg` for `+psycopg2`). The existing `psycopg2-binary` dependency is already installed.

2. **Outcome Signal Detection for Email Opens/Clicks**
   - What we know: Gmail API can report email open/read status through message metadata. Link click tracking requires URL wrapping (adding tracking redirects to links in emails).
   - What's unclear: Whether Gmail API provides reliable open tracking (it depends on image pixel loading which many clients block), and whether the project wants to implement link tracking (adds complexity).
   - Recommendation: For Phase 4.1, focus on reply-based signals (customer replied = engaged) and deal progression signals (stage changed = positive). Open tracking is unreliable; click tracking is out of scope. These are the highest-signal outcomes.

3. **Dashboard Frontend Technology**
   - What we know: The CONTEXT.md specifies dashboard for ad-hoc exploration with interactive charts. The backend is FastAPI (Python). No frontend framework is currently in the project.
   - What's unclear: Whether to build a full frontend (React, Vue, Svelte) or use a Python-native dashboard (Streamlit, Dash) or just provide JSON API endpoints for future frontend consumption.
   - Recommendation: For Phase 4.1, build JSON API endpoints + SSE streaming endpoint. The "dashboard" is the API layer. A frontend can be added in a later phase. This avoids introducing a frontend framework and keeps scope manageable. Include one simple HTML page with embedded charts (using Chart.js CDN) as a proof-of-concept if needed.

4. **Coaching Pattern Extraction Complexity**
   - What we know: Success Criterion 5 requires identifying patterns from escalations and successful interactions for human coaching.
   - What's unclear: Whether this requires LLM-based pattern analysis (expensive, complex) or simpler statistical pattern detection (e.g., "emails sent during morning hours have 2x response rate").
   - Recommendation: Start with statistical patterns (correlations between action attributes and outcomes). Add LLM-based insight generation as a later enhancement if statistical patterns prove insufficient. The statistical approach is testable and deterministic.

## Sources

### Primary (HIGH confidence)
- Context7 `/scikit-learn/scikit-learn` - Calibration curves, Brier score, CalibrationDisplay documentation
- Context7 `/agronholm/apscheduler` - AsyncScheduler, FastAPI integration, SQLAlchemy data store, interval/cron triggers
- Existing codebase: `src/app/models/sales.py` (TenantBase model pattern), `src/app/events/bus.py` (TenantEventBus), `src/app/core/database.py` (async engine, tenant sessions), `src/app/agents/sales/agent.py` (agent execution pattern), `src/app/api/v1/sales.py` (API endpoint pattern)
- [scikit-learn Probability Calibration docs](https://scikit-learn.org/stable/modules/calibration.html) - Calibration theory, reliability diagrams, proper scoring rules
- [scikit-learn Brier Score Loss](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.brier_score_loss.html) - Brier score definition and usage

### Secondary (MEDIUM confidence)
- [APScheduler GitHub](https://github.com/agronholm/apscheduler) - Version 4.x API, async patterns
- [sse-starlette PyPI](https://pypi.org/project/sse-starlette/) - SSE for FastAPI/Starlette
- [sse-starlette GitHub](https://github.com/sysid/sse-starlette) - Production-ready SSE implementation
- [Welford algorithm Wikipedia](https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance) - Online variance computation
- [welford PyPI](https://pypi.org/project/welford/) - numpy-based Welford implementation

### Tertiary (LOW confidence)
- Web search results for sales engagement platform outcome tracking patterns - general industry direction, not specific implementation guidance
- Web search results for FastAPI real-time dashboard approaches - confirms SSE as standard approach but specific library choice needs validation

## Metadata

**Confidence breakdown:**
- Standard stack: HIGH - All libraries verified via Context7 or official docs; numpy and SQLAlchemy already in stack; APScheduler well-documented with FastAPI integration examples
- Architecture: HIGH - Follows established TenantBase, session_factory, and event bus patterns from Phases 1-4; new module structure mirrors existing `agents/sales/` pattern
- Calibration math: HIGH - Brier score and calibration curves are well-established statistical methods; numpy implementation is straightforward and verified against scikit-learn docs
- Outcome tracking: MEDIUM - Signal detection patterns are application-specific; time-windowed approach is sound but specific signal definitions need iteration during implementation
- Analytics/Dashboard: MEDIUM - API endpoint approach is proven; SSE is verified for real-time; specific metric computations and report formats are application-specific
- Coaching patterns: LOW - Success Criterion 5 (coaching module) is vaguely specified; statistical correlation approach recommended but may need LLM enhancement
- Pitfalls: HIGH - Based on known distributed systems issues (race conditions, cold start), proven calibration gotchas (instability, attribution), and established feedback system design patterns

**Research date:** 2026-02-11
**Valid until:** 2026-03-11 (30 days -- core patterns are stable; APScheduler 4.x API should be verified if starting implementation later)
