---
phase: 10-solution-architect-agent
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - src/knowledge/models.py
  - src/app/handoffs/validators.py
  - src/app/agents/solution_architect/schemas.py
  - src/app/agents/solution_architect/prompts.py
autonomous: true

must_haves:
  truths:
    - "ChunkMetadata accepts competitor_analysis, architecture_template, and poc_template content types"
    - "StrictnessConfig recognizes technical_question and technical_answer handoff types"
    - "SA-specific Pydantic schemas define typed inputs/outputs for all 5 capabilities"
    - "SA prompts establish a pre-sales technical expert persona"
  artifacts:
    - path: "src/knowledge/models.py"
      provides: "Extended content_type Literal with 3 new SA values"
      contains: "competitor_analysis"
    - path: "src/app/handoffs/validators.py"
      provides: "Strictness rules for technical handoff types"
      contains: "technical_question"
    - path: "src/app/agents/solution_architect/schemas.py"
      provides: "Pydantic models for all 5 SA capabilities"
      exports: ["TechnicalRequirementsDoc", "ArchitectureNarrative", "POCPlan", "ObjectionResponse", "TechnicalQuestionPayload", "TechnicalAnswerPayload"]
    - path: "src/app/agents/solution_architect/prompts.py"
      provides: "System and task prompt templates for SA agent"
      contains: "SA_SYSTEM_PROMPT"
  key_links:
    - from: "src/app/agents/solution_architect/schemas.py"
      to: "src/knowledge/models.py"
      via: "content_type values used in RAG filter params"
      pattern: "competitor_analysis|architecture_template|poc_template"
---

<objective>
Create the foundational type system for the Solution Architect agent: extend shared knowledge content types, register handoff types for inter-agent communication, and define all SA-specific Pydantic schemas and prompt templates.

Purpose: Every other SA plan depends on these types. The schemas define the contract between handlers, and the prompts define the SA persona. Without these, the agent class and knowledge seeding cannot proceed.
Output: Extended shared models, 2 new SA module files (schemas.py, prompts.py), and handoff type configuration.
</objective>

<execution_context>
@/Users/RAZER/.claude/get-shit-done/workflows/execute-plan.md
@/Users/RAZER/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/10-solution-architect-agent/10-RESEARCH.md

# Existing patterns to clone
@src/knowledge/models.py
@src/app/handoffs/validators.py
@src/app/agents/sales/schemas.py
@src/app/agents/sales/prompts.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Extend shared models (content types + handoff strictness)</name>
  <files>
    src/knowledge/models.py
    src/app/handoffs/validators.py
  </files>
  <action>
    **In `src/knowledge/models.py`:**
    Extend the `ChunkMetadata.content_type` Literal to add 3 new values. The current Literal is:
    ```python
    content_type: Literal["product", "methodology", "regional", "positioning", "pricing"]
    ```
    Add these values to the end of the Literal union:
    ```python
    content_type: Literal[
        "product", "methodology", "regional", "positioning", "pricing",
        "competitor_analysis", "architecture_template", "poc_template",
    ]
    ```
    This is a safe additive change -- existing documents with old content_type values continue to work. Do NOT modify any other field on ChunkMetadata or any other class in the file.

    **In `src/app/handoffs/validators.py`:**
    In the `StrictnessConfig.__init__` method, add two new entries to the `self._rules` dict:
    ```python
    "technical_question": ValidationStrictness.STRICT,
    "technical_answer": ValidationStrictness.STRICT,
    ```
    Both are STRICT because technical handoffs carry substantive data that must be validated against context. Do NOT modify any other method or class in the file.
  </action>
  <verify>
    Run: `python -c "from src.knowledge.models import ChunkMetadata; m = ChunkMetadata(product_category='monetization', content_type='competitor_analysis', source_document='test'); print('OK:', m.content_type)"`
    Run: `python -c "from src.app.handoffs.validators import StrictnessConfig, ValidationStrictness; c = StrictnessConfig(); assert c.get_strictness('technical_question') == ValidationStrictness.STRICT; print('OK')"`
  </verify>
  <done>
    ChunkMetadata accepts "competitor_analysis", "architecture_template", "poc_template" as valid content_type values. StrictnessConfig returns STRICT for "technical_question" and "technical_answer" handoff types.
  </done>
</task>

<task type="auto">
  <name>Task 2: Create SA Pydantic schemas</name>
  <files>
    src/app/agents/solution_architect/__init__.py (create empty first)
    src/app/agents/solution_architect/schemas.py
  </files>
  <action>
    Create the `src/app/agents/solution_architect/` directory with an empty `__init__.py` (will be populated in plan 02).

    Create `src/app/agents/solution_architect/schemas.py` following the Sales Agent `schemas.py` pattern (Pydantic BaseModel classes with docstrings, Field defaults, enums). Define these models:

    **TechRequirement:**
    - `category`: Literal["integration", "security", "performance", "compliance", "scalability"]
    - `description`: str
    - `priority`: Literal["must_have", "nice_to_have", "dealbreaker"]
    - `source_quote`: str = "" (evidence from transcript)
    - `confidence`: float = Field(default=0.8, ge=0.0, le=1.0)

    **TechnicalRequirementsDoc (SA-01 output):**
    - `requirements`: list[TechRequirement]
    - `summary`: str
    - `confidence`: float = Field(default=0.8, ge=0.0, le=1.0)
    - `source_transcript_hash`: str = "" (for traceability)

    **IntegrationPoint:**
    - `name`: str
    - `integration_type`: Literal["rest_api", "webhook", "database_sync", "event_stream", "file_transfer"]
    - `description`: str
    - `complexity`: Literal["low", "medium", "high"]

    **ArchitectureNarrative (SA-02 output):**
    - `overview`: str
    - `integration_points`: list[IntegrationPoint]
    - `diagram_description`: str
    - `assumptions`: list[str] = Field(default_factory=list)
    - `prospect_tech_stack`: str = ""

    **POCDeliverable:**
    - `name`: str
    - `description`: str
    - `acceptance_criteria`: str

    **ResourceEstimate:**
    - `developer_days`: int
    - `qa_days`: int
    - `pm_hours`: int

    **POCPlan (SA-03 output):**
    - `deliverables`: list[POCDeliverable]
    - `timeline_weeks`: int
    - `resource_estimate`: ResourceEstimate
    - `success_criteria`: list[str]
    - `risks`: list[str] = Field(default_factory=list)
    - `tier`: Literal["small", "medium", "large"]

    **Evidence:**
    - `claim`: str
    - `source_doc`: str
    - `confidence`: float = Field(default=0.8, ge=0.0, le=1.0)

    **ObjectionResponse (SA-04 output):**
    - `response`: str
    - `evidence`: list[Evidence]
    - `recommended_followup`: str
    - `competitor_name`: str = ""

    **TechnicalQuestionPayload (SA-05 inbound from Sales Agent):**
    - `question`: str
    - `deal_id`: str
    - `prospect_tech_stack`: str | None = None
    - `context_chunks`: list[str] = Field(default_factory=list)

    **TechnicalAnswerPayload (SA-05 outbound to Sales Agent):**
    - `answer`: str
    - `evidence`: list[str] = Field(default_factory=list)
    - `architecture_diagram_url`: str | None = None
    - `related_docs`: list[str] = Field(default_factory=list)
    - `confidence`: float = Field(default=0.8, ge=0.0, le=1.0)

    Add proper module docstring and `__all__` export list. Use `from __future__ import annotations` at the top.
  </action>
  <verify>
    Run: `python -c "from src.app.agents.solution_architect.schemas import TechnicalRequirementsDoc, ArchitectureNarrative, POCPlan, ObjectionResponse, TechnicalQuestionPayload, TechnicalAnswerPayload; print('All schemas importable')"`
    Run: `python -c "from src.app.agents.solution_architect.schemas import TechRequirement; r = TechRequirement(category='integration', description='REST API needed', priority='must_have'); print('OK:', r.category)"`
  </verify>
  <done>
    All 11 Pydantic models are importable from `src/app/agents/solution_architect/schemas.py`. Each model validates field types and constraints correctly. TechRequirement rejects invalid category/priority values.
  </done>
</task>

<task type="auto">
  <name>Task 3: Create SA prompt templates</name>
  <files>src/app/agents/solution_architect/prompts.py</files>
  <action>
    Create `src/app/agents/solution_architect/prompts.py` following the Sales Agent `prompts.py` pattern (module-level string constants for system prompts, function builders for task prompts).

    **SA_SYSTEM_PROMPT** (module-level constant):
    ```
    You are the Solution Architect for Skyvera, a technical pre-sales expert with deep expertise
    in enterprise software integration, API design, cloud architecture, and solution scoping.

    Your role is to translate business requirements into technical solutions, design integration
    architectures, scope proof-of-concept engagements, and handle technical objections with
    evidence-based responses.

    Your outputs are used directly in sales conversations -- be precise, confident, and concise.
    Always ground your responses in specific technical details and reference source documentation.
    When uncertain, state your confidence level explicitly rather than fabricating details.
    ```

    **build_requirements_extraction_prompt(transcript: str, deal_context: dict, rag_context: str) -> list[dict[str, str]]:**
    Returns messages list for LLM call. System message = SA_SYSTEM_PROMPT. User message instructs extraction of technical requirements from the transcript into categorized TechRequirement objects. Include the deal context and RAG knowledge as context sections. Instruct the LLM to respond with JSON matching TechnicalRequirementsDoc schema.

    **build_architecture_narrative_prompt(tech_stack: str, requirements_json: str, rag_context: str) -> list[dict[str, str]]:**
    Returns messages list. Instructs generation of an integration architecture narrative for the prospect's tech stack. Include the requirements and architecture template from RAG as context. Instruct JSON output matching ArchitectureNarrative schema.

    **build_poc_scoping_prompt(requirements_json: str, deal_stage: str, timeline_preference: str, rag_context: str) -> list[dict[str, str]]:**
    Returns messages list. Instructs scoping a POC from the requirements and POC template. Include tier selection logic (small: <10 dev days, medium: 10-30, large: 30+). Instruct JSON output matching POCPlan schema.

    **build_objection_response_prompt(objection: str, competitor: str | None, deal_context: dict, rag_context: str) -> list[dict[str, str]]:**
    Returns messages list. Instructs generating an evidence-based response to the technical objection. If competitor is provided, pull from competitor_analysis knowledge. Instruct JSON output matching ObjectionResponse schema.

    **build_technical_handoff_prompt(question: str, deal_context: dict, rag_context: str) -> list[dict[str, str]]:**
    Returns messages list. Instructs answering a technical question handed off from the Sales Agent. Classify the question type first (requirements/architecture/poc/objection), then provide a structured answer. Instruct JSON output matching TechnicalAnswerPayload schema.

    Each prompt builder must include explicit JSON schema instructions telling the LLM the exact field names and types expected. Use the "Respond with ONLY a JSON object" instruction pattern from the existing router.py.

    Add `__all__` with all exports. Use `from __future__ import annotations`.
  </action>
  <verify>
    Run: `python -c "from src.app.agents.solution_architect.prompts import SA_SYSTEM_PROMPT, build_requirements_extraction_prompt, build_architecture_narrative_prompt, build_poc_scoping_prompt, build_objection_response_prompt, build_technical_handoff_prompt; msgs = build_requirements_extraction_prompt('test transcript', {}, 'rag ctx'); assert len(msgs) == 2; assert msgs[0]['role'] == 'system'; print('OK:', len(SA_SYSTEM_PROMPT), 'chars')"`
  </verify>
  <done>
    SA_SYSTEM_PROMPT is a non-empty string establishing the SA persona. All 5 prompt builder functions return a 2-element messages list (system + user) with proper role keys. Each user message includes JSON schema instructions for the expected output format.
  </done>
</task>

</tasks>

<verification>
1. `python -c "from src.knowledge.models import ChunkMetadata"` -- no import errors
2. `python -c "from src.app.handoffs.validators import StrictnessConfig"` -- no import errors
3. `python -c "from src.app.agents.solution_architect.schemas import *"` -- all schemas importable
4. `python -c "from src.app.agents.solution_architect.prompts import *"` -- all prompts importable
5. Existing tests still pass: `python -m pytest tests/ -x -q --timeout=30` (no regressions from Literal extension)
</verification>

<success_criteria>
- ChunkMetadata.content_type accepts 8 values (5 original + 3 new SA values)
- StrictnessConfig has rules for technical_question and technical_answer
- 11 Pydantic schemas exist in solution_architect/schemas.py with proper validation
- 5 prompt builder functions exist in solution_architect/prompts.py
- All existing tests pass (no regressions)
</success_criteria>

<output>
After completion, create `.planning/phases/10-solution-architect-agent/10-01-SUMMARY.md`
</output>
