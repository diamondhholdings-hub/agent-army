---
phase: 12-business-analyst-agent
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - src/app/agents/business_analyst/__init__.py
  - src/app/agents/business_analyst/schemas.py
  - src/app/agents/business_analyst/prompts.py
  - src/app/handoffs/validators.py
autonomous: true

must_haves:
  truths:
    - "BA schemas define all four task types with Pydantic validation"
    - "BA handoff type requirements_analysis is registered in StrictnessConfig"
    - "Prompt builders embed JSON schema in user message for structured LLM output"
  artifacts:
    - path: "src/app/agents/business_analyst/schemas.py"
      provides: "All BA domain models, handoff payloads, and task/result types"
      exports: ["BATask", "BAResult", "ExtractedRequirement", "UserStory", "CapabilityGap", "RequirementContradiction", "ProcessDocumentation", "GapAnalysisResult", "BAHandoffRequest", "BAHandoffResponse"]
    - path: "src/app/agents/business_analyst/prompts.py"
      provides: "System prompt + 4 prompt builders for each task type"
      exports: ["BA_SYSTEM_PROMPT", "build_requirements_extraction_prompt", "build_gap_analysis_prompt", "build_user_story_generation_prompt", "build_process_documentation_prompt"]
    - path: "src/app/agents/business_analyst/__init__.py"
      provides: "Package init (minimal, expanded in plan 02)"
    - path: "src/app/handoffs/validators.py"
      provides: "requirements_analysis handoff type registered as STRICT"
      contains: "requirements_analysis"
  key_links:
    - from: "src/app/agents/business_analyst/prompts.py"
      to: "src/app/agents/business_analyst/schemas.py"
      via: "model_json_schema() embedded in prompts"
      pattern: "model_json_schema|json_schema"
    - from: "src/app/handoffs/validators.py"
      to: "StrictnessConfig._rules"
      via: "requirements_analysis -> STRICT mapping"
      pattern: "requirements_analysis.*STRICT"
---

<objective>
Create the BA agent's foundational Pydantic schemas and LLM prompt templates, plus register the BA handoff type in the handoff validator.

Purpose: Schemas and prompts are the foundation that every other BA plan depends on. Getting these right first means handlers, Notion adapter, and tests all have stable types to import.
Output: schemas.py (all domain models), prompts.py (system prompt + 4 builders), __init__.py (minimal package), validators.py update (requirements_analysis STRICT)
</objective>

<execution_context>
@/Users/RAZER/.claude/get-shit-done/workflows/execute-plan.md
@/Users/RAZER/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/12-business-analyst-agent/12-CONTEXT.md
@.planning/phases/12-business-analyst-agent/12-RESEARCH.md

# Pattern references -- clone these structures
@src/app/agents/solution_architect/schemas.py
@src/app/agents/solution_architect/prompts.py
@src/app/agents/project_manager/schemas.py
@src/app/handoffs/validators.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create BA Pydantic schemas and handoff payloads</name>
  <files>
    src/app/agents/business_analyst/__init__.py
    src/app/agents/business_analyst/schemas.py
    src/app/handoffs/validators.py
  </files>
  <action>
Create `src/app/agents/business_analyst/` directory and files.

**schemas.py** -- Follow the SA schemas.py pattern exactly. Define these models:

1. `ExtractedRequirement(BaseModel)`:
   - `requirement_id: str` (unique ID like "REQ-001")
   - `description: str`
   - `category: Literal["functional", "non_functional", "constraint"]`
   - `moscow_priority: Literal["must_have", "should_have", "could_have", "wont_have"]`
   - `stakeholder_domain: Literal["sales", "tech", "ops", "finance"]`
   - `priority_score: Literal["high", "med", "low"]`
   - `extraction_confidence: float = Field(default=0.7, ge=0.0, le=1.0)`
   - `is_low_confidence: bool = False` (computed: True if extraction_confidence < 0.6)
   - `source_quote: str = ""`
   Use a `model_validator(mode="after")` to auto-set `is_low_confidence` from `extraction_confidence < 0.6`.

2. `CapabilityGap(BaseModel)`:
   - `requirement_id: str` (links back to the requirement)
   - `gap_description: str`
   - `severity: Literal["critical", "major", "minor"]`
   - `recommended_action: Literal["build_it", "find_partner", "descope"]`
   - `workaround: str | None = None`
   - `requires_sa_escalation: bool = False`

3. `RequirementContradiction(BaseModel)`:
   - `requirement_ids: list[str]` (the conflicting requirement IDs, min 2)
   - `conflict_description: str`
   - `resolution_suggestion: str`
   - `severity: Literal["blocking", "significant", "minor"]`

4. `UserStory(BaseModel)`:
   - `story_id: str`
   - `as_a: str`
   - `i_want: str`
   - `so_that: str`
   - `acceptance_criteria: list[str]` (min 1 item)
   - `story_points: int` -- validate with `Field(ge=1)` and a validator ensuring value is in [1,2,3,5,8,13] (Fibonacci)
   - `priority: Literal["must_have", "should_have", "could_have", "wont_have"]`
   - `epic_theme: str`
   - `stakeholder_domain: Literal["sales", "tech", "ops", "finance"]`
   - `is_low_confidence: bool = False`
   - `source_requirement_ids: list[str] = Field(default_factory=list)` (traceability)

5. `ProcessDocumentation(BaseModel)`:
   - `process_name: str`
   - `current_state: str` (narrative description)
   - `future_state: str` (narrative description)
   - `delta: str` (what changes between current and future)
   - `stakeholders: list[str]`
   - `assumptions: list[str] = Field(default_factory=list)`

6. `GapAnalysisResult(BaseModel)`:
   - `requirements: list[ExtractedRequirement]`
   - `gaps: list[CapabilityGap]`
   - `contradictions: list[RequirementContradiction]`
   - `coverage_percentage: float = Field(ge=0.0, le=100.0)`
   - `recommended_next_action: str` (for Sales Agent consumption)
   - `requires_sa_escalation: bool = False`

7. `BATask(BaseModel)`:
   - `task_type: Literal["requirements_extraction", "gap_analysis", "user_story_generation", "process_documentation"]`
   - `conversation_text: str`
   - `deal_id: str | None = None`
   - `tenant_id: str`
   - `existing_requirements: list[ExtractedRequirement] = Field(default_factory=list)` (for user_story_generation, pass previously extracted requirements)
   - `metadata: dict[str, Any] = Field(default_factory=dict)`

8. `BAResult(BaseModel)`:
   - `task_type: str`
   - `requirements: list[ExtractedRequirement] = Field(default_factory=list)`
   - `gap_analysis: GapAnalysisResult | None = None`
   - `user_stories: list[UserStory] = Field(default_factory=list)`
   - `process_documentation: ProcessDocumentation | None = None`
   - `error: str | None = None`
   - `confidence: Literal["high", "medium", "low"] = "medium"`
   - `partial: bool = False`

9. `BAHandoffRequest(BaseModel)`:
   - `handoff_type: Literal["requirements_analysis"] = "requirements_analysis"`
   - `conversation_text: str`
   - `deal_id: str`
   - `tenant_id: str`
   - `analysis_scope: Literal["full", "gap_only", "stories_only", "process_only"] = "full"`

10. `BAHandoffResponse(BaseModel)`:
    - `handoff_type: Literal["requirements_analysis"] = "requirements_analysis"`
    - `requirements: list[ExtractedRequirement] = Field(default_factory=list)`
    - `gap_analysis: GapAnalysisResult | None = None`
    - `user_stories: list[UserStory] = Field(default_factory=list)`
    - `process_documentation: ProcessDocumentation | None = None`
    - `recommended_next_action: str = ""`
    - `confidence: float = Field(default=0.7, ge=0.0, le=1.0)`

Include `__all__` listing all exported models.

Use `from __future__ import annotations` at top. Import `Any` from typing where needed.

**__init__.py** -- Create minimal package init that just imports schemas (will be expanded in plan 02 when agent.py and capabilities.py exist):
```python
"""Business Analyst Agent for requirements gathering and analysis."""
from src.app.agents.business_analyst.schemas import (
    BAHandoffRequest,
    BAHandoffResponse,
    BAResult,
    BATask,
    CapabilityGap,
    ExtractedRequirement,
    GapAnalysisResult,
    ProcessDocumentation,
    RequirementContradiction,
    UserStory,
)
```

**validators.py** -- Add `"requirements_analysis": ValidationStrictness.STRICT` to `StrictnessConfig.__init__._rules` dict, with a comment `# BA agent handoff types`. Place it after the PM agent handoff types block.
  </action>
  <verify>
Run: `cd "/Users/RAZER/Documents/projects/sales army" && python -c "from src.app.agents.business_analyst.schemas import BATask, BAResult, ExtractedRequirement, UserStory, GapAnalysisResult, BAHandoffRequest, BAHandoffResponse; print('All BA schemas import OK')"` -- must succeed.

Run: `cd "/Users/RAZER/Documents/projects/sales army" && python -c "
from src.app.agents.business_analyst.schemas import ExtractedRequirement, UserStory
r = ExtractedRequirement(requirement_id='REQ-001', description='test', category='functional', moscow_priority='must_have', stakeholder_domain='tech', priority_score='high', extraction_confidence=0.5)
assert r.is_low_confidence == True, 'Low confidence flag not auto-set'
s = UserStory(story_id='US-001', as_a='user', i_want='feature', so_that='value', acceptance_criteria=['test'], story_points=5, priority='must_have', epic_theme='core', stakeholder_domain='tech')
print('Schema validation OK')
"` -- must succeed.

Run: `cd "/Users/RAZER/Documents/projects/sales army" && python -c "from src.app.handoffs.validators import StrictnessConfig, ValidationStrictness; c = StrictnessConfig(); assert c.get_strictness('requirements_analysis') == ValidationStrictness.STRICT; print('Handoff registration OK')"` -- must succeed.
  </verify>
  <done>
All 10 BA Pydantic models import and validate correctly. ExtractedRequirement auto-flags low confidence below 0.6. UserStory validates Fibonacci story points. requirements_analysis handoff type registered as STRICT in StrictnessConfig.
  </done>
</task>

<task type="auto">
  <name>Task 2: Create BA prompt templates with JSON schema embedding</name>
  <files>src/app/agents/business_analyst/prompts.py</files>
  <action>
Create `prompts.py` following the SA prompts.py pattern. Key pattern: each prompt builder calls `Model.model_json_schema()` and embeds the JSON schema directly in the user message so the LLM returns parseable JSON.

Define:

1. `BA_SYSTEM_PROMPT` -- System prompt establishing the BA agent's role:
   "You are a Business Analyst agent specializing in requirements engineering. You extract structured requirements from conversations, perform gap analysis against product capabilities, detect contradictions, generate user stories, and produce process documentation. Always return valid JSON matching the provided schema. Be thorough but precise -- extract only what the conversation evidence supports."

2. `build_requirements_extraction_prompt(conversation_text: str, deal_context: dict | None = None) -> str`:
   - Embed `ExtractedRequirement.model_json_schema()` in the prompt
   - Instruct LLM to extract ALL requirements from the conversation, categorize them using all three schemes simultaneously (functional/non_functional/constraint, MoSCoW, stakeholder_domain)
   - Include extraction_confidence scoring guidance: 0.9+ for explicit statements, 0.6-0.9 for implied, below 0.6 for inferred/uncertain
   - If deal_context provided, include it as additional context
   - Return instruction to output a JSON array of ExtractedRequirement objects

3. `build_gap_analysis_prompt(requirements: list[dict], capability_chunks: list[str]) -> str`:
   - Embed `GapAnalysisResult.model_json_schema()` (which includes CapabilityGap and RequirementContradiction schemas)
   - Pass the requirements list and product capability chunks
   - Instruct LLM to: (a) compare each requirement against capabilities, (b) identify gaps with recommended_action per gap (build_it/find_partner/descope), (c) detect contradictions between requirements, (d) compute coverage_percentage, (e) set requires_sa_escalation=True for any gap with no workaround and severity=critical
   - Instruct to provide a recommended_next_action string summarizing what the Sales Agent should do

4. `build_user_story_generation_prompt(requirements: list[dict], group_by_context: str | None = None) -> str`:
   - Embed `UserStory.model_json_schema()`
   - Pass requirements to convert into stories
   - Instruct: standard agile format (As-a / I-want / So-that), acceptance criteria (min 2 per story), story points (Fibonacci: 1,2,3,5,8,13), group by both epic_theme AND stakeholder_domain
   - Low-confidence requirements -> include story but set is_low_confidence=True
   - Link back via source_requirement_ids

5. `build_process_documentation_prompt(conversation_text: str, process_context: dict | None = None) -> str`:
   - Embed `ProcessDocumentation.model_json_schema()`
   - Instruct: extract process name, describe current_state, describe future_state, compute delta (what changes), identify stakeholders
   - If process_context provided (e.g., industry, department), include as context

All builders must return `str`. Use `json.dumps(schema, indent=2)` for schema embedding. Import json at top of file.
  </action>
  <verify>
Run: `cd "/Users/RAZER/Documents/projects/sales army" && python -c "
from src.app.agents.business_analyst.prompts import (
    BA_SYSTEM_PROMPT,
    build_requirements_extraction_prompt,
    build_gap_analysis_prompt,
    build_user_story_generation_prompt,
    build_process_documentation_prompt,
)
p1 = build_requirements_extraction_prompt('We need SSO and must have API access')
assert 'extraction_confidence' in p1, 'Schema not embedded in extraction prompt'
p2 = build_gap_analysis_prompt([{'description': 'SSO'}], ['We support SAML'])
assert 'coverage_percentage' in p2, 'Schema not embedded in gap analysis prompt'
p3 = build_user_story_generation_prompt([{'description': 'SSO'}])
assert 'acceptance_criteria' in p3, 'Schema not embedded in story prompt'
p4 = build_process_documentation_prompt('Current flow: manual entry. Future: automated.')
assert 'current_state' in p4, 'Schema not embedded in process doc prompt'
print('All prompts build OK with embedded schemas')
"` -- must succeed.
  </verify>
  <done>
All 4 prompt builders produce prompts with embedded JSON schemas. BA_SYSTEM_PROMPT defined. Each builder follows the SA pattern of embedding model_json_schema() in user message text.
  </done>
</task>

</tasks>

<verification>
1. `python -c "from src.app.agents.business_analyst import BATask, BAResult, ExtractedRequirement"` -- imports succeed
2. `python -c "from src.app.agents.business_analyst.prompts import BA_SYSTEM_PROMPT"` -- imports succeed
3. `python -c "from src.app.handoffs.validators import StrictnessConfig; c = StrictnessConfig(); print(c.get_strictness('requirements_analysis'))"` -- prints "strict"
4. Existing tests still pass: `cd "/Users/RAZER/Documents/projects/sales army" && python -m pytest tests/ -x -q --timeout=30 2>&1 | tail -5`
</verification>

<success_criteria>
- All 10 BA Pydantic schemas defined with proper validation (ExtractedRequirement auto-flags low confidence, UserStory validates Fibonacci)
- 4 prompt builders + system prompt, all embedding JSON schemas in prompts
- requirements_analysis handoff type registered as STRICT
- Existing test suite unbroken
</success_criteria>

<output>
After completion, create `.planning/phases/12-business-analyst-agent/12-01-SUMMARY.md`
</output>
