---
phase: 12-business-analyst-agent
plan: 04
type: execute
wave: 3
depends_on: ["12-02", "12-03"]
files_modified:
  - src/app/main.py
  - tests/test_business_analyst.py
autonomous: true

must_haves:
  truths:
    - "BA agent initializes during app startup and registers in AgentRegistry"
    - "All 4 BA capability handlers return correct structured output"
    - "Unknown task type returns error dict, not exception"
    - "LLM failure triggers fail-open response"
  artifacts:
    - path: "src/app/main.py"
      provides: "BA agent wiring in lifespan (Phase 12 block)"
      contains: "business_analyst"
    - path: "tests/test_business_analyst.py"
      provides: "Integration tests covering all 4 handlers + error cases + registration"
      min_lines: 200
  key_links:
    - from: "src/app/main.py"
      to: "src/app/agents/business_analyst"
      via: "import and instantiate BusinessAnalystAgent in lifespan"
      pattern: "from src.app.agents.business_analyst import"
    - from: "tests/test_business_analyst.py"
      to: "src/app/agents/business_analyst"
      via: "import agent + schemas for test assertions"
      pattern: "from src.app.agents.business_analyst"
---

<objective>
Wire the BA agent into the application lifespan and create comprehensive integration tests covering all 4 handlers, error handling, and registration.

Purpose: Makes the BA agent live in the application and proves all handlers work correctly with mocked LLM/RAG.
Output: main.py updated with BA initialization block, test_business_analyst.py with full test coverage
</objective>

<execution_context>
@/Users/RAZER/.claude/get-shit-done/workflows/execute-plan.md
@/Users/RAZER/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/12-business-analyst-agent/12-CONTEXT.md
@.planning/phases/12-business-analyst-agent/12-RESEARCH.md

# Pattern references
@src/app/main.py
@tests/test_solution_architect.py
@tests/test_project_manager.py

# Dependencies from plans 01-03
@src/app/agents/business_analyst/schemas.py
@src/app/agents/business_analyst/agent.py
@src/app/agents/business_analyst/capabilities.py
@src/app/agents/business_analyst/notion_ba.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Wire BA agent into main.py lifespan</name>
  <files>src/app/main.py</files>
  <action>
Add a Phase 12 BA agent initialization block in `main.py` lifespan, following the exact same pattern as Phase 10 (SA) and Phase 11 (PM). Place it AFTER the Phase 11 PM block and BEFORE the Phase 5 Deal Management block.

```python
    # -- Phase 12: Business Analyst Agent ----------------------------------
    # Requirements engineering agent. Follows the SA/PM Agent pattern:
    # instantiate with shared services, register in AgentRegistry.
    # Fail-tolerant -- BA unavailability does not prevent app startup.

    try:
        from src.app.agents.business_analyst import (
            BusinessAnalystAgent,
            create_ba_registration,
        )

        ba_registration = create_ba_registration()

        ba_agent = BusinessAnalystAgent(
            registration=ba_registration,
            llm_service=getattr(app.state, "llm_service", None)
            or locals().get("llm_service"),
            rag_pipeline=getattr(app.state, "rag_pipeline", None)
            or locals().get("rag_pipeline"),
        )

        # Register in agent registry
        agent_registry = getattr(app.state, "agent_registry", None)
        if agent_registry is not None:
            agent_registry.register(ba_registration)
            ba_registration._agent_instance = ba_agent
        app.state.business_analyst = ba_agent
        log.info("phase12.business_analyst_initialized")
    except Exception as exc:
        log.warning("phase12.business_analyst_init_failed", error=str(exc))
```

Key details:
- Same pattern as SA/PM: try/except, import inside try, create registration, instantiate agent with shared llm_service + rag_pipeline, register in agent_registry, store on app.state
- No notion_ba argument in constructor (NotionBAAdapter is configured separately when the BA database is initialized, same as PM)
- Fail-tolerant: logs warning but does not prevent startup
  </action>
  <verify>
Run: `cd "/Users/RAZER/Documents/projects/sales army" && python -c "
# Verify the wiring code is syntactically correct by importing main
import ast
with open('src/app/main.py') as f:
    tree = ast.parse(f.read())
print('main.py parses OK')
"` -- must succeed.

Grep for the BA block: `grep -n 'phase12.business_analyst' src/app/main.py` -- must find the log line.
  </verify>
  <done>
BA agent initialization block added to main.py lifespan, positioned after PM and before Deal Management. Follows identical pattern to SA/PM wiring.
  </done>
</task>

<task type="auto">
  <name>Task 2: Create comprehensive integration tests</name>
  <files>tests/test_business_analyst.py</files>
  <action>
Create `tests/test_business_analyst.py` following the test_solution_architect.py and test_project_manager.py patterns. All external dependencies (LLM, RAG) are mocked.

**Test setup:**
```python
import json
from unittest.mock import AsyncMock
import pytest
from src.app.agents.business_analyst import (
    BusinessAnalystAgent, create_ba_registration,
)
from src.app.agents.business_analyst.schemas import (
    ExtractedRequirement, UserStory, GapAnalysisResult,
    CapabilityGap, RequirementContradiction, ProcessDocumentation, BAResult,
)
```

**Mock LLM response factory functions** (same pattern as test_solution_architect.py):

1. `_make_requirements_json() -> str` -- Returns JSON array of 3 ExtractedRequirement objects (one with extraction_confidence=0.4 to test low-confidence flagging)

2. `_make_gap_analysis_json() -> str` -- Returns GapAnalysisResult JSON with 2 requirements, 1 gap (recommended_action="build_it"), 1 contradiction, coverage_percentage=75.0

3. `_make_user_stories_json() -> str` -- Returns JSON array of 2 UserStory objects (one is_low_confidence=True), with proper Fibonacci story_points

4. `_make_process_doc_json() -> str` -- Returns ProcessDocumentation JSON with all fields populated

**Agent fixture:**
```python
@pytest.fixture
def ba_agent():
    registration = create_ba_registration()
    mock_llm = AsyncMock()
    mock_rag = AsyncMock()
    agent = BusinessAnalystAgent(
        registration=registration,
        llm_service=mock_llm,
        rag_pipeline=mock_rag,
    )
    return agent, mock_llm, mock_rag
```

**Tests (minimum 14):**

1. `test_registration_correctness` -- create_ba_registration returns agent_id="business_analyst", 4 capabilities
2. `test_requirements_extraction_handler` -- Mock LLM to return _make_requirements_json(). Call execute with type="requirements_extraction". Assert result has requirements list, no error.
3. `test_requirements_extraction_low_confidence_flagging` -- Verify requirement with confidence=0.4 has is_low_confidence=True in result
4. `test_gap_analysis_handler` -- Mock LLM for both extraction (if needed) and gap analysis. Assert result has gap_analysis with gaps and contradictions.
5. `test_gap_analysis_includes_contradictions` -- Verify contradictions are in the same GapAnalysisResult output (not a separate call)
6. `test_gap_analysis_coverage_percentage` -- Assert coverage_percentage is present and within 0-100
7. `test_user_story_generation_handler` -- Mock LLM to return _make_user_stories_json(). Assert result has user_stories list.
8. `test_user_story_fibonacci_validation` -- Verify story_points are valid Fibonacci numbers
9. `test_user_story_low_confidence_flagged` -- Verify low-confidence stories included but flagged
10. `test_process_documentation_handler` -- Mock LLM to return _make_process_doc_json(). Assert result has process_documentation.
11. `test_unknown_task_type_returns_error` -- Call with type="nonexistent". Assert error in result, confidence="low", partial=True.
12. `test_llm_failure_fail_open` -- Mock LLM to raise Exception. Assert error dict returned (not exception raised).
13. `test_handoff_payload_construction` -- Create BAHandoffRequest and BAHandoffResponse, verify they serialize/deserialize correctly
14. `test_notion_block_renderers` -- Import and call all 4 renderers with valid data, assert non-empty block lists returned

All tests use `@pytest.mark.asyncio` where needed. Follow the exact mock pattern from test_solution_architect.py for LLM response mocking.
  </action>
  <verify>
Run: `cd "/Users/RAZER/Documents/projects/sales army" && python -m pytest tests/test_business_analyst.py -v --timeout=30 2>&1 | tail -30` -- all tests must pass.

Run: `cd "/Users/RAZER/Documents/projects/sales army" && python -m pytest tests/test_business_analyst.py --co -q 2>&1` -- must list at least 14 tests.
  </verify>
  <done>
14+ integration tests pass covering all 4 handlers, error handling (unknown type, LLM failure), low-confidence flagging, Fibonacci validation, handoff payloads, and Notion renderers.
  </done>
</task>

</tasks>

<verification>
1. `python -m pytest tests/test_business_analyst.py -v` -- all tests pass
2. `grep 'phase12.business_analyst' src/app/main.py` -- wiring exists
3. `python -m pytest tests/ -x -q --timeout=30 2>&1 | tail -5` -- full test suite passes
</verification>

<success_criteria>
- BA agent initializes in main.py lifespan (fail-tolerant)
- 14+ integration tests pass covering all handlers, error paths, and edge cases
- Full existing test suite still passes
</success_criteria>

<output>
After completion, create `.planning/phases/12-business-analyst-agent/12-04-SUMMARY.md`
</output>
