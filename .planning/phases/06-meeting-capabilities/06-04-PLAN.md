---
phase: 06-meeting-capabilities
plan: 04
type: execute
wave: 3
depends_on: ["06-01", "06-03"]
files_modified:
  - src/app/meetings/realtime/turn_detector.py
  - src/app/meetings/realtime/silence_checker.py
  - src/app/meetings/realtime/pipeline.py
  - meeting-bot-webapp/package.json
  - meeting-bot-webapp/src/index.html
  - meeting-bot-webapp/src/app.js
  - meeting-bot-webapp/src/pipeline/deepgram-stt.js
  - meeting-bot-webapp/src/pipeline/llm-bridge.js
  - meeting-bot-webapp/src/avatar/heygen-session.js
  - meeting-bot-webapp/src/utils/audio-processor.js
  - tests/test_realtime_pipeline.py
autonomous: true

must_haves:
  truths:
    - "TurnDetector distinguishes thinking pauses (2-3s) from end-of-turn pauses (1s)"
    - "SilenceChecker enforces all three strategic silence rules BEFORE speaking: not during thinking pause, not when internal rep speaking, not when confidence below threshold (confidence checked post-LLM but pre-TTS/avatar delivery)"
    - "RealtimePipeline orchestrates streaming STT->LLM->TTS flow with sub-1s latency target"
    - "Output Media webapp captures meeting audio, routes through STT, receives LLM responses, and renders HeyGen avatar"
    - "Agent never interrupts -- strict turn-taking even with active participation posture"
  artifacts:
    - path: "src/app/meetings/realtime/turn_detector.py"
      provides: "TurnDetector for distinguishing pause types"
      exports: ["TurnDetector"]
      min_lines: 50
    - path: "src/app/meetings/realtime/silence_checker.py"
      provides: "SilenceChecker implementing strategic silence rules from CONTEXT.md"
      exports: ["SilenceChecker"]
      min_lines: 60
    - path: "src/app/meetings/realtime/pipeline.py"
      provides: "RealtimePipeline orchestrating STT->LLM->TTS->Avatar"
      exports: ["RealtimePipeline", "PipelineMetrics"]
      min_lines: 120
    - path: "meeting-bot-webapp/src/app.js"
      provides: "Output Media webapp main orchestrator"
      min_lines: 100
    - path: "meeting-bot-webapp/src/pipeline/deepgram-stt.js"
      provides: "Browser-side Deepgram STT WebSocket client"
      min_lines: 40
    - path: "meeting-bot-webapp/src/avatar/heygen-session.js"
      provides: "HeyGen LiveKit avatar session in browser"
      min_lines: 60
  key_links:
    - from: "src/app/meetings/realtime/silence_checker.py"
      to: "src/app/meetings/realtime/turn_detector.py"
      via: "TurnDetector for pause classification"
      pattern: "is_end_of_turn|is_thinking_pause"
    - from: "src/app/meetings/realtime/pipeline.py"
      to: "src/app/meetings/realtime/silence_checker.py"
      via: "SilenceChecker.should_respond called before every response"
      pattern: "should_respond"
    - from: "src/app/meetings/realtime/pipeline.py"
      to: "src/app/meetings/realtime/stt.py"
      via: "DeepgramSTT for streaming transcription"
      pattern: "DeepgramSTT"
    - from: "meeting-bot-webapp/src/app.js"
      to: "meeting-bot-webapp/src/pipeline/llm-bridge.js"
      via: "WebSocket to backend for LLM reasoning"
      pattern: "LLMBridge"
---

<objective>
Build the real-time meeting response pipeline -- TurnDetector, SilenceChecker, RealtimePipeline orchestrator on the backend, and the Output Media webapp that Recall.ai renders as the bot's camera and audio source.

Purpose: This is the critical path for Phase 6 -- the real-time pipeline that enables the agent to listen, reason, and speak in meetings with sub-1s latency. The backend pipeline orchestrates the STT->LLM->TTS flow with strategic silence checks. The webapp runs inside Recall.ai's headless browser, capturing meeting audio, routing it through Deepgram, and rendering the HeyGen avatar.

Output: Real-time pipeline (Python backend) + Output Media webapp (vanilla JS) + unit tests.
</objective>

<execution_context>
@/Users/RAZER/.claude/get-shit-done/workflows/execute-plan.md
@/Users/RAZER/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/06-meeting-capabilities/06-CONTEXT.md
@.planning/phases/06-meeting-capabilities/06-RESEARCH.md
@.planning/phases/06-meeting-capabilities/06-01-SUMMARY.md
@.planning/phases/06-meeting-capabilities/06-03-SUMMARY.md

@src/app/meetings/schemas.py
@src/app/meetings/realtime/stt.py
@src/app/meetings/realtime/tts.py
@src/app/meetings/realtime/avatar.py
@src/app/meetings/bot/manager.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: TurnDetector, SilenceChecker, and RealtimePipeline orchestrator</name>
  <files>
    src/app/meetings/realtime/turn_detector.py
    src/app/meetings/realtime/silence_checker.py
    src/app/meetings/realtime/pipeline.py
    tests/test_realtime_pipeline.py
  </files>
  <action>
    1. Create `src/app/meetings/realtime/turn_detector.py` with `TurnDetector`:
       - Tracks per-speaker silence duration using timestamps
       - Constants: THINKING_PAUSE_MS = 2000, END_OF_TURN_MS = 1000 (per CONTEXT.md)
       - `update_speaker_activity(speaker_id: str, is_speaking: bool) -> None`: track when speakers start/stop
       - `get_silence_duration_ms(speaker_id: str) -> float`: milliseconds since speaker last spoke
       - `is_end_of_turn(speaker_id: str) -> bool`: True if silence >= END_OF_TURN_MS
       - `is_thinking_pause(speaker_id: str) -> bool`: True if silence between END_OF_TURN_MS and THINKING_PAUSE_MS
       - `get_active_speakers() -> list[str]`: speakers currently talking (silence < 300ms)
       - Uses asyncio.get_event_loop().time() for timestamp tracking

    2. Create `src/app/meetings/realtime/silence_checker.py` with `SilenceChecker`:
       - Constructor takes: `turn_detector: TurnDetector`, `participant_roles: dict[str, ParticipantRole]`
       - Constants: CONFIDENCE_THRESHOLD = 0.7

       - `async should_respond(transcript: str, speaker_id: str, meeting_context: dict, confidence_score: float = 1.0) -> bool`:
         Implements ALL THREE strategic silence rules from CONTEXT.md (checked BEFORE speaking):
         a. Customer not still thinking: check turn_detector.is_end_of_turn(speaker_id)
            - If is_thinking_pause: return False (don't rush customer)
         b. Internal rep not speaking: check turn_detector.get_active_speakers() against participant_roles
            - If any active speaker has role INTERNAL: return False (never talk over human salesperson)
         c. Confidence above threshold: if confidence_score < CONFIDENCE_THRESHOLD: return False
            - Per CONTEXT.md locked decision: ALL THREE checks MUST pass before speaking
            - This is called twice in the pipeline flow:
              1. Pre-LLM call (checks a + b only, confidence_score defaults to 1.0)
              2. Post-LLM/pre-TTS call (checks all three, with actual confidence from LLM response)
         Returns True only if ALL three conditions pass

       - `update_participant_role(speaker_id: str, role: ParticipantRole) -> None`: update role mapping
       - `get_participant_role(speaker_id: str) -> ParticipantRole`: defaults to EXTERNAL if unknown

    3. Create `src/app/meetings/realtime/pipeline.py` with `RealtimePipeline` and `PipelineMetrics`:

       `PipelineMetrics` dataclass:
       - speech_end_time, stt_final_time, llm_first_token_time, tts_first_byte_time, audio_play_time (all float)
       - Computed properties: total_latency_ms, stt_latency_ms, llm_latency_ms, tts_latency_ms
       - Per RESEARCH.md: budget 300ms STT + 500ms LLM + 100ms TTS = 900ms target

       `RealtimePipeline`:
       - Constructor takes: stt_client (DeepgramSTT), tts_client (ElevenLabsTTS), avatar_client (HeyGenAvatar), silence_checker (SilenceChecker), llm_service, meeting_context (dict)
       - Constants: LATENCY_BUDGET_MS = 1000, LLM_TIMEOUT_MS = 500, MAX_CONTEXT_TURNS = 10

       - `async process_speech_turn(final_transcript: str, speaker_id: str) -> None`:
         1. Record speech_end_time
         2. Pre-LLM silence check: call silence_checker.should_respond(transcript, speaker_id, meeting_context) -- checks turn-taking + internal rep rules (confidence defaults to 1.0). If False, log and return.
         3. Build LLM prompt: system prompt with meeting briefing context + last MAX_CONTEXT_TURNS from transcript (sliding window per RESEARCH anti-pattern: do NOT send full history)
         4. Stream LLM response with timeout (LLM_TIMEOUT_MS). Use model='fast' (Haiku-class) for real-time (per RESEARCH.md: do NOT use reasoning model). If LLM exceeds timeout and no tokens received, abort and stay silent.
            - LLM response includes confidence_score (0.0-1.0) in first token metadata OR parsed from CONFIDENCE_PREFIX pattern (e.g., "[CONF:0.85]" prefix in response)
            - If LLM returns SILENCE_TOKEN ("[SILENCE]"), treat as confidence=0.0
         5. Post-LLM confidence gate (BEFORE TTS/avatar): call silence_checker.should_respond(transcript, speaker_id, meeting_context, confidence_score=parsed_confidence). If False (confidence below threshold), log "low_confidence_silence" and discard LLM output -- do NOT send to TTS or avatar.
         6. Buffer remaining LLM tokens to sentence boundaries for TTS quality
         7. At each sentence boundary: send text to avatar.speak() for lip-synced delivery
         8. Record metrics at each stage
         9. Log total_latency_ms via structlog

       - `async handle_stt_transcript(transcript: str, is_final: bool, speaker_id: str) -> None`:
         - Accumulates interim results
         - On is_final=True and utterance end: call process_speech_turn with accumulated text

       - `_is_sentence_boundary(text: str) -> bool`:
         - Returns True if text ends with ".", "!", "?", ":", ";"

       - `_build_llm_context(current_transcript: str) -> list[dict]`:
         - System message: meeting briefing + participant info + QBS methodology hints + "respond as an active sales meeting participant"
         - Last N turns from transcript history (sliding window)
         - Current turn as user message
         - Include instruction: "Prefix every response with [CONF:X.XX] where X.XX is your confidence (0.00-1.00) in this response's quality and relevance. If confidence is very low, respond with [SILENCE] instead of speaking."
         - SILENCE_TOKEN = "[SILENCE]" -- if LLM returns this, pipeline treats confidence as 0.0
         - Pipeline parses [CONF:X.XX] prefix, strips it from display text, passes value to post-LLM confidence gate

       - `get_metrics() -> PipelineMetrics`: return latest metrics snapshot

       - Latency degradation handling (per RESEARCH Pitfall 1):
         If total_latency_ms > LATENCY_BUDGET_MS, log warning and track degradation count.
         If degraded 3+ times consecutively, switch to shorter prompts and faster model.

    4. Create `tests/test_realtime_pipeline.py` with unit tests:
       - Test TurnDetector.is_end_of_turn returns True after 1s silence
       - Test TurnDetector.is_thinking_pause returns True between 1-2s silence
       - Test TurnDetector.get_active_speakers tracks active speakers
       - Test SilenceChecker returns False when internal rep speaking
       - Test SilenceChecker returns False during thinking pause
       - Test SilenceChecker returns True on valid end-of-turn with no internal speakers
       - Test SilenceChecker returns False when confidence_score < CONFIDENCE_THRESHOLD
       - Test SilenceChecker returns True when confidence_score >= CONFIDENCE_THRESHOLD with other checks passing
       - Test RealtimePipeline.process_speech_turn with mock LLM and TTS
       - Test RealtimePipeline respects silence checker (skips response when should_respond=False)
       - Test RealtimePipeline handles LLM timeout (stays silent)
       - Test RealtimePipeline SILENCE_TOKEN handling (LLM says don't speak, confidence=0.0)
       - Test RealtimePipeline post-LLM confidence gate blocks low-confidence responses BEFORE TTS
       - Test RealtimePipeline parses [CONF:X.XX] prefix from LLM response
       - Test PipelineMetrics computed properties
       - Test _is_sentence_boundary for various punctuation
       - Minimum 14 tests
  </action>
  <verify>
    Run: `cd "/Users/RAZER/Documents/projects/sales army" && python -m pytest tests/test_realtime_pipeline.py -v`
    All tests pass.
  </verify>
  <done>
    TurnDetector distinguishes pause types. SilenceChecker enforces all three strategic silence rules (including confidence threshold check) BEFORE any TTS/avatar delivery. RealtimePipeline orchestrates STT->LLM->TTS with sub-1s latency target, sentence-boundary buffering, LLM timeout handling, post-LLM confidence gating via [CONF:X.XX] prefix parsing, and SILENCE_TOKEN handling. All unit tests pass.
  </done>
</task>

<task type="auto">
  <name>Task 2: Output Media Webapp (vanilla JS frontend for Recall.ai bot camera)</name>
  <files>
    meeting-bot-webapp/package.json
    meeting-bot-webapp/src/index.html
    meeting-bot-webapp/src/app.js
    meeting-bot-webapp/src/pipeline/deepgram-stt.js
    meeting-bot-webapp/src/pipeline/llm-bridge.js
    meeting-bot-webapp/src/avatar/heygen-session.js
    meeting-bot-webapp/src/utils/audio-processor.js
  </files>
  <action>
    IMPORTANT: Per RESEARCH Pitfall 2, keep this webapp MINIMAL. Vanilla JavaScript, no React/Vue framework, no heavy bundler. This page renders inside Recall.ai's headless browser and must be fast and lightweight.

    1. Create `meeting-bot-webapp/package.json`:
       - name: "meeting-bot-webapp"
       - Only dependency: `livekit-client` (required for HeyGen avatar WebRTC)
       - Add simple build script using esbuild or similar lightweight bundler
       - NO React, NO Vue, NO heavy frameworks

    2. Create `meeting-bot-webapp/src/index.html`:
       - Minimal HTML: single `<video id="avatar-video" autoplay playsinline>` element for avatar
       - `<div id="status">` for debug/connection status
       - `<script type="module" src="app.js">`
       - No CSS framework -- inline minimal styles for full-viewport video

    3. Create `meeting-bot-webapp/src/app.js` (main orchestrator):
       - On page load:
         a. Parse URL query parameters: meeting_id, tenant_id, backend_ws_url, heygen_token, deepgram_key
         b. Initialize audio capture: `navigator.mediaDevices.getUserMedia({ audio: true })` (this captures meeting audio from Recall.ai's virtual microphone)
         c. Initialize DeepgramSTT WebSocket connection
         d. Initialize LLMBridge WebSocket to backend
         e. Initialize HeyGen avatar session (LiveKit room)

       - Audio flow:
         a. Meeting audio -> MediaStreamTrackProcessor -> raw audio frames
         b. Raw audio frames -> DeepgramSTT WebSocket (for transcription)
         c. Deepgram transcript results -> LLMBridge WebSocket (send to backend)
         d. LLM response from backend -> HeyGen speak() for lip-synced avatar delivery

       - Error handling: if any component fails, log error and continue with remaining components
       - Status display: show connection status for each component in #status div

    4. Create `meeting-bot-webapp/src/pipeline/deepgram-stt.js`:
       - `DeepgramSTTClient` class:
         - Connects to Deepgram WebSocket `wss://api.deepgram.com/v1/listen`
         - Configures: model=nova-3, endpointing=300, utterance_end_ms=1000, interim_results=true, diarize=true, encoding=linear16, sample_rate=16000
         - `send(audioData)`: sends raw audio frames to Deepgram
         - `onTranscript(callback)`: fires on transcript result (interim + final)
         - `onUtteranceEnd(callback)`: fires when speaker stops talking
         - `close()`: cleanup WebSocket

    5. Create `meeting-bot-webapp/src/pipeline/llm-bridge.js`:
       - `LLMBridge` class:
         - WebSocket connection to backend: `ws://{backend_ws_url}/ws/meeting/{meeting_id}`
         - `sendTranscript(transcript, isFinal, speakerId)`: sends transcript to backend for LLM processing
         - `onResponse(callback)`: fires when backend sends LLM response text
         - Backend response format: `{ type: "speak", text: "...", confidence: 0.8 }` or `{ type: "silence" }`
         - Reconnection logic: auto-reconnect on disconnect with exponential backoff (1s, 2s, 4s)

    6. Create `meeting-bot-webapp/src/avatar/heygen-session.js`:
       - `HeyGenSession` class:
         - Uses livekit-client Room to connect to HeyGen's LiveKit room
         - `async connect(url, accessToken)`: joins LiveKit room
         - On TrackSubscribed: attach video track to #avatar-video element
         - `async speak(text)`: POST to HeyGen streaming.task API with task_type="repeat"
         - `async sendReaction(reaction)`: for idle behavior (nod, interested look)
         - `disconnect()`: cleanup LiveKit room

    7. Create `meeting-bot-webapp/src/utils/audio-processor.js`:
       - `AudioCaptureProcessor` class:
         - Takes MediaStream from getUserMedia
         - Uses MediaStreamTrackProcessor to get raw audio frames
         - Converts AudioData frames to Int16Array (linear16 PCM) for Deepgram
         - `start(callback)`: begins reading audio frames, calls callback with PCM bytes
         - `stop()`: stops processing
  </action>
  <verify>
    Run: `cd "/Users/RAZER/Documents/projects/sales army/meeting-bot-webapp" && cat package.json | python -c "import sys,json; d=json.load(sys.stdin); assert 'livekit-client' in d.get('dependencies',{}); print('Webapp package OK')"`
    Run: `ls "/Users/RAZER/Documents/projects/sales army/meeting-bot-webapp/src/app.js" && echo "Webapp entry point exists"`
  </verify>
  <done>
    Output Media webapp is a minimal vanilla JS project that: captures meeting audio via getUserMedia, streams to Deepgram for STT, routes transcripts to backend via WebSocket, receives LLM responses, and drives HeyGen LiveKit avatar for lip-synced speaking. No heavy frameworks -- optimized for Recall.ai's headless browser rendering.
  </done>
</task>

</tasks>

<verification>
1. `python -m pytest tests/test_realtime_pipeline.py -v` -- all tests pass
2. All Python pipeline modules importable: TurnDetector, SilenceChecker, RealtimePipeline, PipelineMetrics
3. meeting-bot-webapp/ directory exists with package.json, index.html, app.js, and all pipeline/avatar/utils JS files
4. SilenceChecker enforces strict turn-taking (never interrupts)
5. Pipeline uses model='fast' for real-time (not reasoning model)
6. Webapp is vanilla JS with only livekit-client dependency
</verification>

<success_criteria>
Real-time meeting pipeline is complete -- backend orchestrates STT->LLM->TTS with strategic silence enforcement and sub-1s latency target. Output Media webapp runs inside Recall.ai, captures meeting audio, renders HeyGen avatar, and coordinates the full real-time response flow. Agent never interrupts (strict turn-taking) and stays silent when confidence is low.
</success_criteria>

<output>
After completion, create `.planning/phases/06-meeting-capabilities/06-04-SUMMARY.md`
</output>
