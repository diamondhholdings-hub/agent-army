---
phase: 01-infrastructure-foundation
plan: 03
type: execute
wave: 2
depends_on: ["01-01"]
files_modified:
  - Dockerfile
  - .dockerignore
  - .github/workflows/deploy.yml
  - .github/workflows/test.yml
  - .env.example
  - src/app/core/monitoring.py
  - src/app/api/v1/health.py
  - src/app/config.py
  - src/app/main.py
  - scripts/backup.py
  - scripts/restore.py
  - scripts/provision_tenant.py
autonomous: false

user_setup:
  - service: gcp
    why: "Google Cloud Platform for Cloud Run deployment, Cloud SQL, Secret Manager"
    env_vars:
      - name: GCP_PROJECT_ID
        source: "GCP Console -> Project Settings (https://console.cloud.google.com/iam-admin/settings)"
      - name: GCP_PROJECT_NUMBER
        source: "GCP Console -> Project Settings -> Project number"
      - name: GCP_REGION
        source: "Choose a region (e.g., us-central1)"
    dashboard_config:
      - task: "Enable Cloud Run API"
        location: "GCP Console -> APIs & Services -> Enable APIs (https://console.cloud.google.com/apis/library/run.googleapis.com)"
      - task: "Enable Secret Manager API"
        location: "GCP Console -> APIs & Services -> Enable APIs (https://console.cloud.google.com/apis/library/secretmanager.googleapis.com)"
      - task: "Enable Cloud Build API"
        location: "GCP Console -> APIs & Services -> Enable APIs (https://console.cloud.google.com/apis/library/cloudbuild.googleapis.com)"
      - task: "Create Workload Identity Pool for GitHub Actions"
        location: "GCP Console -> IAM -> Workload Identity Federation (https://console.cloud.google.com/iam-admin/workload-identity-pools)"
      - task: "Create service account github-actions@{project}.iam.gserviceaccount.com with Cloud Run Admin and Secret Manager Accessor roles"
        location: "GCP Console -> IAM -> Service Accounts"
  - service: github
    why: "GitHub repository for CI/CD pipeline"
    env_vars: []
    dashboard_config:
      - task: "Set repository secrets: GCP_PROJECT_ID, GCP_PROJECT_NUMBER, GCP_REGION"
        location: "GitHub -> Repository Settings -> Secrets and variables -> Actions"
      - task: "Set repository secrets for per-tenant API keys (ANTHROPIC_API_KEY, OPENAI_API_KEY)"
        location: "GitHub -> Repository Settings -> Secrets and variables -> Actions"

must_haves:
  truths:
    - "Docker image builds successfully with all dependencies and runs the FastAPI server"
    - "GitHub Actions CI runs tests on every push and PR"
    - "GitHub Actions CD deploys to Cloud Run staging on push to main"
    - "Per-tenant secrets are stored in Google Secret Manager, not environment variables"
    - "Health check endpoints distinguish liveness (is the process alive) from readiness (can it serve requests)"
    - "Prometheus metrics are exposed for request count, latency, and error rate per tenant"
    - "Sentry captures exceptions with tenant context for debugging"
    - "Backup script can dump a single tenant's schema without affecting others"
    - ".env.example documents all required environment variables"
  artifacts:
    - path: "Dockerfile"
      provides: "Multi-stage Docker build for production FastAPI server"
      contains: "uvicorn"
    - path: ".github/workflows/deploy.yml"
      provides: "CI/CD pipeline: test -> build -> deploy to Cloud Run staging"
      contains: "deploy-cloudrun"
    - path: ".github/workflows/test.yml"
      provides: "CI pipeline: lint + test on every push/PR"
      contains: "pytest"
    - path: "src/app/core/monitoring.py"
      provides: "Prometheus metrics and Sentry integration"
      contains: "prometheus_client"
    - path: "scripts/backup.py"
      provides: "Per-tenant schema backup script"
      contains: "pg_dump"
    - path: "scripts/provision_tenant.py"
      provides: "CLI script to provision new tenants"
      contains: "provision_tenant"
    - path: ".env.example"
      provides: "Documentation of all environment variables"
      contains: "ANTHROPIC_API_KEY"
  key_links:
    - from: "Dockerfile"
      to: "src/app/main.py"
      via: "CMD runs uvicorn against the app factory"
      pattern: "uvicorn.*src.app.main"
    - from: ".github/workflows/deploy.yml"
      to: "Dockerfile"
      via: "Builds Docker image and deploys to Cloud Run"
      pattern: "gcloud builds submit"
    - from: "src/app/core/monitoring.py"
      to: "src/app/main.py"
      via: "Monitoring middleware added to FastAPI app"
      pattern: "PrometheusMiddleware|instrument"
    - from: "scripts/backup.py"
      to: "src/app/models/shared.py"
      via: "Reads tenant list from shared.tenants to enumerate schemas"
      pattern: "shared.tenants"
---

<objective>
Build the deployment pipeline, monitoring infrastructure, and environment management: Dockerfile for containerization, GitHub Actions for CI/CD to Cloud Run, Google Secret Manager for per-tenant secrets, Prometheus metrics with Sentry error tracking, and backup/restore scripts for per-tenant data protection.

Purpose: The platform cannot be validated without deployment infrastructure. This plan ensures that code changes flow automatically from commit to staging, secrets are managed securely per tenant, and the system is observable in production. Backup scripts protect against data loss from the start.

Output: Production-ready Docker container, CI/CD pipeline deploying to Cloud Run staging, monitoring dashboard with Prometheus metrics, Sentry error tracking with tenant context, per-tenant backup/restore scripts, and a comprehensive .env.example.
</objective>

<execution_context>
@/Users/RAZER/.claude/get-shit-done/workflows/execute-plan.md
@/Users/RAZER/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/01-infrastructure-foundation/01-RESEARCH.md
@.planning/phases/01-infrastructure-foundation/01-01-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Docker, CI/CD pipeline, secrets management, and environment configuration</name>
  <files>
    Dockerfile
    .dockerignore
    .github/workflows/deploy.yml
    .github/workflows/test.yml
    .env.example
    src/app/config.py
    scripts/provision_tenant.py
  </files>
  <action>
    **1. Create Dockerfile** -- Multi-stage production build:

    ```dockerfile
    # Stage 1: Build
    FROM python:3.12-slim AS builder
    WORKDIR /app
    RUN pip install uv
    COPY pyproject.toml uv.lock* ./
    RUN uv sync --frozen --no-dev
    COPY src/ src/
    COPY alembic/ alembic/
    COPY alembic.ini .

    # Stage 2: Production
    FROM python:3.12-slim
    WORKDIR /app
    RUN useradd -m -r appuser && apt-get update && apt-get install -y --no-install-recommends postgresql-client && rm -rf /var/lib/apt/lists/*
    COPY --from=builder /app /app
    USER appuser
    ENV PORT=8080
    EXPOSE 8080
    CMD ["uv", "run", "uvicorn", "src.app.main:app", "--host", "0.0.0.0", "--port", "8080"]
    ```

    Notes:
    - Multi-stage build keeps image small (~200MB vs ~800MB).
    - Non-root user (`appuser`) for security.
    - postgresql-client included for backup scripts and migrations.
    - Cloud Run sets PORT env var; default to 8080.
    - If uv is not in PATH in production image, use the full venv path instead: `.venv/bin/uvicorn`.

    **2. Create .dockerignore:**
    ```
    .git
    .github
    .planning
    .env
    __pycache__
    *.pyc
    .pytest_cache
    .mypy_cache
    .ruff_cache
    tests/
    docs/
    *.md
    ```

    **3. Create .github/workflows/test.yml** -- CI pipeline:
    ```yaml
    name: Test
    on:
      push:
        branches: [main, develop]
      pull_request:
        branches: [main]

    jobs:
      lint-and-test:
        runs-on: ubuntu-latest
        services:
          postgres:
            image: postgres:16
            env:
              POSTGRES_USER: agent_army
              POSTGRES_PASSWORD: agent_army_dev
              POSTGRES_DB: agent_army
            ports: ["5432:5432"]
            options: >-
              --health-cmd pg_isready
              --health-interval 10s
              --health-timeout 5s
              --health-retries 5
          redis:
            image: redis:7
            ports: ["6379:6379"]
            options: >-
              --health-cmd "redis-cli ping"
              --health-interval 10s
              --health-timeout 5s
              --health-retries 5

        steps:
          - uses: actions/checkout@v4
          - uses: astral-sh/setup-uv@v4
          - run: uv sync
          - run: uv run ruff check src/
          - run: uv run ruff format --check src/
          - run: uv run pytest tests/ -v --tb=short
            env:
              DATABASE_URL: postgresql+asyncpg://agent_army:agent_army_dev@localhost:5432/agent_army
              REDIS_URL: redis://localhost:6379/0
              ENVIRONMENT: testing
              JWT_SECRET_KEY: test-secret-key-for-ci
    ```

    **4. Create .github/workflows/deploy.yml** -- CD pipeline:
    ```yaml
    name: Deploy to Cloud Run
    on:
      push:
        branches: [main]

    env:
      PROJECT_ID: ${{ secrets.GCP_PROJECT_ID }}
      REGION: ${{ secrets.GCP_REGION }}
      SERVICE: agent-army-api

    jobs:
      deploy-staging:
        runs-on: ubuntu-latest
        permissions:
          contents: read
          id-token: write  # Required for Workload Identity Federation

        steps:
          - uses: actions/checkout@v4

          - id: auth
            uses: google-github-actions/auth@v2
            with:
              workload_identity_provider: projects/${{ secrets.GCP_PROJECT_NUMBER }}/locations/global/workloadIdentityPools/github/providers/github
              service_account: github-actions@${{ secrets.GCP_PROJECT_ID }}.iam.gserviceaccount.com

          - uses: google-github-actions/setup-gcloud@v2

          - name: Build and push Docker image
            run: gcloud builds submit --tag gcr.io/$PROJECT_ID/$SERVICE --timeout=600

          - name: Deploy to Cloud Run (staging)
            uses: google-github-actions/deploy-cloudrun@v2
            with:
              service: ${{ env.SERVICE }}-staging
              image: gcr.io/${{ env.PROJECT_ID }}/${{ env.SERVICE }}
              region: ${{ env.REGION }}
              env_vars: |
                ENVIRONMENT=staging
              secrets: |
                DATABASE_URL=database-url-staging:latest
                REDIS_URL=redis-url-staging:latest
                ANTHROPIC_API_KEY=anthropic-api-key:latest
                OPENAI_API_KEY=openai-api-key:latest
                JWT_SECRET_KEY=jwt-secret-key:latest
              flags: |
                --min-instances=0
                --max-instances=10
                --memory=1Gi
                --cpu=1
                --timeout=300
                --concurrency=80
    ```

    Notes on secrets management (INF-08):
    - Per-tenant secrets stored in Google Secret Manager, NOT in environment variables.
    - Secret naming convention: `{tenant-slug}-{secret-name}` (e.g., `skyvera-google-sa-key`, `jigtree-google-sa-key`).
    - Platform-level secrets (API keys, JWT secret) are deployed via Cloud Run secrets mount.
    - Per-tenant secrets (Google SA keys, custom configs) are loaded at runtime from Secret Manager by the tenant provisioning service.
    - The config.py should add: `GCP_PROJECT_ID` setting for Secret Manager client.

    **5. Create .env.example** documenting all environment variables:
    ```
    # Database
    DATABASE_URL=postgresql+asyncpg://agent_army:agent_army_dev@localhost:5432/agent_army

    # Redis
    REDIS_URL=redis://localhost:6379/0

    # Environment
    ENVIRONMENT=development  # development | staging | production

    # Authentication
    JWT_SECRET_KEY=generate-with-openssl-rand-hex-32
    JWT_ACCESS_TOKEN_EXPIRE_MINUTES=30
    JWT_REFRESH_TOKEN_EXPIRE_DAYS=7

    # LLM Providers
    ANTHROPIC_API_KEY=sk-ant-...
    OPENAI_API_KEY=sk-...

    # Google Workspace (deferred to Phase 4)
    # GOOGLE_SERVICE_ACCOUNT_FILE=path/to/service-account.json

    # GCP (for Secret Manager and deployment)
    GCP_PROJECT_ID=your-gcp-project-id

    # Monitoring
    SENTRY_DSN=https://...@sentry.io/...
    LOG_LEVEL=INFO

    # Schema Config
    SHARED_SCHEMA=shared
    ```

    **6. Create scripts/provision_tenant.py** -- CLI tenant provisioning:
    - Standalone async script that provisions a tenant by calling the provisioning service.
    - Usage: `uv run python scripts/provision_tenant.py --slug skyvera --name "Skyvera"`
    - Connects to database directly (reads DATABASE_URL from env or .env file).
    - Provisions schema, runs migrations, registers tenant.
    - Optionally creates initial admin user: `--admin-email admin@skyvera.com --admin-password ...`.
    - Prints tenant details on success.

    **7. Update src/app/config.py** to add:
    - `GCP_PROJECT_ID`: str, default "".
    - `SENTRY_DSN`: str, default "".
  </action>
  <verify>
    1. `docker build -t agent-army .` builds successfully.
    2. Use docker-compose for integration testing: add a temporary `app` service to docker-compose.yml that builds from Dockerfile and depends on postgres/redis services, then `docker compose up app` and verify `/health` returns 200 at http://localhost:8080/health. Remove the temporary service after verification. Note: `--network host` does not work on macOS; docker-compose networking is the portable approach.
    3. `.env.example` exists and documents all environment variables.
    4. `.github/workflows/test.yml` and `.github/workflows/deploy.yml` are valid YAML (check with `python -c "import yaml; yaml.safe_load(open('.github/workflows/test.yml'))"` or similar).
    5. `scripts/provision_tenant.py --slug test-verify --name "Test Verify"` provisions a tenant successfully.
  </verify>
  <done>
    Docker image builds and runs the FastAPI server. GitHub Actions CI pipeline runs linting and tests with service containers for PostgreSQL and Redis. CD pipeline deploys to Cloud Run staging via Workload Identity Federation. Secrets management uses Google Secret Manager with per-tenant secret naming convention. .env.example documents all configuration. CLI provisioning script works.
  </done>
</task>

<task type="auto">
  <name>Task 2: Monitoring, alerting, and backup/restore infrastructure</name>
  <files>
    src/app/core/monitoring.py
    src/app/api/v1/health.py
    src/app/main.py
    scripts/backup.py
    scripts/restore.py
  </files>
  <action>
    **1. Create src/app/core/monitoring.py** -- Prometheus metrics and Sentry:

    **Prometheus metrics:**
    - `http_requests_total` (Counter): labels = method, endpoint, status_code, tenant_id. Incremented on every request.
    - `http_request_duration_seconds` (Histogram): labels = method, endpoint, tenant_id. Records request duration.
    - `llm_requests_total` (Counter): labels = model, tenant_id, status (success/error). Incremented on every LLM call.
    - `llm_request_duration_seconds` (Histogram): labels = model, tenant_id. Records LLM call duration.
    - `llm_tokens_used_total` (Counter): labels = model, tenant_id, token_type (prompt/completion). Tracks token usage.
    - `active_tenants` (Gauge): Number of active tenants.
    - `db_pool_size` (Gauge): Current database connection pool size.

    Create `MetricsMiddleware(BaseHTTPMiddleware)`:
    - On each request: increment request counter, record duration in histogram.
    - Extract tenant_id from context (if available, else "unknown").
    - Skip metrics path itself (`/metrics`).

    Create `/metrics` endpoint that returns Prometheus exposition format using `prometheus_client.generate_latest()`.

    **Sentry integration:**
    - `init_sentry(dsn: str, environment: str)` function:
      - `sentry_sdk.init(dsn=dsn, environment=environment, traces_sample_rate=0.1 for production / 1.0 for staging, integrations=[FastApiIntegration()])`.
      - Set `before_send` callback that adds tenant_id and user_id to event tags from the contextvars (if available).
    - This should be called in `create_app()` during startup if `SENTRY_DSN` is configured.

    **LLM metrics helper** -- decorators or context managers for the LLM service to report metrics:
    - `track_llm_call(model, tenant_id)` context manager that records duration, token usage, and success/error status to Prometheus counters.

    **2. Update src/app/api/v1/health.py** -- Enhanced health checks:
    - `GET /health` (liveness): Simple 200 response. Cloud Run uses this to check if the process is alive. No external dependencies checked.
    - `GET /health/ready` (readiness): Checks:
      - Database connection: `SELECT 1` succeeds.
      - Redis connection: `PING` returns `PONG`.
      - Returns 200 if both pass with `{"status": "ready", "checks": {"database": "ok", "redis": "ok"}}`.
      - Returns 503 if either fails with details about which check failed.
    - `GET /health/startup` (startup): Same as readiness but used by Cloud Run during initial startup. Gives more time for the first connection.

    **3. Update src/app/main.py** to add:
    - MetricsMiddleware.
    - Sentry initialization in lifespan.
    - `/metrics` endpoint.
    - Health check routers.

    **4. Create scripts/backup.py** -- Per-tenant backup script (INF-09):

    Usage: `uv run python scripts/backup.py --tenant skyvera --output ./backups/`

    Functionality:
    - Connects to PostgreSQL using DATABASE_URL from env.
    - If `--tenant` is specified: dumps only that tenant's schema using `pg_dump --schema=tenant_skyvera`.
    - If `--all` is specified: dumps shared schema + all tenant schemas (iterates shared.tenants table).
    - Output format: compressed SQL (`pg_dump -Fc` for custom format, supports parallel restore).
    - Output filename: `{tenant_slug}_{timestamp}.dump` (e.g., `skyvera_20260210_120000.dump`).
    - Includes a manifest file listing all backed-up schemas and their sizes.
    - Logs backup status via structlog.

    **5. Create scripts/restore.py** -- Per-tenant restore script:

    Usage: `uv run python scripts/restore.py --file ./backups/skyvera_20260210_120000.dump --tenant skyvera`

    Functionality:
    - Restores a specific tenant's schema from a backup file.
    - Safety check: prompts for confirmation before restoring (can be bypassed with `--yes`).
    - Drops existing schema if it exists (with cascade), then restores from dump.
    - Re-registers tenant in shared.tenants if needed.
    - Verifies RLS policies are intact after restore.
    - Note: Use `pg_restore` for custom format dumps.

    **6. Environment management (INF-07):**
    - The environment is controlled by the `ENVIRONMENT` setting (development, staging, production).
    - Per-environment behavior differences (configured in config.py):
      - `development`: DEBUG logging, CORS allow all, no Sentry, Prometheus enabled.
      - `staging`: INFO logging, CORS configurable, Sentry enabled (100% sample rate), Prometheus enabled, min-instances=0.
      - `production`: INFO logging, CORS restricted, Sentry enabled (10% sample rate), Prometheus enabled, min-instances=1.
    - Per-tenant environments: Each tenant's data exists in the same database but different schemas. The environment (dev/staging/prod) is at the deployment level, not the tenant level. Tenants don't get separate staging environments in v1 -- the platform has one staging environment with all tenants.
    - Document this architecture decision in the summary.
  </action>
  <verify>
    1. `curl http://localhost:8000/metrics` returns Prometheus exposition format with `http_requests_total` and other metrics.
    2. `curl http://localhost:8000/health` returns `{"status": "ok"}`.
    3. `curl http://localhost:8000/health/ready` returns 200 with database and Redis check results.
    4. `uv run python scripts/backup.py --tenant test-alpha --output /tmp/backups/` creates a backup file.
    5. `uv run python scripts/backup.py --all --output /tmp/backups/` backs up all tenants.
    6. Docker image with monitoring starts and exposes metrics.
    7. If Sentry DSN is configured, exceptions are captured with tenant_id tag.
  </verify>
  <done>
    Prometheus metrics track request count, latency, LLM token usage, and error rates per tenant. Sentry captures exceptions with tenant context. Health check endpoints distinguish liveness from readiness. Per-tenant backup script dumps individual schemas. Restore script recovers tenant data. Environment management controls behavior per deployment tier (dev/staging/production).
  </done>
</task>

<task type="checkpoint:human-verify" gate="blocking">
  <what-built>
    Complete deployment and monitoring infrastructure:
    1. Docker image that builds and runs the FastAPI server
    2. GitHub Actions CI (test on push/PR) and CD (deploy to Cloud Run staging)
    3. Prometheus metrics at /metrics with tenant-scoped counters
    4. Sentry error tracking with tenant context
    5. Liveness and readiness health check endpoints
    6. Per-tenant backup and restore scripts
    7. .env.example documenting all configuration
  </what-built>
  <how-to-verify>
    1. Build Docker image: `docker build -t agent-army .` (verify it builds successfully)
    2. Run via docker-compose: `docker compose up` with all services, verify /health returns 200 at http://localhost:8080/health
    3. Verify /metrics returns Prometheus format at http://localhost:8080/metrics
    4. Check GitHub Actions workflow files exist and are valid YAML
    5. Run backup script: `uv run python scripts/backup.py --tenant skyvera --output /tmp/backups/`
    6. Review .env.example for completeness
    7. Run full test suite: `uv run pytest tests/ -v`
  </how-to-verify>
  <resume-signal>Type "approved" or describe any issues with the deployment pipeline, monitoring, or environment setup</resume-signal>
</task>

</tasks>

<verification>
1. **INF-06 (Deployment pipeline):** Dockerfile builds production image. GitHub Actions CI runs tests. CD deploys to Cloud Run staging with Workload Identity Federation (no long-lived keys).
2. **INF-07 (Environment management):** ENVIRONMENT setting controls behavior (dev/staging/production). Cloud Run manages per-environment deployments. .env.example documents all variables.
3. **INF-08 (Secrets management):** Google Secret Manager for all sensitive values. Per-tenant secret naming convention. Cloud Run mounts secrets at deploy time. No secrets in environment variables or code.
4. **INF-09 (Backup and disaster recovery):** Per-tenant backup via pg_dump --schema. Restore via pg_restore with safety checks. Supports both individual tenant and full-platform backups.
5. **INF-10 (Monitoring and alerting):** Prometheus metrics for requests, latency, LLM usage, and errors. Sentry for exception tracking with tenant context. Health checks for Cloud Run liveness and readiness probes.
</verification>

<success_criteria>
- Docker image builds in under 5 minutes and runs successfully
- GitHub Actions CI workflow passes on a test push
- /metrics endpoint returns valid Prometheus exposition format
- /health and /health/ready endpoints return appropriate status codes
- Backup script creates valid dump files per tenant
- .env.example covers all required configuration
- `uv run pytest tests/ -v` passes all tests
</success_criteria>

<output>
After completion, create `.planning/phases/01-infrastructure-foundation/01-03-SUMMARY.md`
</output>
