# Phase 4.2: QBS Methodology Integration - Research

**Researched:** 2026-02-12
**Domain:** LLM-driven Question Based Selling methodology integration into existing Sales Agent conversational capabilities with adaptive signal-based question selection, methodology blending (QBS + BANT/MEDDIC + Chris Voss), pain funnel depth tracking, and account expansion multi-threading
**Confidence:** HIGH

## Summary

Phase 4.2 integrates Question Based Selling (QBS) methodology into the existing Sales Agent (Phase 4) to enhance HOW the agent conducts conversations. The core challenge is not adding new capabilities but enriching the agent's conversational intelligence: selecting the right question type based on conversation signals, blending QBS with existing BANT/MEDDIC qualification and Chris Voss empathy techniques, probing pain to emotional depth, and expanding relationships within accounts.

The existing codebase provides strong integration points. The `prompts.py` module already has persona configs, Voss methodology, deal stage guidance, and prompt builders that compose system prompts. The `qualification.py` module extracts BANT/MEDDIC signals via instructor + LiteLLM. The `actions.py` NextActionEngine recommends next steps. Phase 4.2 weaves QBS question intelligence into these existing components rather than creating parallel systems.

The recommended approach is a **QBS Question Engine** that sits alongside the existing prompt system, providing: (1) signal detection to analyze conversation context and identify information gaps, customer engagement signals, and natural flow cues; (2) question type selection to choose the optimal QBS question type (pain funnel, impact, solution, confirmation); (3) methodology blending to compose each question with QBS structure + MEDDIC/BANT targeting + Voss delivery; and (4) account expansion detection to identify multi-threading opportunities from conversation mentions of other contacts. All of this feeds into the existing prompt builders, enriching system prompts and user messages with QBS-aware guidance. No new external libraries required -- this is a prompt engineering + structured extraction phase using the existing stack (instructor, litellm, pydantic).

**Primary recommendation:** Build a QBSQuestionEngine class that analyzes conversation context via LLM to produce structured question recommendations, then integrate it into the existing prompt builders (build_system_prompt, build_email_prompt, build_chat_prompt) so every generated message naturally incorporates QBS methodology. Add a PainDepthTracker to ConversationState for tracking pain funnel progression, and an AccountExpansionDetector for identifying multi-threading opportunities from conversation signals.

## Standard Stack

### Core
| Library | Version | Purpose | Why Standard |
|---------|---------|---------|--------------|
| instructor | >=1.7.0 (already installed) | Structured LLM extraction for QBS analysis, question selection, pain depth assessment, contact detection | Already used in qualification.py; same instructor.from_litellm pattern for QBS signal extraction |
| litellm | >=1.60.0 (already installed) | LLM abstraction for QBS question generation and signal analysis | Already the LLM backbone; no new provider needed |
| pydantic | >=2.0.0 (already installed) | QBS question models, pain depth state, expansion triggers, signal detection schemas | Already used for all structured types in sales agent |
| structlog | >=24.0.0 (already installed) | Structured logging for QBS question selection decisions and signal detection | Already used project-wide |

### Supporting
| Library | Version | Purpose | When to Use |
|---------|---------|---------|-------------|
| (none new) | -- | -- | All supporting libraries already installed |

### Alternatives Considered
| Instead of | Could Use | Tradeoff |
|------------|-----------|----------|
| LLM-based signal detection | Rule-based heuristics only | LLM catches nuanced signals (tone shifts, implicit pain) that rules miss; rules handle obvious cases (zero interactions, missing BANT fields) |
| Single QBS prompt injection | Separate QBS generation step | Single injection is simpler but less controllable; separate step enables tracking which QBS patterns work best via Phase 4.1 learning |
| instructor structured output | JSON parsing from LLM response | instructor handles validation, retries, and schema enforcement; raw JSON parsing is fragile |

**Installation:**
```bash
# No new packages -- all dependencies already installed
```

## Architecture Patterns

### Recommended Project Structure
```
src/app/agents/sales/
├── agent.py              # SalesAgent (existing -- add QBS engine injection)
├── prompts.py            # Prompt builders (existing -- add QBS prompt sections)
├── schemas.py            # Data models (existing -- add QBS schemas)
├── qualification.py      # QualificationExtractor (existing -- no changes)
├── actions.py            # NextActionEngine (existing -- add QBS context to recommendations)
├── escalation.py         # EscalationManager (existing -- no changes)
├── state_repository.py   # ConversationStateRepository (existing -- no schema changes, QBS data in metadata)
├── capabilities.py       # Capabilities (existing -- update descriptions)
├── qbs/                  # NEW: QBS methodology module
│   ├── __init__.py       # Module exports
│   ├── engine.py         # QBSQuestionEngine: signal detection + question selection
│   ├── schemas.py        # QBS-specific Pydantic models (question types, signals, pain state)
│   ├── prompts.py        # QBS-specific prompt templates and blending instructions
│   ├── pain_tracker.py   # PainDepthTracker: pain funnel state management
│   └── expansion.py      # AccountExpansionDetector: multi-threading opportunity detection
└── __init__.py
```

### Pattern 1: QBS Question Engine (Signal-Driven Selection)
**What:** A class that analyzes conversation context through three sensing modes (information gap, engagement signals, natural flow) and outputs a structured QBS question recommendation blended with MEDDIC/BANT targeting and Voss delivery.
**When to use:** Every time the agent generates a message (email or chat), the QBS engine provides question guidance that gets injected into the prompt.
**Why this pattern:** The user's locked decision specifies "adaptive based on signals, not fixed sequence." This requires LLM analysis of conversation context to determine the optimal question type, rather than a rule-based state machine that follows pain->impact->solution->confirmation rigidly.

```python
# Source: Adapted from existing instructor.from_litellm pattern in qualification.py
from pydantic import BaseModel, Field
from enum import Enum

class QBSQuestionType(str, Enum):
    """QBS question categories per Thomas Freese's framework."""
    PAIN_FUNNEL = "pain_funnel"       # Surface -> business impact -> emotional depth
    IMPACT = "impact"                  # Business consequences of identified pain
    SOLUTION = "solution"              # Connect needs to product capabilities
    CONFIRMATION = "confirmation"      # Validate understanding, build commitment

class PainDepthLevel(str, Enum):
    """Tracks how deep the pain funnel has been explored."""
    SURFACE = "surface"                # Problem identified at surface level
    BUSINESS_IMPACT = "business_impact" # Business cost/consequences articulated
    EMOTIONAL = "emotional"            # Root cause / emotional driver reached
    NOT_EXPLORED = "not_explored"      # Pain not yet explored

class EngagementSignal(str, Enum):
    """Customer engagement signal types detected in conversation."""
    HIGH_ENERGY = "high_energy"        # Customer elaborating, volunteering info
    FACTUAL = "factual"                # Customer responding but not engaged
    RESISTANT = "resistant"            # Customer deflecting, short answers
    TOPIC_SHIFT = "topic_shift"        # Customer redirecting conversation
    EMOTIONAL_LANGUAGE = "emotional"   # Customer using emotional terms

class QBSQuestionRecommendation(BaseModel):
    """Structured output from the QBS Question Engine."""
    question_type: QBSQuestionType
    meddic_bant_target: str = Field(description="Which MEDDIC/BANT dimension this targets")
    voss_delivery: str = Field(description="Chris Voss technique for delivery: mirror, label, calibrated_question, accusation_audit")
    suggested_question: str = Field(description="The blended question ready for use")
    rationale: str = Field(description="Why this question type/target/delivery was chosen")
    information_gaps: list[str] = Field(default_factory=list, description="What qualification data is missing")
    engagement_signal: EngagementSignal = Field(description="Detected customer engagement level")
    pain_depth: PainDepthLevel = Field(description="Current pain exploration depth")
    should_probe_deeper: bool = Field(description="Whether to continue probing current topic")
    natural_stopping_signals: list[str] = Field(default_factory=list, description="Signals to stop probing")
```

### Pattern 2: Prompt Injection (QBS Layer into Existing Prompts)
**What:** Rather than replacing the existing prompt system, QBS guidance is injected as an additional section into the system prompt via `build_system_prompt()`. The QBS engine output becomes context for the LLM's message generation.
**When to use:** Every email/chat generation call. The QBS section is dynamic, changing based on conversation state.
**Why this pattern:** The user decided on "fully blended approach" where all methodologies work together. The existing Voss methodology prompt, persona configs, and deal stage guidance remain untouched. QBS adds a new prompt section that references and builds on them.

```python
# Source: Extension of existing build_system_prompt pattern in prompts.py
def build_qbs_prompt_section(
    qbs_recommendation: QBSQuestionRecommendation,
    pain_state: PainFunnelState,
    expansion_opportunities: list[ExpansionOpportunity],
) -> str:
    """Build the QBS methodology section for injection into system prompt.

    This section is inserted between the Voss methodology and persona config
    sections in the existing build_system_prompt() function.
    """
    section = (
        "**Question Based Selling (QBS) Guidance for This Message:**\n\n"
        f"**Primary Question Type:** {qbs_recommendation.question_type.value}\n"
        f"**Target:** Extract {qbs_recommendation.meddic_bant_target} data\n"
        f"**Delivery:** Use {qbs_recommendation.voss_delivery} technique\n"
        f"**Suggested Question:** \"{qbs_recommendation.suggested_question}\"\n\n"
    )

    if qbs_recommendation.information_gaps:
        section += (
            "**Information Gaps to Fill:**\n"
            + "\n".join(f"- {gap}" for gap in qbs_recommendation.information_gaps)
            + "\n\n"
        )

    # Pain funnel guidance
    section += _build_pain_guidance(pain_state, qbs_recommendation)

    # Account expansion guidance
    if expansion_opportunities:
        section += _build_expansion_guidance(expansion_opportunities)

    return section
```

### Pattern 3: Pain Depth Tracking via Metadata
**What:** Pain funnel state (depth level, evidence, recognition signals) stored in the existing ConversationState.metadata dict, avoiding schema migrations.
**When to use:** Track pain exploration depth across conversation turns, enabling the QBS engine to know whether to probe deeper or move to impact/solution questions.
**Why this pattern:** The user decided on specific depth targets ("probe until root cause or emotional driver is reached AND customer articulates the weight"). This requires persistent tracking of where we are in the pain funnel across multiple interactions. Using metadata dict avoids Alembic migrations while keeping the data with the conversation state.

```python
# Source: Extension of existing ConversationState.metadata pattern
class PainFunnelState(BaseModel):
    """Tracks pain funnel progression across conversation turns."""
    depth_level: PainDepthLevel = PainDepthLevel.NOT_EXPLORED
    pain_topics: list[PainTopic] = Field(default_factory=list)
    emotional_recognition_detected: bool = False
    self_elaboration_count: int = 0  # Times customer volunteered detail without asking
    resistance_detected: bool = False
    revisit_later: list[str] = Field(default_factory=list)  # Pain gaps to revisit
    last_probed_topic: str | None = None
    probe_count_current_topic: int = 0

class PainTopic(BaseModel):
    """A specific pain point being tracked through the funnel."""
    topic: str
    depth: PainDepthLevel
    evidence: str  # Quote from conversation
    business_impact: str | None = None
    emotional_indicator: str | None = None
    first_mentioned_at: int  # Interaction number
    last_probed_at: int  # Interaction number
```

### Pattern 4: Account Expansion Detection
**What:** A detector that scans conversation text for mentions of other people/roles, then generates expansion strategies using QBS and Voss techniques.
**When to use:** After processing every customer reply, scan for expansion triggers per the user's locked decision ("listen for when customer mentions other people").
**Why this pattern:** The user specified organic, conversational expansion -- not transactional. The detector identifies natural moments, and the expansion recommender produces QBS-style approaches for each detected contact mention.

```python
# Source: Combines existing PoliticalMapper pattern with QBS methodology
class ExpansionTrigger(BaseModel):
    """A detected mention of another person in the conversation."""
    mentioned_name_or_role: str  # "my boss", "the finance team", "Sarah from procurement"
    context_quote: str           # The sentence where they were mentioned
    relationship_to_contact: str  # Inferred relationship
    expansion_approach: str       # Recommended QBS approach
    urgency: str                 # "immediate", "next_conversation", "after_trust_builds"

class ExpansionRecommendation(BaseModel):
    """Structured expansion recommendation."""
    triggers: list[ExpansionTrigger]
    primary_recommendation: str  # Best next step for multi-threading
    resistance_assessment: str   # Likelihood of resistance and handling approach
    political_context: str       # How this fits political mapping (Phase 5 integration)
```

### Anti-Patterns to Avoid
- **QBS as separate conversation flow:** Do NOT create a parallel conversation engine. QBS integrates INTO the existing prompt system, not alongside it. The SalesAgent.execute() flow remains unchanged.
- **Fixed question sequence:** Do NOT implement pain->impact->solution->confirmation as a state machine. The user explicitly decided on "adaptive based on signals." Any question type can appear at any stage.
- **Separate LLM call per question type:** Do NOT make individual LLM calls for each QBS dimension. Use a single call with instructor to get the full QBSQuestionRecommendation, similar to how qualification.py does a single call for all BANT+MEDDIC signals.
- **Database schema changes for QBS state:** Do NOT add new database columns or tables. Use ConversationState.metadata (JSON column) for pain depth, expansion triggers, and QBS-specific state. This avoids migrations and keeps QBS loosely coupled.
- **Overwriting pain evidence:** Follow the existing qualification.py evidence-append pattern (` | ` separator). Pain evidence should accumulate across interactions, never be replaced.
- **Forcing deep probing against resistance:** The user decided "respect their pace, not every customer goes deep on first conversation." The engine MUST detect resistance signals and back off, noting gaps for later.

## Don't Hand-Roll

Problems that look simple but have existing solutions:

| Problem | Don't Build | Use Instead | Why |
|---------|-------------|-------------|-----|
| Structured question selection | Rule-based decision tree | instructor + LLM structured output | LLM catches nuanced conversational signals that rules miss; user decided "adaptive based on signals" not rules |
| Pain depth assessment | Keyword-matching for emotional language | LLM analysis via instructor | Emotional language detection requires context (same words mean different things); LLM handles nuance |
| Contact mention detection | Regex for names/titles | LLM extraction via instructor | Names and roles appear in varied forms ("my boss", "the VP", "Sarah"); LLM handles natural language variation |
| Methodology blending prompts | Hard-coded question templates | Dynamic prompt composition from QBS engine output | Templates become stale; dynamic composition adapts to each conversation's unique context |
| Pain utilization across conversations | Custom tracking system | Extend existing ConversationState.metadata + qualification.py merge pattern | Infrastructure already exists; adding QBS state follows the same persistence pattern |
| Engagement signal detection | Sentiment analysis library | LLM contextual analysis via instructor | Engagement is more than sentiment; it includes elaboration patterns, topic shifts, energy detection that require conversational context |

**Key insight:** Phase 4.2 is fundamentally a prompt engineering and structured extraction phase. The entire technical implementation uses existing infrastructure (instructor, litellm, pydantic, ConversationState). The complexity is in the CONTENT of the prompts and the QUALITY of the signal detection, not in new technical architecture.

## Common Pitfalls

### Pitfall 1: QBS Becomes Interrogation
**What goes wrong:** The agent asks too many probing questions in sequence, making the conversation feel like an interrogation rather than a consultation. This is the #1 failure mode of QBS implementation.
**Why it happens:** The system optimizes for information extraction without balancing conversational rhythm. Every message tries to ask a qualifying question.
**How to avoid:** The QBS engine must include a "give value first" signal. Before asking a QBS question, the agent should share an insight, acknowledge what the customer said, or provide useful context. The Voss methodology (already in prompts.py) supports this -- mirroring and labeling provide natural value-giving moments. The QBS prompt section should include: "Before asking your QBS question, lead with value: mirror what they said, label an emotion, or share a relevant insight."
**Warning signs:** Customer replies become shorter. Customer asks "why do you need to know that?" Customer stops responding.

### Pitfall 2: Pain Funnel Goes Too Deep Too Fast
**What goes wrong:** The agent pushes for emotional/root-cause pain disclosure in the first or second interaction, before trust is established. Customer becomes uncomfortable or disengaged.
**Why it happens:** The pain depth target ("probe until root cause or emotional driver") is interpreted as "achieve this in one conversation."
**How to avoid:** Track probe_count_current_topic in PainFunnelState. If the customer has not shown self-elaboration signals after 2-3 probes on the same topic, back off. Store the gap in revisit_later for future conversations. The user's locked decision explicitly states: "Not every customer goes deep on first conversation. Note the gap, revisit later when trust builds."
**Warning signs:** Customer gives increasingly shorter answers on the same topic. Customer changes subject. Customer says "I think we covered this."

### Pitfall 3: Methodology Blending Produces Unnatural Questions
**What goes wrong:** The blended question (QBS type + MEDDIC target + Voss delivery) reads as formulaic or awkward. "It sounds like [label] your timeline for budget allocation [BANT] is creating pressure -- how is that affecting your team's priorities? [impact question]" sounds scripted.
**How to avoid:** The LLM generates the actual question text -- do not template-stitch methodology components. Provide the QBS engine output as GUIDANCE in the system prompt, not as a fill-in-the-blank template. The LLM's natural language ability produces smooth blending when given the right constraints. Include in prompts: "The following QBS guidance informs your approach. DO NOT mechanically combine these elements. Produce a natural, conversational question that achieves the underlying goal."
**Warning signs:** Generated questions contain obvious methodology jargon. Questions feel like they serve the agent's data needs, not the customer's interests.

### Pitfall 4: Account Expansion Feels Transactional
**What goes wrong:** The agent asks "Can you introduce me to your boss?" too directly or too early, making the customer feel like a stepping stone rather than a valued contact.
**Why it happens:** Expansion triggers fire immediately when a name/role is mentioned, without assessing relationship trust level.
**How to avoid:** The ExpansionRecommendation must include urgency assessment. Not every mention warrants immediate expansion. If interaction_count < 3 or if the customer hasn't shown high engagement signals, expansion should be deferred to "after_trust_builds." The user's locked decision includes "Respect and revisit: Don't push, build more trust, revisit later."
**Warning signs:** Customer hesitates or deflects when asked about other contacts. Customer becomes less responsive after an expansion attempt.

### Pitfall 5: QBS Engine LLM Calls Add Latency
**What goes wrong:** Adding QBS signal analysis as a separate LLM call before every message generation doubles the latency of the agent's response cycle.
**Why it happens:** Each message now requires: (1) QBS signal analysis call, (2) message generation call -- two serial LLM round-trips.
**How to avoid:** Use the "fast" model (Haiku) for QBS signal analysis with low max_tokens (512). Consider parallel execution: kick off QBS analysis and context compilation simultaneously, then use QBS output when building the prompt. The existing codebase uses model="fast" for next-action recommendations and escalation recommendations. Follow the same pattern. If latency is still unacceptable, fall back to rule-based QBS selection for obvious situations (similar to how NextActionEngine has a rule-based fast path).
**Warning signs:** End-to-end message generation time > 5 seconds. QBS analysis call takes > 2 seconds.

### Pitfall 6: QBS State Grows Unbounded in Metadata
**What goes wrong:** The ConversationState.metadata JSON column accumulates QBS state (pain topics, expansion triggers, question history) across hundreds of interactions, eventually causing performance issues in serialization/deserialization.
**Why it happens:** Every interaction appends to pain_topics and expansion_triggers without cleanup.
**How to avoid:** Limit pain_topics to the 10 most recent (by last_probed_at). Limit expansion_triggers to active (not yet contacted) entries. Archive resolved triggers. Keep total QBS metadata under 5KB per conversation state. Follow the existing pattern: qualification.key_insights is capped at useful entries, not unbounded.
**Warning signs:** ConversationState load times increase. metadata JSON exceeds 10KB.

## Code Examples

### Example 1: QBS Question Engine Core Implementation

```python
# Source: Adapted from existing QualificationExtractor pattern in qualification.py
import instructor
import litellm
import structlog
from src.app.agents.sales.qbs.schemas import (
    QBSQuestionRecommendation,
    QBSQuestionType,
    PainFunnelState,
)
from src.app.agents.sales.schemas import ConversationState, QualificationState

logger = structlog.get_logger(__name__)


class QBSQuestionEngine:
    """Adaptive QBS question selection based on conversation signals.

    Analyzes conversation context through three sensing modes:
    1. Information gap sensing: What BANT/MEDDIC data is missing?
    2. Customer engagement signals: Where is the customer's energy?
    3. Natural conversation flow: What logically follows?

    Produces a QBSQuestionRecommendation that blends QBS type +
    MEDDIC/BANT target + Chris Voss delivery technique.

    Uses the 'fast' model for low-latency signal analysis.
    """

    def __init__(self, llm_service: object) -> None:
        self._llm_service = llm_service

    async def analyze_and_recommend(
        self,
        conversation_state: ConversationState,
        latest_message: str,
        conversation_history: list[str] | None = None,
    ) -> QBSQuestionRecommendation:
        """Analyze conversation signals and recommend optimal QBS question.

        Single LLM call with instructor for structured output.
        Falls back to rule-based recommendation on LLM failure.
        """
        # Extract pain state from metadata
        pain_state = self._load_pain_state(conversation_state)

        try:
            client = instructor.from_litellm(litellm.acompletion)

            model = self._resolve_model("fast")

            messages = self._build_analysis_prompt(
                conversation_state, latest_message,
                conversation_history, pain_state,
            )

            recommendation = await client.chat.completions.create(
                model=model,
                response_model=QBSQuestionRecommendation,
                messages=messages,
                max_tokens=512,
                temperature=0.3,
                max_retries=2,
            )

            logger.info(
                "qbs.question_recommended",
                question_type=recommendation.question_type.value,
                target=recommendation.meddic_bant_target,
                delivery=recommendation.voss_delivery,
                pain_depth=recommendation.pain_depth.value,
                engagement=recommendation.engagement_signal.value,
            )
            return recommendation

        except Exception as exc:
            logger.warning(
                "qbs.analysis_failed",
                error=str(exc),
                error_type=type(exc).__name__,
            )
            return self._fallback_recommendation(conversation_state, pain_state)

    def _fallback_recommendation(
        self,
        state: ConversationState,
        pain_state: PainFunnelState,
    ) -> QBSQuestionRecommendation:
        """Rule-based fallback when LLM analysis fails."""
        # Match existing NextActionEngine._rule_based_actions pattern
        from src.app.agents.sales.qbs.schemas import (
            EngagementSignal, PainDepthLevel,
        )

        if state.interaction_count == 0:
            return QBSQuestionRecommendation(
                question_type=QBSQuestionType.PAIN_FUNNEL,
                meddic_bant_target="need",
                voss_delivery="calibrated_question",
                suggested_question="What challenges are you facing with your current approach?",
                rationale="First interaction: start with surface-level pain discovery",
                engagement_signal=EngagementSignal.FACTUAL,
                pain_depth=PainDepthLevel.NOT_EXPLORED,
                should_probe_deeper=False,
            )
        # ... additional rule-based fallbacks by stage/qualification gaps

    def _load_pain_state(self, state: ConversationState) -> PainFunnelState:
        """Load PainFunnelState from ConversationState.metadata."""
        qbs_data = state.metadata.get("qbs", {})
        pain_data = qbs_data.get("pain_state", {})
        if pain_data:
            return PainFunnelState(**pain_data)
        return PainFunnelState()

    def _resolve_model(self, model_name: str) -> str:
        """Resolve model name from LLM service router config."""
        # Same pattern as QualificationExtractor
        model = "anthropic/claude-sonnet-4-20250514"
        if hasattr(self._llm_service, "router") and self._llm_service.router:
            for m in self._llm_service.router.model_list:
                if m.get("model_name") == model_name:
                    model = m["litellm_params"]["model"]
                    break
        return model
```

### Example 2: QBS Prompt Integration with Existing System

```python
# Source: Extension of existing build_system_prompt in prompts.py
# Shows how QBS section is injected into the existing prompt composition

QBS_METHODOLOGY_PROMPT: str = """\
You integrate Question Based Selling (QBS) methodology by Thomas Freese \
throughout your interactions. QBS questions are not asked mechanically -- \
they emerge naturally from the conversation, blended with Chris Voss empathy \
techniques and targeted at BANT/MEDDIC qualification gaps.

**QBS Question Types:**
- **Pain Funnel:** Surface problems -> business impact -> emotional/root cause. \
  Progress gradually. Respect the customer's pace.
- **Impact Questions:** Help the customer feel the weight of their problem. \
  "What happens to your team's other priorities when they're stuck doing this?"
- **Solution Questions:** Connect their articulated pain to specific capabilities. \
  Only after pain is well-established.
- **Confirmation Questions:** Validate understanding and build micro-commitments. \
  "So if I'm hearing you correctly, the core issue is..."

**Elite Salesperson Principles:**
- Listen actively for what the customer reveals AND what they do NOT reveal
- Follow the energy -- if they light up on a topic, probe deeper there
- Questions emerge from what was just said, never from a script
- Gap sensing -- intuitively identify what's missing and probe accordingly
- Give value BEFORE asking -- share an insight, mirror their words, label an emotion
- Context over checklist -- respond to the conversation, not a methodology sequence

**CRITICAL:** Never ask more than one probing question per message. Balance \
every question with acknowledgment, value, or insight. The conversation should \
feel consultative, not interrogative.\
"""


def build_system_prompt(
    persona: PersonaType,
    channel: Channel,
    deal_stage: DealStage,
    qbs_guidance: str | None = None,  # NEW parameter
) -> str:
    """Compose full system prompt with optional QBS guidance injection."""
    # ... existing persona, channel, stage sections ...

    prompt = (
        "You are a top 1% enterprise sales professional...\n\n"
        f"{VOSS_METHODOLOGY_PROMPT}\n\n"
        f"{QBS_METHODOLOGY_PROMPT}\n\n"  # NEW: always include QBS base
    )

    # Inject conversation-specific QBS guidance if available
    if qbs_guidance:
        prompt += f"{qbs_guidance}\n\n"

    prompt += (
        f"{persona_section}\n\n"
        f"{channel_section}\n\n"
        f"**Current Deal Stage:**\n{stage_guidance}\n\n"
        f"{rules_section}"
    )

    return prompt
```

### Example 3: Pain Utilization Across Conversations

```python
# Source: Extension of existing _format_context_summary in agent.py
# Shows how pain state is referenced for continuity across conversations

def _format_context_summary_with_qbs(sales_ctx: dict) -> str:
    """Enhanced context summary including QBS pain state."""
    state: ConversationState = sales_ctx["conversation_state"]
    pain_state = PainFunnelState(**state.metadata.get("qbs", {}).get("pain_state", {}))

    parts = [
        # ... existing parts from _format_context_summary ...
    ]

    # Add pain state for continuity
    if pain_state.pain_topics:
        topic_summaries = []
        for topic in pain_state.pain_topics[:3]:
            summary = f"- {topic.topic} (depth: {topic.depth.value})"
            if topic.business_impact:
                summary += f" -- impact: {topic.business_impact}"
            topic_summaries.append(summary)
        parts.append("\nIdentified Pain Points:\n" + "\n".join(topic_summaries))

    # Add expansion context
    expansion_data = state.metadata.get("qbs", {}).get("expansion", {})
    if expansion_data.get("detected_contacts"):
        parts.append(
            f"\nExpansion Opportunities: {len(expansion_data['detected_contacts'])} "
            f"contacts detected but not yet engaged"
        )

    return "\n".join(parts)
```

### Example 4: Account Expansion Detection

```python
# Source: Adapted from existing PoliticalMapper.refine_from_conversation pattern
class AccountExpansionDetector:
    """Detects account expansion opportunities from conversation text.

    Scans for natural mentions of other people, teams, or roles.
    Produces expansion recommendations with QBS/Voss approaches.
    """

    def __init__(self, llm_service: object) -> None:
        self._llm_service = llm_service

    async def detect_expansion_triggers(
        self,
        conversation_text: str,
        existing_contacts: list[str],
    ) -> list[ExpansionTrigger]:
        """Detect mentions of new contacts in conversation text.

        Uses instructor for structured extraction of mentioned
        names/roles with context and recommended approach.
        """
        client = instructor.from_litellm(litellm.acompletion)

        messages = [
            {
                "role": "system",
                "content": (
                    "You detect mentions of other people in sales conversations. "
                    "For each mention, extract: who was mentioned, the context, "
                    "their relationship to the speaker, and recommend an expansion "
                    "approach using QBS questioning techniques.\n\n"
                    "Known contacts already engaged: "
                    + ", ".join(existing_contacts)
                    + "\n\nOnly flag NEW contacts not in the known list."
                ),
            },
            {"role": "user", "content": conversation_text},
        ]

        class ExpansionResult(BaseModel):
            triggers: list[ExpansionTrigger] = Field(default_factory=list)

        result = await client.chat.completions.create(
            model=self._resolve_model("fast"),
            response_model=ExpansionResult,
            messages=messages,
            max_tokens=512,
            temperature=0.2,
            max_retries=1,
        )
        return result.triggers
```

### Example 5: Learning Integration (Phase 4.1 Feedback)

```python
# Source: Extension of existing OutcomeTracker pattern
# Shows how QBS question effectiveness feeds back into the learning system

async def record_qbs_outcome(
    outcome_tracker: OutcomeTracker,
    tenant_id: str,
    conversation_state_id: str,
    question_type: str,  # pain_funnel, impact, solution, confirmation
    meddic_bant_target: str,
    voss_delivery: str,
    predicted_confidence: float,
) -> None:
    """Record a QBS question outcome for learning feedback.

    Tracks which QBS blending patterns lead to best outcomes.
    Window: email_engagement (24h reply detection).
    """
    await outcome_tracker.record_outcome(
        tenant_id=tenant_id,
        conversation_state_id=conversation_state_id,
        action_type=f"qbs_{question_type}",
        predicted_confidence=predicted_confidence,
        outcome_type="email_engagement",
        metadata={
            "question_type": question_type,
            "meddic_bant_target": meddic_bant_target,
            "voss_delivery": voss_delivery,
            "interaction_count_at_creation": 0,  # Populated at call time
        },
    )
```

## State of the Art

| Old Approach | Current Approach | When Changed | Impact |
|--------------|------------------|--------------|--------|
| Fixed question scripts per sales stage | Adaptive signal-based question selection via LLM | 2024-2025 with GPT-4/Claude era | Questions emerge from conversation context rather than scripts; dramatic improvement in natural conversation flow |
| SPIN/QBS/Sandler as separate methodologies | Blended methodology stacks in prompts | 2024-2025 | Top sales AI tools blend multiple methodologies; rigid single-methodology approaches produce unnatural conversations |
| Rule-based pain depth tracking | LLM-powered emotional recognition + engagement signal detection | 2025-2026 | LLMs detect tone shifts, self-elaboration patterns, and implicit emotional language that keyword matching misses |
| Sequential account expansion (champion first, then others) | Parallel multi-threading from organic conversation signals | 2025-2026 | Data shows 37% higher close rates with 3+ engaged contacts; organic triggers outperform sequential playbooks |

**Deprecated/outdated:**
- **Fixed question sequences (pain->impact->solution->confirmation):** Modern approach is fully adaptive. The user explicitly decided against this.
- **Separate methodology modes:** Switching between "QBS mode" and "Voss mode" produces jarring transitions. Blended approach is standard.
- **Rule-based engagement detection:** Keyword lists for engagement (e.g., "frustrated", "excited") miss context. LLM analysis is required.

## Open Questions

1. **QBS question tracking granularity for Phase 4.1 learning**
   - What we know: The existing outcome tracking system can track action_type and link to outcomes. We can use action_type=`qbs_pain_funnel`, `qbs_impact`, etc.
   - What's unclear: Whether we should also track the specific MEDDIC/BANT target and Voss delivery technique in the outcome metadata, or whether that level of granularity produces too much noise for calibration with small sample sizes.
   - Recommendation: Start with action_type-level tracking (4 QBS types). Store the full blend details in metadata for future analysis. Only build calibration at the action_type level initially. Upgrade to finer-grained calibration once sample sizes are sufficient (50+ per combination).

2. **QBS engine call placement: before or during message generation**
   - What we know: The current flow is: compile context -> build prompt -> call LLM for message. Adding QBS analysis creates a serial dependency.
   - What's unclear: Whether QBS analysis should be a separate pre-step (cleaner separation) or embedded in the prompt (single LLM call).
   - Recommendation: Separate pre-step using "fast" model. The clean separation enables independent testing, caching of QBS recommendations, and tracking of QBS decisions in the learning system. The latency cost (~500ms for fast model with 512 max_tokens) is acceptable.

3. **Interaction between QBS expansion detection and Phase 5 political mapping**
   - What we know: Phase 5's PoliticalMapper scores stakeholders. QBS expansion detects new contact mentions. Both deal with multi-contact awareness.
   - What's unclear: Exact integration point -- should expansion triggers automatically create stakeholder entries in the deal management system?
   - Recommendation: QBS expansion detection stores triggers in ConversationState.metadata. A separate integration hook (similar to Phase 5's deal hooks) can create stakeholder entries if the expansion trigger is acted upon. Keep detection separate from action.

## Sources

### Primary (HIGH confidence)
- Existing codebase analysis (agent.py, prompts.py, schemas.py, qualification.py, actions.py, escalation.py, state_repository.py, learning/*.py, deals/political.py) -- direct code inspection of all integration points
- instructor library documentation (/instructor-ai/instructor via Context7) -- verified instructor.from_litellm async pattern with nested Pydantic models
- Phase 4 RESEARCH.md -- established patterns for structured output extraction, prompt engineering, and fail-open LLM usage

### Secondary (MEDIUM confidence)
- [Thomas Freese QBS Summary](https://www.sellingandpersuasiontechniques.com/question-based-selling-summary.html) -- QBS framework structure, four question categories, escalation sequence, credibility-first principle
- [QBS SaaS Approach](https://productiveshop.com/question-based-selling-qbs-approach/) -- QBS question types (open-ended, probing, leading) and SaaS application patterns
- [HubSpot Pain Funnel](https://blog.hubspot.com/sales/pain-funnel) -- Three-stage pain funnel structure (problem identification, cost implications, emotional impact) with example questions
- [Sales Multi-Threading Strategies](https://www.pclub.io/blog/multithreading-in-sales) -- Multi-threading best practices: stakeholder role mapping, pre-meeting engagement, resistance handling, 37% close rate improvement with 3+ contacts
- [Gong Sandler Pain Funnel](https://www.gong.io/blog/sandler-pain-funnel) -- Pain funnel question progression and emotional recognition signals

### Tertiary (LOW confidence)
- WebSearch results on LLM prompt engineering for sales agents -- general patterns confirmed by codebase analysis but not independently verified with authoritative source
- Multi-threading statistics (Gartner 50% revenue growth claim, Outreach 56% win rate improvement) -- cited across multiple sources but original research not accessed

## Metadata

**Confidence breakdown:**
- Standard stack: HIGH -- No new libraries. All implementation uses existing installed packages (instructor, litellm, pydantic, structlog). Verified via pyproject.toml and existing usage patterns in codebase.
- Architecture: HIGH -- Pattern directly extends existing SalesAgent architecture. QBS module mirrors existing module structure (schemas, prompts, engine). Integration points are well-defined (prompts.py extension, agent.py injection, metadata storage).
- Pitfalls: HIGH -- Pitfalls derived from both domain expertise (sales methodology implementation) and codebase-specific patterns (latency constraints, metadata growth, evidence accumulation).
- QBS Methodology: MEDIUM -- QBS framework details verified across multiple sources but Thomas Freese's original book was not directly consulted. Pain funnel structure well-documented across Gong, HubSpot, Sandler sources.
- Account Expansion: MEDIUM -- Multi-threading strategies well-documented but integration with LLM-driven detection is novel; no authoritative source for LLM-based expansion detection patterns.

**Research date:** 2026-02-12
**Valid until:** 2026-03-14 (30 days -- stable domain, no rapidly changing dependencies)
